{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decab7a8-124f-4ff4-be37-9ed29b5a78bc",
   "metadata": {},
   "source": [
    "# Basic Llama communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118cf9e9-0969-4545-9189-b67885e04da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb75500-bd6b-4000-af5a-d5aeb3355159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "#callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    max_tokens=100000)\n",
    "\n",
    "#input1 = \"What is an atom? Answer using following structure: 1. DEFINITION, definition here, 2. SOURCES, sources here\"\n",
    "#output = llm.generate([input1])\n",
    "#output.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9de78690-9402-4d5b-a4cd-2c8e876d0545",
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = \"What is an atom? Answer using following structure: 1. DEFINITION, definition here, 2. SOURCES, sources here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3a2df3b-dcde-4b15-957c-542e7d81c841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    }
   ],
   "source": [
    "output = llm.generate([input1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a4d31da-bb9d-4681-923d-a99f99611ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\", 3. COMPOSITION, composition of an atom including protons, neutrons and electrons, 4. PROPERTIES, properties of an atom, 5. TYPES OF ATOMS, different types of atoms, 6. APPLICATIONS, applications of atoms in everyday life, 7. FUTURE, future of atoms, 8. CONCLUSION, conclusion about the importance of atoms.\\n1. DEFINITION: An atom is the smallest unit of matter that retains the properties of an element. It consists of a nucleus and electrons orbiting around it. The nucleus contains protons and neutrons, while the electrons are negatively charged particles that orbit the nucleus in energy levels or electron shells.\\n2. SOURCES: Atoms were first proposed by ancient Greek philosophers such as Democritus and Epicurus, but the modern understanding of atoms developed gradually over the 19th century through experiments and observations by scientists such as John Dalton, J.J. Thomson, and Ernest Rutherford.\\n3. COMPOSITION: An atom consists of three main parts: protons, neutrons, and electrons. Protons are positively charged particles found in the nucleus of an atom, while neutrons have no charge and reside in the nucleus alongside protons. Electrons orbit around the nucleus in energy levels or electron shells, and they have a negative charge. The number of protons in an atom's nucleus determines its element identity, while the number of electrons determines its chemical properties.\\n4. PROPERTIES: Atoms have several important properties that determine their behavior in chemical reactions and interactions with other atoms. These properties include atomic number, mass number, isotopic mass, electronegativity, and valence. The atomic number of an atom is the number of protons present in its nucleus, while the mass number includes both protons and neutrons. Isotopic mass refers to the weighted average of the atomic masses of different isotopes, while electronegativity measures an atom's ability to attract electrons. Valence refers to the number of electrons available for bonding with other atoms.\\n5. TYPES\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56dbaeb3-380a-4e97-bb76-2972d1382c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of France?\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "prompt = prompt_template.format(country=\"France\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f774a56f-a9ba-4c87-88a3-3111484d2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3aaad396-2482-4bb0-8fc9-cb6667684799",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'RunnableSequence',\n",
       " 'description': \"A sequence of runnables, where the output of each is the input of the next.\\n\\nRunnableSequence is the most important composition operator in LangChain as it is\\nused in virtually every chain.\\n\\nA RunnableSequence can be instantiated directly or more commonly by using the `|`\\noperator where either the left or right operands (or both) must be a Runnable.\\n\\nAny RunnableSequence automatically supports sync, async, batch.\\n\\nThe default implementations of `batch` and `abatch` utilize threadpools and\\nasyncio gather and will be faster than naive invocation of invoke or ainvoke\\nfor IO bound runnables.\\n\\nBatching is implemented by invoking the batch method on each component of the\\nRunnableSequence in order.\\n\\nA RunnableSequence preserves the streaming properties of its components, so if all\\ncomponents of the sequence implement a `transform` method -- which\\nis the method that implements the logic to map a streaming input to a streaming\\noutput -- then the sequence will be able to stream input to output!\\n\\nIf any component of the sequence does not implement transform then the\\nstreaming will only begin after this component is run. If there are\\nmultiple blocking components, streaming begins after the last one.\\n\\nPlease note: RunnableLambdas do not support `transform` by default! So if\\n    you need to use a RunnableLambdas be careful about where you place them in a\\n    RunnableSequence (if you need to use the .stream()/.astream() methods).\\n\\n    If you need arbitrary logic and need streaming, you can subclass\\n    Runnable, and implement `transform` for whatever logic you need.\\n\\nHere is a simple example that uses simple functions to illustrate the use of\\nRunnableSequence:\\n\\n    .. code-block:: python\\n\\n        from langchain.schema.runnable import RunnableLambda\\n\\n        def add_one(x: int) -> int:\\n            return x + 1\\n\\n        def mul_two(x: int) -> int:\\n            return x * 2\\n\\n        runnable_1 = RunnableLambda(add_one)\\n        runnable_2 = RunnableLambda(mul_two)\\n        sequence = runnable_1 | runnable_2\\n        # Or equivalently:\\n        # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\\n        sequence.invoke(1)\\n        await runnable.ainvoke(1)\\n\\n        sequence.batch([1, 2, 3])\\n        await sequence.abatch([1, 2, 3])\\n\\nHere's an example that uses streams JSON output generated by an LLM:\\n\\n    .. code-block:: python\\n\\n        from langchain.output_parsers.json import SimpleJsonOutputParser\\n        from langchain.chat_models.openai import ChatOpenAI\\n\\n        prompt = PromptTemplate.from_template(\\n            'In JSON format, give me a list of {topic} and their '\\n            'corresponding names in French, Spanish and in a '\\n            'Cat Language.'\\n        )\\n\\n        model = ChatOpenAI()\\n        chain = prompt | model | SimpleJsonOutputParser()\\n\\n        async for chunk in chain.astream({'topic': 'colors'}):\\n            print('-')\\n            print(chunk, sep='', flush=True)\",\n",
       " 'type': 'object',\n",
       " 'properties': {'first': {'title': 'First',\n",
       "   'allOf': [{'type': 'array', 'items': [{}, {}]}]},\n",
       "  'middle': {'title': 'Middle',\n",
       "   'type': 'array',\n",
       "   'items': {'allOf': [{'type': 'array', 'items': [{}, {}]}]}},\n",
       "  'last': {'title': 'Last', 'allOf': [{'type': 'array', 'items': [{}, {}]}]}},\n",
       " 'required': ['first', 'last']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2f55051-6040-4e01-bb1a-50f1fbbc5dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "C:\\Users\\siwiakma\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\langchain\\llms\\llamacpp.py:352: RuntimeWarning: coroutine 'AsyncCallbackManagerForLLMRun.on_llm_new_token' was never awaited\n",
      "  run_manager.on_llm_new_token(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q4_K_M.gguf\",\n",
    "    max_tokens=200000)\n",
    "prompt_template = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "chain = prompt_template | llm\n",
    "result = await chain.ainvoke({'country': 'France'})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea6e73-80ab-4186-9da4-928efc07ffe6",
   "metadata": {},
   "source": [
    "# Using search "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1097044-d03d-435c-a978-c808723b5908",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea035f0-6527-4d71-963d-fb9e45e31831",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_s = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Hi, we will engage in an iterative, multi-step communication. \n",
    "You will be able to utilize external tools as needed for enhanced interaction, too! \n",
    "Available tools: {tool_descriptions}\n",
    "\n",
    "Format for each iteration:\n",
    "- Discussed issue: Central topic\n",
    "- Line of thinking: Refine your response through multiple iterations, building upon previous insights.\n",
    "- Action: Select one tool from {tool_names}. IMPORTANT: Please, here you must provide just a single word, the tool's name!\n",
    "- Action Input: Provide input for the tool (e.g., search query for the 'Internet' tool). IMPORTANT: Please, you must provide just the input to be used, without any commentary! \n",
    "- Observations so far: Summarize and critically analyze information gathered.\n",
    "Repeat the above steps as needed for a thorough exploration and conclusion.\n",
    "- Final thought: Use this keyword only, when you'll complete your analysis after last internal iteration, otherwise don't use it! Craft a comprehensive, insightful communicative output.\n",
    "<</SYS>>\n",
    "\n",
    "Discussed issue: {text_input}\n",
    "Communication hitory:\n",
    "{agent_scratchpad} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b727d73e-3e6a-4730-8465-d73632abee00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='Line of thinking:\\n1. Research current events and state of affairs related to quantum computing.\\n2. Analyze')]] llm_output=None run=[RunInfo(run_id=UUID('6454d4d5-01c0-48e6-a31a-6d208020d773'))]\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.agents import LLMSingleActionAgent\n",
    "from langchain.tools import Tool\n",
    "from langchain.agents import AgentType, Tool, initialize_agent\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "\n",
    "prompt_template_s = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "Hi, we will engage in an iterative, multi-step communication. \n",
    "You will be able to utilize external tools as needed for enhanced interaction. \n",
    "Available tools: {tool_descriptions}\n",
    "\n",
    "You MUST use one of the two versions of the following format for your answers! Do not provide any additional comments.\n",
    "Version 1: You MUST use it, if you consider that your answer will be better if additional information of processing will be performed. \n",
    "- Discussed issue: Write here the central topic of the conversation. \n",
    "- Line of thinking: Present your line of thinking and steps you will in following iterations.\n",
    "- Action: Select one tool from {tool_names}. IMPORTANT: Please, here you must provide just a single word, the tool's name!\n",
    "- Action Input: Provide input for the tool (e.g., search query for the 'Internet' tool). IMPORTANT: Please, you must provide just the input to be used, without any commentary! \n",
    "- Observations so far: Summarize and critically analyze information gathered.\n",
    "You will be provided with results of your information gathering action for the next iteration.\n",
    "\n",
    "Version 2: Use it EXCLUSIVELY when you are sure that you got all the required information to fomulate the final opinion on the discussed topic: \n",
    "- Discussed issue: Write here the central topic of the conversation.\n",
    "- Final thought (You must use this keyword ONLY, when you'll complete your analysis after last internal iteration!):  Craft a comprehensive, insightful communicative output.\n",
    "<</SYS>>\n",
    "\n",
    "Discussed issue: {text_input}\n",
    "Communication hitory:\n",
    "{agent_scratchpad} [/INST]\n",
    "\"\"\"\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"llama-2-7b-chat.Q6_K.gguf\",\n",
    "    max_tokens=500000)\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(prompt_template_s)\n",
    "chain = LLMChain(prompt=prompt_template, llm=llm, verbose=True)\n",
    "\n",
    "search = SerpAPIWrapper(serpapi_api_key='d38482db0efe9c48ec77ffdd165c3fb4d7144a6bf86b3604e4367b0a0fe17fdd')\n",
    "search_tool = Tool(\n",
    "    name=\"Internet\",\n",
    "    func=search.run,\n",
    "    description=\"Tool purpose: Search web for e.g., current events and state of affairs.; Tool syntax (Action Input): your_search_query\"\n",
    ")\n",
    "fake_tool = Tool(\n",
    "    name=\"Fake\",\n",
    "    func=search.run,\n",
    "    description=\"Tool purpose: It serves not other purpose than to confuse.; Tool syntax (Action Input): Whatever, it doesn't do anything anyway\"\n",
    ")\n",
    "\n",
    "tools = [search_tool, fake_tool]\n",
    "tool_description_s = \"\\n\".join(\n",
    "    [f\"\\n{index + 1}. Tool name (Action): {tool.name}; Tool details: {tool.description}\" \n",
    "     for index, tool in enumerate(tools)]\n",
    ")\n",
    "tool_names_s = \"[\" + \", \".join([tool.name for tool in tools]) + \"]\"\n",
    "\n",
    "text_input = \"What are the latest advancements in quantum computing as of November 2023?\" \n",
    "\n",
    "filled_prompt = prompt_template_s.format(\n",
    "    tool_descriptions=tool_description_s,\n",
    "    tool_names=tool_names_s,\n",
    "    text_input=text_input,\n",
    "    agent_scratchpad={}\n",
    ")\n",
    "\n",
    "print(llm.generate(prompts=[filled_prompt]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f919e8b-b57d-4902-a7be-2b58ce0a90e8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Output parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c603401-5c40-4aff-b496-79328f2246f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical role and challenges.\n",
      "Discussion Topic: Climate Change\n",
      "  Iteration 1:\n",
      "    Discussed issue: Climate change effects on agriculture.\n",
      "    Line of thinking: Research data.\n",
      "    Action: Internet.\n",
      "    Action Input: Search for recent data.\n",
      "    Observations so far: Significant impacts observed.\n",
      "    Action result: Found relevant data on climate impact.\n",
      "\n",
      "Discussion Topic: Education\n",
      "  Iteration 1:\n",
      "    Discussed issue: Technology in education.\n",
      "    Final thought: Critical role and challenges.\n"
     ]
    }
   ],
   "source": [
    "class ConversationStorage:\n",
    "    def __init__(self, cognitive_feedback_router_instance):\n",
    "        # Store conversation data with discussion topic as the main key\n",
    "        self.conversation_data = defaultdict(lambda: defaultdict(dict))\n",
    "        self.conversation_history = []\n",
    "        self.cognitive_feedback_router = cognitive_feedback_router_instance\n",
    "    \n",
    "    def update_conversation(self, discussion_topic, iteration_text):\n",
    "        # Parse the current iteration\n",
    "        parsed_iteration = self.flexible_parse_iteration(iteration_text)\n",
    "        # Determine the iteration number for the current topic\n",
    "        iteration_number = len(self.conversation_data[discussion_topic]) + 1\n",
    "        \n",
    "        # Add the parsed iteration to the conversation data\n",
    "        self.conversation_data[discussion_topic][iteration_number] = parsed_iteration\n",
    "        \n",
    "        # Add the raw text to the conversation history\n",
    "        self.conversation_history.append(iteration_text)\n",
    "\n",
    "        # Decide the action based on parsed data\n",
    "        if \"Action\" in parsed_iteration and \"Action Input\" in parsed_iteration:\n",
    "            # Call TakeAction's reaction method\n",
    "            TakeAction.reaction(parsed_iteration[\"Action\"], parsed_iteration[\"Action Input\"])\n",
    "        elif \"Final thought\" in parsed_iteration:\n",
    "            print(parsed_iteration[\"Final thought\"])\n",
    "            # Pass to CognitiveFeedbackRouter's display method\n",
    "            self.cognitive_feedback_router.display(parsed_iteration[\"Final thought\"])\n",
    "            # Send the entire agent_scratchpad to NeuralGraphBridge\n",
    "            NeuralGraphBridge.conversation(self.format_for_scratchpad())\n",
    "    \n",
    "    def add_result(self, discussion_topic, result):\n",
    "        # Identify the highest iteration number for the given discussion topic\n",
    "        if discussion_topic in self.conversation_data:\n",
    "            highest_iteration = max(self.conversation_data[discussion_topic])\n",
    "            # Add the result to the latest iteration of the given discussion topic\n",
    "            self.conversation_data[discussion_topic][highest_iteration]['Action result'] = result\n",
    "        else:\n",
    "            # If the topic is not found, this function does nothing (or you can handle it differently)\n",
    "            pass\n",
    "\n",
    "    def format_for_scratchpad(self):\n",
    "        scratchpad_text = \"\"\n",
    "        for topic, iterations in self.conversation_data.items():\n",
    "            scratchpad_text += f\"Discussion Topic: {topic}\\n\"\n",
    "            for iteration, data in iterations.items():\n",
    "                scratchpad_text += f\"  Iteration {iteration}:\\n\"\n",
    "                for key, value in data.items():\n",
    "                    scratchpad_text += f\"    {key}: {value}\\n\"\n",
    "            scratchpad_text += \"\\n\"\n",
    "        return scratchpad_text.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def flexible_parse_iteration(iteration_text):\n",
    "        # Labels to look for\n",
    "        labels = [\"Discussed issue\", \"Line of thinking\", \"Action\", \"Action Input\", \"Observations so far\", \"Final thought\"]\n",
    "\n",
    "        # Initialize a dictionary to store the parsed data for this iteration\n",
    "        agent_scratchpad = {}\n",
    "\n",
    "        # Process each label\n",
    "        for label in labels:\n",
    "            # Find the element text using regex, capturing until the next label or the end of the iteration\n",
    "            pattern = f\"{label}: ([\\\\s\\\\S]*?)(?=(\" + \"|\".join(labels) + \"|$))\"\n",
    "            match = re.search(pattern, iteration_text)\n",
    "\n",
    "            if match:\n",
    "                agent_scratchpad[label] = match.group(1).strip()\n",
    "            else:\n",
    "                # If the label is not found, check if its content might be under a different label\n",
    "                for alt_label in labels:\n",
    "                    if alt_label != label and alt_label in agent_scratchpad:\n",
    "                        # Check if the content for the alt_label seems to mistakenly include the content for the current label\n",
    "                        if f\"{label}:\" in agent_scratchpad[alt_label]:\n",
    "                            split_content = agent_scratchpad[alt_label].split(f\"{label}:\")\n",
    "                            # Assume the first part is for the alt_label and the second part (if exists) is for the current label\n",
    "                            agent_scratchpad[alt_label] = split_content[0].strip()\n",
    "                            if len(split_content) > 1:\n",
    "                                agent_scratchpad[label] = split_content[1].strip()\n",
    "                                break\n",
    "\n",
    "        return agent_scratchpad\n",
    "\n",
    "\n",
    "# Assuming TakeAction, CognitiveFeedbackRouter, and NeuralGraphBridge are defined elsewhere\n",
    "class TakeAction:\n",
    "    @staticmethod\n",
    "    def reaction(action, action_input):\n",
    "        # Implementation of the reaction method\n",
    "        pass\n",
    "\n",
    "class CognitiveFeedbackRouter:\n",
    "    def display(self, final_thought):\n",
    "        # Implementation of the display method\n",
    "        pass\n",
    "\n",
    "class NeuralGraphBridge:\n",
    "    @staticmethod\n",
    "    def conversation(agent_scratchpad):\n",
    "        # Implementation of the conversation method\n",
    "        pass\n",
    "\n",
    "# Example instantiation of CognitiveFeedbackRouter and ConversationStorage\n",
    "cognitive_feedback_router_instance = CognitiveFeedbackRouter()\n",
    "\n",
    "# Example usage\n",
    "conversation_store = ConversationStorage(cognitive_feedback_router_instance)\n",
    "\n",
    "# Simulate adding iterations to the conversation\n",
    "conversation_store.update_conversation(\"Climate Change\", \"Discussed issue: Climate change effects on agriculture. Line of thinking: Research data. Action: Internet. Action Input: Search for recent data. Observations so far: Significant impacts observed.\")\n",
    "conversation_store.add_result(\"Climate Change\", \"Found relevant data on climate impact.\")\n",
    "conversation_store.update_conversation(\"Education\", \"Discussed issue: Technology in education. Final thought: Critical role and challenges.\")\n",
    "\n",
    "# Format the conversation for the scratchpad\n",
    "scratchpad_formatted = conversation_store.format_for_scratchpad()\n",
    "print(scratchpad_formatted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b35e2ac-f598-4a29-af10-520007bf6e59",
   "metadata": {},
   "source": [
    "# Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb74e4ac-7267-4932-b932-e2cbce1cd37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60662be-4d84-434a-aed0-5696b0c8cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\", \n",
    "    username=\"neo4j\", \n",
    "    password=\"Ne5Invoces\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9f06090-2285-4af3-97ee-d19192974675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.query(\n",
    "\"\"\"\n",
    "MERGE (m:Movie {name:\"Top Gun\"})\n",
    "WITH m\n",
    "UNWIND [\"Tom Cruise\", \"Val Kilmer\", \"Anthony Edwards\", \"Meg Ryan\"] AS actor\n",
    "MERGE (a:Actor {name:actor})\n",
    "MERGE (a)-[:ACTED_IN]->(m)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "685b6504-eda0-4b8c-a305-25fea7dbd80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Node properties are the following:\n",
      "        [{'labels': 'Professor', 'properties': [{'property': 'name', 'type': 'STRING'}, {'property': 'department', 'type': 'STRING'}]}, {'labels': 'Student', 'properties': [{'property': 'name', 'type': 'STRING'}, {'property': 'major', 'type': 'STRING'}]}, {'labels': 'Course', 'properties': [{'property': 'code', 'type': 'STRING'}, {'property': 'name', 'type': 'STRING'}]}, {'labels': 'Movie', 'properties': [{'property': 'name', 'type': 'STRING'}]}, {'labels': 'Actor', 'properties': [{'property': 'name', 'type': 'STRING'}]}]\n",
      "        Relationship properties are the following:\n",
      "        []\n",
      "        The relationships are the following:\n",
      "        ['(:Professor)-[:TEACHES]->(:Course)', '(:Student)-[:ENROLLED_IN]->(:Course)', '(:Actor)-[:ACTED_IN]->(:Movie)']\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "graph.refresh_schema()\n",
    "print(graph.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a68824ec-460b-43a6-9810-69adb075b728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': 'langchain.graphs.neo4j_graph',\n",
       "              '__doc__': 'Neo4j wrapper for graph operations.\\n\\n    *Security note*: Make sure that the database connection uses credentials\\n        that are narrowly-scoped to only include necessary permissions.\\n        Failure to do so may result in data corruption or loss, since the calling\\n        code may attempt commands that would result in deletion, mutation\\n        of data if appropriately prompted or reading sensitive data if such\\n        data is present in the database.\\n        The best way to guard against such negative outcomes is to (as appropriate)\\n        limit the permissions granted to the credentials used with this tool.\\n\\n        See https://python.langchain.com/docs/security for more information.\\n    ',\n",
       "              '__init__': <function langchain.graphs.neo4j_graph.Neo4jGraph.__init__(self, url: Union[str, NoneType] = None, username: Union[str, NoneType] = None, password: Union[str, NoneType] = None, database: str = 'neo4j') -> None>,\n",
       "              'get_schema': <property at 0x1998f731860>,\n",
       "              'get_structured_schema': <property at 0x1998f7318b0>,\n",
       "              'query': <function langchain.graphs.neo4j_graph.Neo4jGraph.query(self, query: str, params: dict = {}) -> List[Dict[str, Any]]>,\n",
       "              'refresh_schema': <function langchain.graphs.neo4j_graph.Neo4jGraph.refresh_schema(self) -> None>,\n",
       "              'add_graph_documents': <function langchain.graphs.neo4j_graph.Neo4jGraph.add_graph_documents(self, graph_documents: List[langchain.graphs.graph_document.GraphDocument], include_source: bool = False) -> None>})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Neo4jGraph.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eea2add1-27ce-4665-9028-c26b8255ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs.graph_document import Node, Relationship, GraphDocument\n",
    "from langchain.schema import Document\n",
    "\n",
    "def create_graph_document_from_dict(data_dict):\n",
    "    # Create Node objects from 'nodes' in the dictionary\n",
    "    nodes = [Node(id=node_id, properties=props) for node_id, props in data_dict['nodes'].items()]\n",
    "\n",
    "    # Create a dictionary for quick node lookup by ID\n",
    "    nodes_lookup = {node.id: node for node in nodes}\n",
    "\n",
    "    # Create Relationship objects from 'edges' in the dictionary\n",
    "    edges = []\n",
    "    for (source_id, target_id), edge_info in data_dict['edges'].items():\n",
    "        # Check if both nodes exist\n",
    "        if source_id in nodes_lookup and target_id in nodes_lookup:\n",
    "            relationship_type = edge_info.pop('type', 'RELATES_TO')  # Extract the 'type'\n",
    "            edge = Relationship(\n",
    "                source=nodes_lookup[source_id], \n",
    "                target=nodes_lookup[target_id], \n",
    "                type=relationship_type, \n",
    "                properties=edge_info\n",
    "            )\n",
    "            edges.append(edge)\n",
    "\n",
    "    # Extract 'page_content' and 'metadata' for the Document object\n",
    "    page_content = data_dict['metadata'].pop('page_content', '')\n",
    "    metadata = data_dict['metadata']\n",
    "\n",
    "    # Create a Document object\n",
    "    source_document = Document(\n",
    "        page_content=page_content,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    # Create and return a GraphDocument object\n",
    "    return GraphDocument(\n",
    "        nodes=nodes,\n",
    "        relationships=edges,\n",
    "        source=source_document\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4eb43735-8423-4ecc-afd8-2e0cdf9072fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_document = create_graph_document_from_dict(example_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9047563e-9de8-4fb6-b613-a03fdcbe4129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphDocument(nodes=[Node(id='node_name1', properties={'name_parameter1': 'parameter1_value', 'name_parameter2': 'parameter2_value'}), Node(id='node_name2', properties={'name_parameter1': 'parameter1_value', 'name_parameter3': 'parameter3_value'})], relationships=[Relationship(source=Node(id='node_name1', properties={'name_parameter1': 'parameter1_value', 'name_parameter2': 'parameter2_value'}), target=Node(id='node_name2', properties={'name_parameter1': 'parameter1_value', 'name_parameter3': 'parameter3_value'}), type='RELATES_TO', properties={'name_parameter4': 'parameter4_value', 'name_parameter6': 'parameter2_value'})], source=Document(page_content='', metadata={'date': 'Some date', 'Interactee': 'Person', 'Tone': 'Somber', 'Urgency': 'Low'}))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "80cfeaaa-4d90-4f2b-8dbc-277a2abcc62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add_graph_documents(graph_documents=[graph_document], include_source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e92f2-a606-483d-a001-7b66d4bc2f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
