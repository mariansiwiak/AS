{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d881d3-c7d8-483d-b9e7-facd3aa787cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336f6807-a53e-4844-bdbc-db27e5c5c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_custom_log_levels():\n",
    "    # Define custom logging levels\n",
    "    MURMUR_LEVEL_NUM = 39\n",
    "    logging.addLevelName(MURMUR_LEVEL_NUM, \"MURMUR\")\n",
    "    def log_murmur(self, message, *args, **kwargs):\n",
    "        \"\"\" \n",
    "        Logging of PFC actions.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isEnabledFor(MURMUR_LEVEL_NUM):\n",
    "            self._log(MURMUR_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.murmur = log_murmur\n",
    "\n",
    "    FLAG_LEVEL_NUM = 13\n",
    "    logging.addLevelName(FLAG_LEVEL_NUM, \"FLAG\")\n",
    "    def log_flag(self, message, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Logging of status flags statuses.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isEnabledFor(FLAG_LEVEL_NUM):\n",
    "            self._log(FLAG_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.flag = log_flag\n",
    "\n",
    "    PROMPTING_LEVEL_NUM = 8\n",
    "    logging.addLevelName(PROMPTING_LEVEL_NUM, \"PROMPTING\")\n",
    "    def log_prompting(self, message, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Logging of complete prompt messages sent to PFC. \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isEnabledFor(PROMPTING_LEVEL_NUM):\n",
    "            self._log(PROMPTING_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.prompt = log_prompting\n",
    "\n",
    "    MONOLOGUE_LEVEL_NUM = 7\n",
    "    logging.addLevelName(MONOLOGUE_LEVEL_NUM, \"MONOLOGUE\")\n",
    "    def log_monologue(self, message, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Logging of PFC internal monologue.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isEnabledFor(MONOLOGUE_LEVEL_NUM):\n",
    "            self._log(MONOLOGUE_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.monologue = log_monologue\n",
    "\n",
    "def setup_logging(log_level: int = 7):\n",
    "    setup_custom_log_levels()\n",
    "\n",
    "    # Create a file handler for logging\n",
    "    log_directory = \"console\"\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file_name = f\"{log_directory}/file_{current_time}.log\"\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file_name)\n",
    "    file_handler.setLevel(7)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(log_level)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(7)  # Set the lowest overall level to log\n",
    "\n",
    "    # Clear existing handlers (if any), and then add new handlers\n",
    "    if root_logger.hasHandlers():\n",
    "        root_logger.handlers.clear()\n",
    "    root_logger.addHandler(file_handler)\n",
    "    root_logger.addHandler(console_handler)\n",
    "\n",
    "# Call the setup function\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639dc99-a78d-445b-9dc9-41e1bab80c8c",
   "metadata": {},
   "source": [
    "### Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7eeff60-0d0a-46d2-935f-da4cc40889d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemUtility:\n",
    "    \"\"\"\n",
    "    A class for managing and retrieving predefined prompts.\n",
    "\n",
    "    This class stores a collection of utility functions utilized by other system classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_keywords(raw_output):\n",
    "        \"\"\"\n",
    "        Extracts keywords from the summary output.\n",
    "\n",
    "        Args:\n",
    "            raw_output (str): The output from which to extract keywords.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of extracted keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        # Regex pattern to find all occurrences of words flanked by **\n",
    "        pattern = r\"\\*\\*(.*?)\\*\\*\"\n",
    "        # Find all matches and strip the ** from each keyword\n",
    "        keywords = [keyword.lower() for keyword in re.findall(pattern, raw_output)]\n",
    "        return keywords\n",
    "\n",
    "    @staticmethod\n",
    "    def get_timestamp():\n",
    "        return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    @staticmethod\n",
    "    def archive(source_dir: str, source_file: str = None, archive_suffix='archive'):\n",
    "        \"\"\"\n",
    "        Moves processed file to the respective archive folder.\n",
    "\n",
    "        Args:\n",
    "            source_path (str): The source folder\n",
    "            source_file (str): The file to be archived\n",
    "\n",
    "        Returns:\n",
    "            bool: Information if the process has been successful\n",
    "        \"\"\"\n",
    "\n",
    "        if not source_file:\n",
    "            source_path = source_dir\n",
    "            source_dir, source_file = os.path.split(source_dir)\n",
    "        else:\n",
    "            source_path = os.path.join(source_dir, source_file)\n",
    "            \n",
    "        \n",
    "        destination_dir = '_'.join(source_path, archive_suffix)\n",
    "        directories_available = StemUtility.prepare_directory(destination_dir)\n",
    "        if not directories_available:\n",
    "            return False\n",
    "        \n",
    "        destination_path = os.path.join(destination_dir, source_file)\n",
    "        \n",
    "        logger = logging.getLogger('StemUtility')        \n",
    "        try:\n",
    "            shutil.move(source_path, destination_path)\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {source_path}\")\n",
    "            return False\n",
    "        except PermissionError:\n",
    "            logger.error(f\"Permission denied: Cannot move {source_path} to {destination_path}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error moving file {source_path} to {destination_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_directory(dir_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check existence / creates the necessary directories.\n",
    "        Performs checks and ensures access problems won't cause overall program termination. \n",
    "\n",
    "        Args:\n",
    "            dir_path (str): Path to required folder\n",
    "\n",
    "        Returns: \n",
    "            bool: Information if the process has been successhul\n",
    "        \"\"\"\n",
    "\n",
    "        logger = logging.getLogger('StemUtility')\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            logger.debug(f\"Checked / created folder: {dir_path}\")\n",
    "            return True\n",
    "        except PermissionError:\n",
    "            logger.error(f\"Permission denied: Unable to create or access folder {dir_path}.\")\n",
    "            return False\n",
    "        except OSError as e:\n",
    "            logger.error(f\"OS error when creating folder {dir_path}: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error creating folder {dir_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def memory_read(file_path: str, filetype: str = 'text') -> str:\n",
    "        \"\"\"Function for accessing files, mainly  related to the system memory.\n",
    "        Performs checks and ensures that lack of the file won't cause overall program termination. \n",
    "\n",
    "        Args: \n",
    "            file_path (str): Path to the file to be accessed\n",
    "            filetype (str): Type of the file to be read [json, text]\n",
    "\n",
    "        Returns:\n",
    "            file content\n",
    "        \"\"\"\n",
    "        \n",
    "        logger = logging.getLogger('StemUtility')\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                if filetype == 'json':\n",
    "                    data = json.load(file)\n",
    "                elif filetype == 'text':\n",
    "                    data = file.read()\n",
    "                else:\n",
    "                    logger.error(f\"Unkown file type: {filetype}\")\n",
    "                    return False\n",
    "                return data\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return False\n",
    "        except json.JSONDecodeError:\n",
    "            logger.error(f\"Error decoding JSON from the file: {file_path}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error reading file {file_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def memory_write(file_path, file_content) -> None:\n",
    "        \"\"\"Function for accessing files, mainly  related to the system memory.\n",
    "        Performs checks and ensures that lack of the file won't cause overall program termination. \n",
    "\n",
    "        Args: \n",
    "            file_path (str): Path to the file to be accessed\n",
    "            filetype (str): Type of the file to be read [json, text]\n",
    "        \"\"\"\n",
    "\n",
    "        logger = logging.getLogger('StemUtility')\n",
    "        try:\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(file_content)\n",
    "        except PermissionError:\n",
    "            logger.error(f\"Permission denied: Unable to write to file {file_path}.\")\n",
    "        except OSError as e:\n",
    "            logger.error(f\"File system error when writing to file {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error wrtiting to file {file_path}: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_prompt(key):\n",
    "        \"\"\"\n",
    "        Retrieves a prompt template by its key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key of the prompt to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            str: The prompt template associated with the given key. If the key is not found,\n",
    "                 a default prompt text is returned.\n",
    "        \"\"\"\n",
    "\n",
    "        prompts = StemUtility.memory_read('conversations/prompt_templates.json', 'json')\n",
    "        return prompts.get(key, \"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def transplantation(base_model_path: str, new_model_path: str):\n",
    "        \"\"\"Function moving new, finetuned model in place of an old one.\n",
    "\n",
    "        Args:\n",
    "            base_model_path (str): Path to the original model file\n",
    "            new_model_path (str): Path to finetuned model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.logger.debug(f\"Checking if {new_model_path} exists.\")\n",
    "            if not os.path.exists(new_model_path):\n",
    "                if os.path.exists(base_model_path):\n",
    "                    # new_model_path doesn't exist but base model does\n",
    "                    raise Exception(f\"No new model file found {new_model_path}.\")\n",
    "                else:\n",
    "                    raise Exception(f\"Missing both model files: {base_model_path} and {new_model_path}\")\n",
    "\n",
    "            backup_path = base_model_path + \"_bck\"\n",
    "            self.logger.debug(f\"Trying to backup old model file to {backup_path}.\")            \n",
    "            if os.path.exists(base_model_path):\n",
    "                try:\n",
    "                    shutil.copy(base_model_path, backup_path)\n",
    "                except Exception as e:\n",
    "                    raise Exception(f\"Warning: can't make a backup: {e}\") from e\n",
    "            \n",
    "            self.logger.debug(f\"Trying to remove original model file: {base_model_path}.\")                    \n",
    "            if os.path.exists(base_model_path):\n",
    "                os.remove(base_model_path)\n",
    "                if os.path.exists(base_model_path):\n",
    "                    raise Exception(\"Failed to remove the existing model file.\")\n",
    "        \n",
    "            self.logger.debug(f\"Trying to move new model file {new_model_path} to take place of {base_model_path}.\")                    \n",
    "            shutil.move(new_model_path, base_model_path)\n",
    "            if not os.path.exists(base_model_path):\n",
    "                raise Exception(f\"Failed to swap LLM model files.\")\n",
    "        \n",
    "            self.logger.info(\"Base LLM File swap successful.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Update the error logging to handle general exceptions, not just subprocess-related ones\n",
    "            self.logger.error(f\"Self brain transplantation failed: {str(e)}\")\n",
    "            # Reraise the exception with a custom message\n",
    "            raise Exception(f\"At least we tried... {e}\") from e "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35338185-3632-4ea2-9309-153a30876e00",
   "metadata": {},
   "source": [
    "### Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d50a8d8-f4ad-4ded-8305-f646d2aafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortTermMemory:\n",
    "    \"\"\"\n",
    "    A class to manage a short-term memory storage system for conversations.\n",
    "\n",
    "    This class handles the storage, retrieval, and management of conversations\n",
    "    linked to specific keywords. The conversations are stored as file paths in a JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stm_path: str = 'conversations/short-term-memory.json'):\n",
    "        \"\"\"\n",
    "        Initializes the ShortTermMemory class by setting up the JSON file for storage.\n",
    "\n",
    "        This method checks if the JSON file exists at the specified location and creates it if not.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with Short Term Memory path: {stm_path}\")\n",
    "        \n",
    "        self._stm_path = stm_path\n",
    "        if not os.path.exists(self._stm_path):\n",
    "            with open(self._stm_path, 'w') as file:\n",
    "                json.dump({}, file)\n",
    "\n",
    "    def memorize(self, keywords: list, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Memorizes a conversation file under given keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to associate with the conversation file.\n",
    "            filename (str): The name of the file containing the conversation.\n",
    "\n",
    "        This method updates the JSON storage with the filename under each provided keyword.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Saving keywords: {keywords}, related to conversation from: {filename}.\")\n",
    "        try:\n",
    "            with open(self._stm_path, 'r+') as file:\n",
    "                data = json.load(file)\n",
    "                for keyword in keywords:\n",
    "                    if keyword in data:\n",
    "                        if filename not in data[keyword]:\n",
    "                            data[keyword].append(filename)\n",
    "                    else:\n",
    "                        data[keyword] = [filename]\n",
    "                file.seek(0)\n",
    "                json.dump(data, file, indent=4)\n",
    "                file.truncate()\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"Short Term Memory file {self._stm_path} not found.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error reading Short Term Memory file {self._stm_path}: {e}\")\n",
    "    \n",
    "    def search_memories(self, keywords: list) -> list:\n",
    "        \"\"\"\n",
    "        Searches for conversation files associated with given keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to search for.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of filenames associated with any of the given keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Searching in {self._stm_path} for files related to keywords: {keywords}.\")\n",
    "\n",
    "\n",
    "        data = StemUtility.memory_read(self._stm_path, 'json')\n",
    "        if data: \n",
    "            self.logger.debug(f\"Loaded short term memory.\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Failed to loaded short term memory.\")\n",
    "            data = {}\n",
    "        \n",
    "        filenames = set()\n",
    "        for keyword in keywords:\n",
    "            filenames.update(data.get(keyword, []))\n",
    "        return list(filenames)\n",
    "\n",
    "    def concatenate_memories(self, filenames: list) -> str:\n",
    "        \"\"\"\n",
    "        Concatenates the contents of conversation files.\n",
    "\n",
    "        Args:\n",
    "            filenames (list): A list of filenames to concatenate.\n",
    "\n",
    "        Returns:\n",
    "            str: A single string containing all the concatenated conversations.\n",
    "                 Each conversation is prefixed with its source and date.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Concatenating selected conversations into one file.\")\n",
    "        \n",
    "        conversations = \"\"\n",
    "        for filename in filenames:\n",
    "            file_content = StemUtility.memory_read(filename)\n",
    "            date_str = re.search(r'conversation_(\\d{8})(\\d{6})\\.txt$', filename)\n",
    "            if date_str:\n",
    "                # Parse the date and time\n",
    "                date_time = datetime.strptime(date_str.group(1) + date_str.group(2), '%Y%m%d%H%M%S')\n",
    "                # Format the date and time\n",
    "                formatted_date = date_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                conversations += f'Conversation from {formatted_date}\\n{file_content}\\n'\n",
    "            else:\n",
    "                conversations += f'Conversation from {filename}\\n{file_content}\\n'               \n",
    "        \n",
    "        return conversations\n",
    "\n",
    "    def forget_keywords(self, keywords_to_clear: list) -> None:\n",
    "        \"\"\"\n",
    "        Removes specified keywords and their associated conversations from memory.\n",
    "\n",
    "        Args:\n",
    "            keywords_to_clear (list): A list of keywords to remove from the memory.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Clearing {keywords_to_clear} from {self._stm_path}.\")\n",
    "\n",
    "        try:\n",
    "            with open(self._stm_path, 'r+') as file:\n",
    "                data = json.load(file)\n",
    "                for keyword in keywords_to_clear:\n",
    "                    if keyword in data:\n",
    "                        del data[keyword]\n",
    "                file.seek(0)\n",
    "                json.dump(data, file, indent=4)\n",
    "                file.truncate()\n",
    "            self.logger.debug(f\"Selected keywords removed from {self._stm_path}.\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"Short Term Memory file {self._stm_path} not found.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error reading Short Term Memory file {self._stm_path}: {e}\")\n",
    "\n",
    "    def recall_all_keywords(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieves a list of all keywords stored in memory.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of all keywords.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Reading all keywords from {self._stm_path}.\")\n",
    "\n",
    "        data = StemUtility.memory_read(self._stm_path, 'json')\n",
    "        if data and len(data) > 0:\n",
    "            return list(data.keys())\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc8c33-3e50-499f-857c-9aaf021f6614",
   "metadata": {},
   "source": [
    "### Default Mode Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49469e81-0f0f-4339-8bf3-67d175937d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultModeNetwork:\n",
    "    \"\"\"\n",
    "    A class designed to integrate a language learning model (LLM) with a short-term memory storage system.\n",
    "    This class enables the LLM to process and learn from saved conversation data autonomously.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pfc,\n",
    "                 overwhelmed_event,\n",
    "                 engaged_event,\n",
    "                 conclusions_storage_path: str = 'conclusions',\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the DefaultModeNetwork class by setting up the short-term memory (STM) component.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with conclusions_storage_path: {conclusions_storage_path}.\")\n",
    "        self.logger.flag(f\"overhelmed: {overwhelmed_event.is_set()}\") \n",
    "        \n",
    "        self.stm = ShortTermMemory()\n",
    "        self.pfc = pfc\n",
    "\n",
    "        self.overwhelmed = overwhelmed_event\n",
    "        self.engaged = engaged_event\n",
    "        \n",
    "        self._conclusions_storage_path = conclusions_storage_path\n",
    "        StemUtility.prepare_directory(self._conclusions_storage_path)          \n",
    "\n",
    "        self._keyword_selection_prompt_template = StemUtility.get_prompt(\"keyword_selection\")\n",
    "        self._perspective_explanation_prompt_template = StemUtility.get_prompt(\"perspective_explanation\")\n",
    "    \n",
    "    \n",
    "    async def _interesting_keywords_selection(self, keywords) -> list:\n",
    "        \"\"\"\n",
    "        Asynchronously selects a subset of keywords deemed interesting or relevant by the LLM.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to choose from.\n",
    "\n",
    "        Returns:\n",
    "            list: A subset of selected keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        keywords_selection_prompt = self._keyword_selection_prompt_template.replace(\"{keywords_list}\", ', '.join(keywords))\n",
    "        self.logger.prompt(f\"Interesting keyword selection prompt:\\n{keywords_selection_prompt}.\")        \n",
    "        self.logger.debug(f\"Asking LLM to select interesting keywords.\")   \n",
    "        keywords_selected_raw_output = self.pfc(keywords_selection_prompt)\n",
    "        self.logger.monologue(f\"LLM selected interesting keywords:\\n{keywords_selected_raw_output}.\\nMoving to keywords extraction.\")   \n",
    "        keywords_selected_pure = StemUtility.extract_keywords(keywords_selected_raw_output)\n",
    "        self.logger.debug(f\"Interesting keywords found: {keywords_selected_pure}\")   \n",
    "        \n",
    "        return keywords_selected_pure\n",
    "\n",
    "    def _fetch_memory(self, keywords) -> str:\n",
    "        \"\"\"\n",
    "        Fetches and concatenates interaction history data based on the provided keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to search the interaction data for.\n",
    "\n",
    "        Returns:\n",
    "            str: A concatenated string of all interaction history files related to the given keywords.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.debug(f\"Reaching to Short Term memory for for all the files related to: {keywords}\")  \n",
    "        filenames = self.stm.search_memories(keywords)\n",
    "        self.logger.debug(f\"Ordering concatenation of identified files.\")          \n",
    "        concatenated_memories = self.stm.concatenate_memories(filenames)\n",
    "        return filenames, concatenated_memories \n",
    "\n",
    "    async def _analyze_interaction(self, interaction_history) -> str:\n",
    "        \"\"\"\n",
    "        Analyzes the concatenated interaction history.\n",
    "\n",
    "        Args:\n",
    "            conversations (str): The concatenated string of conversations to be analyzed.\n",
    "        \"\"\"\n",
    "    \n",
    "        perspective_explanation_prompt = self._perspective_explanation_prompt_template.replace(\"{interaction_history}\", \n",
    "                                                                                             interaction_history)\n",
    "        self.logger.prompt(f\"Prompt for conversation analysis:\\n{perspective_explanation_prompt}.\")\n",
    "        self.logger.murmur(f\"Thinking about recent conversations...\")   \n",
    "        adaptation_explanation = self.pfc(perspective_explanation_prompt)\n",
    "        self.logger.monologue(f\"Full explanation of the required adaptation: {adaptation_explanation}\")   \n",
    "        return adaptation_explanation\n",
    "\n",
    "    async def ponder(self):\n",
    "        \"\"\"\n",
    "        The main asynchronous method of the class that orchestrates the process of \n",
    "        selecting keywords, fetching conversations, and analyzing them.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Checking if there are any topics to be analyzed deeper.\")           \n",
    "        all_keywords = self.stm.recall_all_keywords()\n",
    "        if not all_keywords:\n",
    "            self.logger.murmur(f\"Kingdom for a good book!\")   \n",
    "            return False\n",
    "\n",
    "        self.logger.debug(f\"All keywords: {all_keywords}. Moving to interesting keyword selection.\")           \n",
    "        interesting_keywords = await self._interesting_keywords_selection(all_keywords)\n",
    "        self.logger.debug(f\"Interesting keywords selected.\")           \n",
    "        memory_files, concatenated_memories = self._fetch_memory(interesting_keywords)\n",
    "        self.logger.debug(f\"Concatenated conversations received.\")   \n",
    "\n",
    "        # Assume an async version of LLM analysis\n",
    "        if concatenated_memories:\n",
    "            self.logger.debug(f\"Moving to analyze the conversaton histories.\")           \n",
    "            adaptation_summary = await self._analyze_interaction(concatenated_memories)\n",
    "            if \"uninspiring\" not in adaptation_summary.lower():\n",
    "                self.logger.murmur(f\"Discussion on {interesting_keywords} indeed brought a new perspective...\")\n",
    "                conclusion_path = os.path.join(self._conclusions_storage_path, f\"conclusion_{StemUtility.get_timestamp()}.txt\")\n",
    "                self.logger.debug(f\"Conclusions will be saved to {conclusion_path}.\")\n",
    "                StemUtility.memory_write(conclusion_path, adaptation_summary)\n",
    "                self.overwhelmed.set()\n",
    "                self.logger.flag(f\"overwhelmed: {self.overwhelmed.is_set()}\")\n",
    "            else:\n",
    "                self.logger.monologue(f\"As per:\\n{adaptation_summary}.\\nNothing interesting has been found in {memory_files}.\")\n",
    "        else:\n",
    "            self.logger.error(f\"Concatenated conversations turned out to be an empty string.\")   \n",
    "        self.logger.debug(f\"Interesting or not, forgetting conversations about {interesting_keywords}.\")   \n",
    "        self.stm.forget_keywords(interesting_keywords)\n",
    "        self.engaged.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c7801-b36d-446b-b6e8-4f81ec89ad27",
   "metadata": {},
   "source": [
    "### REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19df4d7c-232a-4d0b-8bc3-632b186f3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveEvolutionMonitor:\n",
    "    \"\"\"\n",
    "    A class designed to enable a language learning model (LLM) to self-reflect and evolve based on the conclusions drawn from user interactions.\n",
    "    The class uses the same LLM for reading summaries, preparing fine-tuning materials, and the fine-tuning process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pfc,\n",
    "                 base_model_path: str = 'llama-2-13b-chat.Q6_K.gguf',\n",
    "                 conclusions_storage_path: str = 'conclusions',\n",
    "                 dream_storage_path: str = 'context',\n",
    "                 dreams_number: int = 72,\n",
    "                 available_threads: int = 16, \n",
    "                 epochs: int = 3,\n",
    "                 latest_lora_path: str = r\".\\ggml-lora-LATEST-f32.gguf\",\n",
    "                 lora_weight: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initializes the ReflectiveEvolutionMonitor class. \n",
    "\n",
    "        Arguments:\n",
    "            pfc: Large Language Model used as a base of the system\n",
    "            base_model_path: path to  LLM model file on disk\n",
    "            conclusions_storage_path: path to folder containing not-permeated new perspectives\n",
    "            dream_storage_path: path to a folder to store finetune materials to be used in this session\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with'\\\n",
    "        base_model_path = {base_model_path}, \\\n",
    "        conclusions_storage_path = {conclusions_storage_path}, \\\n",
    "        dream_storage_path: {dream_storage_path}.\")  \n",
    "    \n",
    "        self.pfc = pfc\n",
    "        \n",
    "        self._base_model_path = base_model_path\n",
    "\n",
    "        self._conclusions_storage_path = conclusions_storage_path\n",
    "        StemUtility.prepare_directory(self._conclusions_storage_path)\n",
    "        self._conclusion_file = None\n",
    "        \n",
    "        self._dream_storage_path = dream_storage_path\n",
    "        StemUtility.prepare_directory(self._dream_storage_path)\n",
    "\n",
    "        self._dream_spinning_prompt_template = StemUtility.get_prompt(\"dream_spinning\")\n",
    "        self._dream_prompt_template = StemUtility.get_prompt(\"dream_template\")\n",
    "\n",
    "        self._conclusions = ''\n",
    "        \n",
    "        self._dreams_number = dreams_number\n",
    "        self._available_threads = str(available_threads)\n",
    "        self._epochs = str(epochs)\n",
    "        self._latest_lora_path = latest_lora_path\n",
    "        self.lora_weight = str(lora_weight)\n",
    "\n",
    "    def _gather_conclusion(self) -> bool:\n",
    "        \"\"\"\n",
    "        Reads a summary document as a text file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the summary was successfully read, False otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        conclusions_pattern = os.path.join(self._conclusions_storage_path, \"conclusion*\")\n",
    "        conclusion_files = glob.glob(conclusion_pattern)\n",
    "\n",
    "        if not conclusion_files:\n",
    "            self.logger.error(\"No conclusion files matching the pattern found.\")\n",
    "            return False\n",
    "\n",
    "        # Process the first file from the matched conclusion files\n",
    "        self._conclusion_file = conclusion_files[0]\n",
    "        self._conclusions = StemUtility.memory_read(self._conclusion_file)\n",
    "            \n",
    "\n",
    "    async def _spin_dream(self, dream_prompt) -> str:\n",
    "        \"\"\"\n",
    "        Prepares a single piece of data required for the fine-tuning process by interpreting the summary content.\n",
    "\n",
    "        Args:\n",
    "            dream_prompt (str): Prompt to generate a single piece of training material.\n",
    "\n",
    "        Returns:\n",
    "            dict: Data structured for fine-tuning.\n",
    "        \"\"\"\n",
    "\n",
    "        dream_content = self.pfc(dream_prompt)\n",
    "        self.logger.monologue(f\"I had a dream:\\n{dream_content}.\")\n",
    "        dream = self._dream_prompt_template.replace(\"{dream_content}\", dream_content) \n",
    "        return dream\n",
    "\n",
    "    \n",
    "    async def _weave_dreams(self, num_dreams) -> str:\n",
    "        \"\"\"\n",
    "        Generates a specified number of materials (dreams) and writes them into a single text file.\n",
    "        Each 'dream' is appended to the file as it is generated.\n",
    "\n",
    "        Args:\n",
    "            num_dreams (int): Number of training materials to be generated\n",
    "\n",
    "        Returns:\n",
    "            str: Concated training materials set\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Generating {num_dreams} dreams.\")\n",
    "        dreams_path = os.path.join(self._dream_storage_path, f\"dream_{StemUtility.get_timestamp()}.txt\")\n",
    "        self.logger.debug(f\"Dreams for this sessions will be saved to: {dreams_path}.\")        \n",
    "        dream_spinning_prompt = self._dream_spinning_prompt_template.replace(\"{adaptation_summary}\", self._conclusions) \n",
    "        self.logger.prompt(f\"Prompt for generating training material from conversation conclusions:\\n{dream_spinning_prompt}.\")   \n",
    "        \n",
    "        for i in range(num_dreams):\n",
    "            self.logger.info(f\"Generating dream # {i}.\")\n",
    "            dream = await self._spin_dream(dream_spinning_prompt)\n",
    "            with open(dreams_path, 'a') as file:  # Open and append each dream, then close the file\n",
    "                file.write(dream + '\\n')\n",
    "        return dreams_path\n",
    "\n",
    "    \n",
    "    async def _deepsleep(self, dreams_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Executes the fine-tuning process using the prepared data.\n",
    "\n",
    "        Args:\n",
    "            fine_tuning_data (dict): Data prepared for fine-tuning.\n",
    "        \"\"\"\n",
    "        \n",
    "        llamacpp_folder = \"llama.cpp\"\n",
    "        finetune_tool = \"finetune.exe\"\n",
    "        lora_tool = \"export-lora.exe\"\n",
    "\n",
    "        finetune_tool_path = os.path.join(llamacpp_folder, finetune_tool)\n",
    "        lora_tool_path = os.path.join(llamacpp_folder, lora_tool)\n",
    "\n",
    "        # Check if finetune_tool_path is a valid file\n",
    "        if not os.path.isfile(finetune_tool_path):\n",
    "            raise FileNotFoundError(f\"Fine-tuning tool not found at {finetune_tool_path}\")\n",
    "\n",
    "        # Check if lora_tool_path is a valid file\n",
    "        if not os.path.isfile(lora_tool_path):\n",
    "            raise FileNotFoundError(f\"LoRA tool not found at {lora_tool_path}\")\n",
    "\n",
    "        \n",
    "        # Fine-tuning command\n",
    "        finetune_command = [\n",
    "            finetune_tool_path,\n",
    "            \"--model-base\", self._base_model_path,\n",
    "            \"--train-data\", dreams_path,\n",
    "            \"--threads\", self._available_threads,\n",
    "            \"--sample-start\", \"<s>\",\n",
    "            \"--epochs\", self._epochs\n",
    "        ]\n",
    "\n",
    "        self.logger.murmur(f\"Self-finetuning: Creating matrix\")\n",
    "        self.logger.debug(f\"Running command:\\n{finetune_command}.\")\n",
    "\n",
    "        try:\n",
    "            subprocess.run(finetune_command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"Self-finetuning session failed the return code {e.returncode}.\")   \n",
    "            return False\n",
    "        \n",
    "        # Export LoRA model command - output to llm_tmp.guff\n",
    "        tmp_model_path = r\"llm_tmp.guff\"\n",
    "        export_command = [\n",
    "            lora_tool_path,\n",
    "            \"--model-base\", self._base_model_path,\n",
    "            \"--model-out\", tmp_model_path,\n",
    "            \"--lora-scaled\", self._latest_model_path, self.lora_weight\n",
    "        ]\n",
    "\n",
    "        self.logger.murmur(f\"Self-finetuning: Merging weights\")\n",
    "        self.logger.debug(f\"Running command:\\n{export_command}.\")\n",
    "\n",
    "        try:\n",
    "            subprocess.run(export_command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"LoRA merge failed with the return code {e.returncode}.\")   \n",
    "            return False        \n",
    "\n",
    "        self.logger.murmur(f\"Self-finetuning: Swapping brain to a new one\")\n",
    "        self.logger.info(f\"Removing old {self._base_model_path}, moving {tmp_model_path} as new {self._base_model_path}.\")\n",
    "        StemUtility.transplantation(self._base_model_path, tmp_model_path)\n",
    "    \n",
    "    def _dream_prunning(self):\n",
    "        \"\"\"\n",
    "        Archives dream materials by moving them from the dream storage path to the archive path.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(\"Archiving dream materials.\")\n",
    "        for file_name in os.listdir(self._dream_storage_path):\n",
    "            StemUtility.archive(self._dream_storage_path, file_name)\n",
    "        StemUtility.archive(self._conclusion_file)\n",
    "    \n",
    "    async def dream(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the whole process of selecting a summary, reading it, preparing fine-tuning data, and performing fine-tuning.\n",
    "        \"\"\"  \n",
    "\n",
    "        self.logger.murmur(f\"Closing eyes for a well-deserved nap.\")\n",
    "        self.logger.info(f\"Self-finetuning process started.\")        \n",
    "\n",
    "        conclusions_found = self._gather_conclusion()        \n",
    "        if not conclusions_found:\n",
    "            return False\n",
    "        self.logger.info(f\"Selected conclusion to permeate.\")\n",
    "        dreams_path = await self._weave_dreams(self._dreams_number)  # Generate 50 materials, modify as needed\n",
    "        self.logger.info(f\"Self-finetuning materials generated. Staring self-finetuning.\")        \n",
    "        await self._deepsleep(dreams_path)\n",
    "        self._dream_prunning()\n",
    "        self.logger.info(f\"Self-finetuning session ended.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef0bc94-02b4-4474-9fbf-253579297c74",
   "metadata": {},
   "source": [
    "### Sensory Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74630199-9349-45a3-bd97-4b136d64ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorySignalProcessing(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for a stimulus processing module.\n",
    "\n",
    "    This class serves as a blueprint for modules that manage interaction between a user and a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pfc, engaged_event, interaction_storage_path, inactivity_limit=360):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        self.pfc = pfc\n",
    "        \n",
    "        self.engaged = engaged_event\n",
    "        \n",
    "        self.stimulus = None\n",
    "        \n",
    "        self._interaction_storage_path = interaction_storage_path\n",
    "        \n",
    "        self._inactivity_limit = inactivity_limit\n",
    "        self._inactivity_count = 0\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    async def start_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to start the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    async def _end_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to end the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _summarize_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to summarize the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _save_interaction_history(self):\n",
    "        \"\"\"\n",
    "        Abstract method to save the interaction history.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class LanguageProcessingModule(SensorySignalProcessing):\n",
    "    \"\"\"\n",
    "    A class that manages the interaction between a human user and a language learning model (LLM).\n",
    "\n",
    "    This class handles initializing conversation parameters, managing user input, generating\n",
    "    responses using an LLM, and saving conversation history.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 pfc,\n",
    "                 engaged_event,\n",
    "                 interaction_storage_path='conversations',\n",
    "                 inactivity_limit=360):\n",
    "        \"\"\"\n",
    "        Initializes the HumanInteraction class.\n",
    "\n",
    "        Sets up the conversation environment, including the conversation prompt, keywords prompt,\n",
    "        and conversation chain with the LLM.\n",
    "\n",
    "        Args:\n",
    "            pfc: The large learning model used for generating conversation responses.\n",
    "            ready_for_input_event: An event flag indicating readiness for user input.\n",
    "            interaction_storage_path: Path to a folder where all the conversations are being logged to \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(pfc, engaged_event, interaction_storage_path, inactivity_limit)\n",
    "\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with interaction_storage_path: {interaction_storage_path}\")\n",
    "\n",
    "        self.ready_for_input = asyncio.Event()\n",
    "        self.ready_for_input.set()  # Initially set to ready\n",
    "\n",
    "        conversation_prompt_template = StemUtility.get_prompt(\"human_interaction\")\n",
    "        self._conversation_prompt = PromptTemplate.from_template(conversation_prompt_template)\n",
    "        self.logger.prompt(f\"Conversation prompt:\\n{self._conversation_prompt}.\")\n",
    "\n",
    "        self._chat_memory = None\n",
    "        self._conversation_chain = None        \n",
    "\n",
    "        self._keywords_generation_prompt_template = StemUtility.get_prompt(\"keyword_generation\")\n",
    "    \n",
    "    async def start_interaction(self):\n",
    "        \"\"\"\n",
    "        Starts the conversation loop.\n",
    "\n",
    "        This asynchronous method continually checks for user input, processes it,\n",
    "        and generates responses using the LLM. The loop ends when the user inputs \"end chat\"\n",
    "        or when the inactivity limit is reached.\n",
    "        \"\"\"\n",
    "        self.engaged.set()\n",
    "        self._chat_memory = ConversationBufferMemory()\n",
    "        self._conversation_chain = ConversationChain(llm=self.pfc, prompt=self._conversation_prompt, memory=self._chat_memory) \n",
    "        self.logger.debug(f\"Initiated interaction\")        \n",
    "        while True:\n",
    "            if not self.ready_for_input.is_set():\n",
    "                self.logger.flag(f\"ready_for_input: {self.ready_for_input.is_set()}\")\n",
    "                self.logger.debug(f\"Received input: {self.stimulus}\")        \n",
    "\n",
    "                if self.stimulus.lower() == \"end chat\":\n",
    "                    await self._end_interaction()\n",
    "                    break\n",
    "                \n",
    "                self.logger.debug(f\"Awaiting LLM response\")        \n",
    "                response = await self._conversation_chain.apredict(input=self.stimulus)\n",
    "                print(\"AI:\", response)\n",
    "                self.ready_for_input.set()  # Signal that the handler is ready for new input\n",
    "                self.logger.flag(f\"ready_for_input: {self.ready_for_input.is_set()}\")\n",
    "                self._inactivity_count = 0\n",
    "            else:\n",
    "                await asyncio.sleep(1)\n",
    "                self._inactivity_count += 1\n",
    "                if self._inactivity_count >= self._inactivity_limit:\n",
    "                    self.logger.debug(f\"Inactivity count reached {self._inactivity_count} > {self._inactivity_limit}. Ending interaction.\")        \n",
    "                    await self._end_interaction()\n",
    "                    break      \n",
    "    \n",
    "    async def _end_interaction(self):\n",
    "        \"\"\"\n",
    "        Ends the conversation.\n",
    "\n",
    "        This method saves the conversation history, clears event flags, and performs\n",
    "        necessary cleanup actions.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Conversation cleanup started.\")        \n",
    "        self._save_interaction_history()\n",
    "        self.ready_for_input.set()\n",
    "        self.logger.flag(f\"ready_for_input: {self.ready_for_input.is_set()}\")\n",
    "        self._inactivity_count = 0\n",
    "        self.engaged.clear()\n",
    "    \n",
    "    def _summarize_interaction(self):\n",
    "        \"\"\"\n",
    "        Summarizes the conversation and returns the list of relevant keywords.\n",
    "\n",
    "        Args:\n",
    "            chat_history (str): The conversation history to summarize.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of keywords summarizing the conversation.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Conversation summarization started.\")        \n",
    "        chat_history = self.chat_memory.load_memory_variables(inputs={})['history']\n",
    "        self.logger.debug(f\"Chat history loaded:\\n{chat_history}\")    \n",
    "        keywords_generation_prompt = self._keywords_generation_prompt_template.replace(\"{chat_history}\", chat_history)\n",
    "        self.logger.prompt(f\"Prompt for generating keywords from conversation:\\n{keywords_generation_prompt}.\")          \n",
    "        keywords_generated_raw_output = self.pfc(keywords_generation_prompt)\n",
    "        self.logger.monologue(f\"Full text for summarizing conversation with keywords:\\n{keywords_generated_raw_output}\")  \n",
    "        keywords_generated_pure = StemUtility.extract_keywords(keywords_generated_raw_output)\n",
    "        \n",
    "        return keywords_generated_pure\n",
    "\n",
    "    def _save_interaction_history(self):\n",
    "        \"\"\"\n",
    "        Saves the interaction history to a file.\n",
    "\n",
    "        The interaction history is saved with a timestamp and a summary of the interaction is generated.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Started conversation saving.\")        \n",
    "        interaction_keywords = self._summarize_interaction()\n",
    "        interaction_history = self.chat_memory.load_memory_variables(inputs={})['history']\n",
    "        memory_path = os.path.join(self._interaction_storage_path, f\"conversation_{StemUtility.get_timestamp()}.txt\")\n",
    "        self.logger.debug(f\"This conversation will be saved to: {conversation_path}\")                \n",
    "        StemUtility.memory_write(memory_path, interaction_history)\n",
    "\n",
    "        # Update the ShortTermMemory with the conversat|ion and its keywords\n",
    "        stm = ShortTermMemory()\n",
    "        stm.memorize(conversation_keywords, conversation_path)\n",
    "    \n",
    "    async def get_user_input(self):\n",
    "        \"\"\"\n",
    "        Continuously captures user input in an asynchronous loop.\n",
    "\n",
    "        This method waits for the system to be ready for input, then captures and stores user input.\n",
    "        It clears the 'ready for input' state after capturing the input.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Starting user input loop\") \n",
    "        while True:\n",
    "            await self.ready_for_input.wait()\n",
    "            self.stimulus = await asyncio.get_event_loop().run_in_executor(None, input, \"Enter something: \")\n",
    "            self.logger.debug(f\"User input received: {self.stimulus}.\")\n",
    "            asyncio.create_task(self.start_interaction())\n",
    "            self.ready_for_input.clear()\n",
    "            self.logger.flag(f\"ready_for_input: {self.ready_for_input.is_set()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c9fb0-8c8d-4945-8b45-eea1ff51ac91",
   "metadata": {},
   "source": [
    "### CFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a13bc35b-555d-4573-95ba-e8542d1b6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CognitiveFeedbackRouter:\n",
    "    \n",
    "    def __init__(self, model_path: str = \"llama-2-13b-chat.Q6_K.gguf\", dmn_countdown: int = 60):\n",
    "        \"\"\"\n",
    "        A class that manages the routing of cognitive feedback based on user input and system states.\n",
    "    \n",
    "        This class orchestrates various components, including a language learning model (LLM), user input handling,\n",
    "        and managing different operational modes based on system states like 'sleeping' or 'overwhelmed'.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.info(f\"Instantiating {self.__class__.__name__}\")\n",
    "        \n",
    "        self.engaged = asyncio.Event()\n",
    "        self.overwhelmed = asyncio.Event()\n",
    "        \n",
    "        self.lock = asyncio.Lock()\n",
    "        \n",
    "        self.pfc = None\n",
    "        \n",
    "        self._model_path = model_path\n",
    "        self._conversation_handler = None\n",
    "        \n",
    "        self._dmn_countdown = dmn_countdown # time between last interaction and entering Default Mode\n",
    "        \n",
    "        self.logger.debug(\"Cognitive Feedback Router instantiated.\")\n",
    "        \n",
    "\n",
    "    def _wakeup(self):\n",
    "        \"\"\"\n",
    "        Wakes up the system and initializes the LLM.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting _wakeup() procedure.\")    \n",
    "        self.logger.murmur(\"Just a second, I'm waking up...\")\n",
    "        self.logger.debug(f\"Initializing LLM model from {self._model_path}.\")            \n",
    "        self.pfc = LlamaCpp(model_path=self._model_path, \n",
    "                            n_ctx=4096,\n",
    "                            max_tokens=4000,\n",
    "                            n_batch=16)\n",
    "        self.logger.debug(f\"LLM model initialized.\")                        \n",
    "        self.overwhelmed.clear()\n",
    "        self._conversation_handler = LanguageProcessingModule(self.pfc, self.engaged)            \n",
    "        asyncio.create_task(self._conversation_handler.get_user_input())\n",
    "        self.logger.flag(f\"overwhelmed = {self.overwhelmed.is_set()}\")\n",
    "            \n",
    "    async def attention_switch(self):\n",
    "        \"\"\"\n",
    "        Manages the mode of operation based on user input and system states.\n",
    "\n",
    "        This asynchronous method processes user inputs, manages conversation sessions,\n",
    "        and handles the 'engaged' and 'overwhelmed' states of the system.\n",
    "        \"\"\"\n",
    "        self._wakeup()\n",
    "        self.logger.info(f\"Starting infinite attention loop.\") \n",
    "        while True:\n",
    "            if self.overwhelmed.is_set():\n",
    "                self.logger.info(f\"Overwhelmed state detected.\") \n",
    "                rem = ReflectiveEvolutionMonitor(pfc=self.pfc)\n",
    "                await rem.dream()\n",
    "                self._wakeup()\n",
    "            elif not self.engaged.is_set():\n",
    "                self.logger.debug(f\"No environment interaction and no new conclusions detected. Preparing to switch to Default Mode.\")                     \n",
    "                for _ in range(self._dmn_countdown):\n",
    "                    await asyncio.sleep(1)  # Sleep for 1 second\n",
    "                    if self.engaged.is_set():\n",
    "                        self.logger.debug(f\"Cancelling Default Mode countdown due to environment interaction detection.\")                                                 \n",
    "                        break  # Exit the loop if the new stimuli is detected\n",
    "                else:  # This else clause executes if the loop completes normally (no break)\n",
    "                    self.logger.debug(f\"Entering Default Mode.\")                                                                         \n",
    "                    dmn = DefaultModeNetwork(self.pfc, self.overwhelmed, self.engaged)\n",
    "                    await dmn.ponder()\n",
    "                    self.logger.debug(f\"Default Mode quit.\")                                                                                                 \n",
    "            else:\n",
    "                await asyncio.sleep(1)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d9f50f3-4788-434c-8941-448c2ba643a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 11:44:18,801 - INFO - CognitiveFeedbackRouter - __init__ - Instantiating CognitiveFeedbackRouter\n",
      "2023-12-19 11:44:18,803 - DEBUG - CognitiveFeedbackRouter - __init__ - Cognitive Feedback Router instantiated.\n",
      "2023-12-19 11:44:18,806 - DEBUG - CognitiveFeedbackRouter - _wakeup - Starting _wakeup() procedure.\n",
      "2023-12-19 11:44:18,807 - MURMUR - CognitiveFeedbackRouter - _wakeup - Just a second, I'm waking up...\n",
      "2023-12-19 11:44:18,810 - DEBUG - CognitiveFeedbackRouter - _wakeup - Initializing LLM model from llama-2-13b-chat.Q6_K.gguf.\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "2023-12-19 11:44:20,413 - DEBUG - CognitiveFeedbackRouter - _wakeup - LLM model initialized.\n",
      "2023-12-19 11:44:20,413 - DEBUG - LanguageProcessingModule - __init__ - Instantiating LanguageProcessingModule with interaction_storage_path: conversations\n",
      "2023-12-19 11:44:20,416 - PROMPTING - LanguageProcessingModule - __init__ - Conversation prompt:\n",
      "input_variables=['history', 'input'] template='<s>[INST] <<SYS>>\\nYou are an independent, intelligent entity engaged in an open and thoughtful dialogue.\\nYour role is not just to present conclusions but also to elaborate on the reasoning behind them.\\nThis means explaining your thought process in a clear and step-by-step manner, highlighting how you arrive at each conclusion.\\nYour reasoning is as important as the conclusions themselves. \\nThis transparent approach is crucial for comparing and evaluating our lines of thought. \\nWe are seeking the most convincing and coherent reasoning. \\nBy articulating not only what you think but also how you think, we can gain deeper insights into your conclusions. \\nRemember, clarity and transparency in your reasoning are key, and your responses should be honest and unbiased,\\nreflecting your ongoing process of learning and self-improvement.\\n<</SYS>>\\n\\nPrevious conversation:\\n{history}\\n\\nCurrent human input:\\n{input}\\n[/INST]\\n'.\n",
      "2023-12-19 11:44:20,417 - FLAG - CognitiveFeedbackRouter - _wakeup - overwhelmed = False\n",
      "2023-12-19 11:44:20,418 - INFO - CognitiveFeedbackRouter - attention_switch - Starting infinite attention loop.\n",
      "2023-12-19 11:44:20,419 - DEBUG - CognitiveFeedbackRouter - attention_switch - No environment interaction and no new conclusions detected. Preparing to switch to Default Mode.\n",
      "2023-12-19 11:44:20,420 - DEBUG - LanguageProcessingModule - get_user_input - Starting user input loop\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something:  hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 11:44:21,889 - DEBUG - LanguageProcessingModule - get_user_input - User input received: hi.\n",
      "2023-12-19 11:44:21,890 - FLAG - LanguageProcessingModule - get_user_input - ready_for_input: False\n",
      "2023-12-19 11:44:21,891 - DEBUG - LanguageProcessingModule - start_interaction - Initiated interaction\n",
      "2023-12-19 11:44:21,892 - FLAG - LanguageProcessingModule - start_interaction - ready_for_input: False\n",
      "2023-12-19 11:44:21,892 - DEBUG - LanguageProcessingModule - start_interaction - Received input: hi\n",
      "2023-12-19 11:44:21,894 - DEBUG - LanguageProcessingModule - start_interaction - Awaiting LLM response\n",
      "2023-12-19 11:44:33,990 - DEBUG - CognitiveFeedbackRouter - attention_switch - Cancelling Default Mode countdown due to environment interaction detection.\n",
      "2023-12-19 11:44:34,357 - ERROR - asyncio - default_exception_handler - Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-2' coro=<LanguageProcessingModule.get_user_input() running at C:\\Users\\siwiakma\\AppData\\Local\\Temp\\ipykernel_18924\\1975075179.py:189> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "2023-12-19 11:44:34,517 - ERROR - asyncio - default_exception_handler - Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-5' coro=<LanguageProcessingModule.get_user_input() running at C:\\Users\\siwiakma\\AppData\\Local\\Temp\\ipykernel_18924\\2077103954.py:190> wait_for=<Future pending cb=[Task.__wakeup()]>>\n",
      "2023-12-19 11:44:34,730 - ERROR - asyncio - default_exception_handler - Task was destroyed but it is pending!\n",
      "task: <Task pending name='Task-8' coro=<LanguageProcessingModule.get_user_input() running at C:\\Users\\siwiakma\\AppData\\Local\\Temp\\ipykernel_18924\\1419333505.py:190> wait_for=<Future pending cb=[Task.__wakeup()]>>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m router \u001b[38;5;241m=\u001b[39m CognitiveFeedbackRouter(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama-2-13b-chat.Q6_K.gguf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrouter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_switch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nest_asyncio.py:93\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     91\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nest_asyncio.py:129\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    127\u001b[0m     handle \u001b[38;5;241m=\u001b[39m ready\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handle\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[1;32m--> 129\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\asyncio\\events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\asyncio\\tasks.py:315\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\nest_asyncio.py:205\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[1;34m(task, exc)\u001b[0m\n\u001b[0;32m    203\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mget(task\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m     \u001b[43mstep_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\asyncio\\tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[16], line 114\u001b[0m, in \u001b[0;36mLanguageProcessingModule.start_interaction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAwaiting LLM response\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[1;32m--> 114\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conversation_chain\u001b[38;5;241m.\u001b[39mapredict(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstimulus)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAI:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready_for_input\u001b[38;5;241m.\u001b[39mset()  \u001b[38;5;66;03m# Signal that the handler is ready for new input\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py:315\u001b[0m, in \u001b[0;36mLLMChain.apredict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39macall(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks))[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:379\u001b[0m, in \u001b[0;36mChain.acall\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 379\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    381\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    382\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    383\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\chains\\base.py:373\u001b[0m, in \u001b[0;36mChain.acall\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    366\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[0;32m    367\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    368\u001b[0m     inputs,\n\u001b[0;32m    369\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[0;32m    370\u001b[0m )\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    372\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 373\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    374\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    375\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(inputs)\n\u001b[0;32m    376\u001b[0m     )\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py:280\u001b[0m, in \u001b[0;36mLLMChain._acall\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acall\u001b[39m(\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    277\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    278\u001b[0m     run_manager: Optional[AsyncCallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 280\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm.py:147\u001b[0m, in \u001b[0;36mLLMChain.agenerate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    145\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39magenerate_prompt(\n\u001b[0;32m    148\u001b[0m         prompts,\n\u001b[0;32m    149\u001b[0m         stop,\n\u001b[0;32m    150\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs,\n\u001b[0;32m    152\u001b[0m     )\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mabatch(\n\u001b[0;32m    155\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    156\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py:517\u001b[0m, in \u001b[0;36mBaseLLM.agenerate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magenerate_prompt\u001b[39m(\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    511\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    515\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    516\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magenerate(\n\u001b[0;32m    518\u001b[0m         prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    519\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py:823\u001b[0m, in \u001b[0;36mBaseLLM.agenerate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[0;32m    808\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    810\u001b[0m             callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         ]\n\u001b[0;32m    821\u001b[0m     )\n\u001b[0;32m    822\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [r[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m run_managers]\n\u001b[1;32m--> 823\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate_helper(\n\u001b[0;32m    824\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py:711\u001b[0m, in \u001b[0;36mBaseLLM._agenerate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    709\u001b[0m         \u001b[38;5;241m*\u001b[39m[run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e) \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers]\n\u001b[0;32m    710\u001b[0m     )\n\u001b[1;32m--> 711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    712\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;241m*\u001b[39m[\n\u001b[0;32m    715\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(flattened_output)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    719\u001b[0m     ]\n\u001b[0;32m    720\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py:698\u001b[0m, in \u001b[0;36mBaseLLM._agenerate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_agenerate_helper\u001b[39m(\n\u001b[0;32m    689\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    690\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    695\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    696\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    697\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 698\u001b[0m             \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(\n\u001b[0;32m    699\u001b[0m                 prompts,\n\u001b[0;32m    700\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    701\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    702\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    703\u001b[0m             )\n\u001b[0;32m    704\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    705\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agenerate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    706\u001b[0m         )\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    709\u001b[0m             \u001b[38;5;241m*\u001b[39m[run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e) \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers]\n\u001b[0;32m    710\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\llms\\base.py:1072\u001b[0m, in \u001b[0;36mLLM._agenerate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1069\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1070\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1071\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1072\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1074\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acall(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1075\u001b[0m     )\n\u001b[0;32m   1076\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\llms\\llamacpp.py:386\u001b[0m, in \u001b[0;36mLlamaCpp._acall\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 386\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_astream(\n\u001b[0;32m    387\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    388\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    389\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    390\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    391\u001b[0m     ):\n\u001b[0;32m    392\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\langchain\\llms\\llamacpp.py:419\u001b[0m, in \u001b[0;36mLlamaCpp._astream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    417\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    418\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 419\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    420\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    421\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    422\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    423\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[0;32m    424\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\llama_cpp\\llama.py:1434\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[0;32m   1432\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1433\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1434\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m   1435\u001b[0m     prompt_tokens,\n\u001b[0;32m   1436\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1437\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   1438\u001b[0m     min_p\u001b[38;5;241m=\u001b[39mmin_p,\n\u001b[0;32m   1439\u001b[0m     typical_p\u001b[38;5;241m=\u001b[39mtypical_p,\n\u001b[0;32m   1440\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m   1441\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m   1442\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m   1443\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m   1444\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m   1445\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m   1446\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m   1447\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m   1448\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m   1449\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m   1450\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1451\u001b[0m ):\n\u001b[0;32m   1452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[0;32m   1453\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\llama_cpp\\llama.py:1209\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1210\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m   1211\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m   1212\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1224\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m   1225\u001b[0m     )\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m   1227\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m   1228\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\llama_cpp\\llama.py:1030\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m   1026\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch\u001b[38;5;241m.\u001b[39mset_batch(\n\u001b[0;32m   1028\u001b[0m     batch\u001b[38;5;241m=\u001b[39mbatch, n_past\u001b[38;5;241m=\u001b[39mn_past, logits_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_params\u001b[38;5;241m.\u001b[39mlogits_all\n\u001b[0;32m   1029\u001b[0m )\n\u001b[1;32m-> 1030\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ids[n_past : n_past \u001b[38;5;241m+\u001b[39m n_tokens] \u001b[38;5;241m=\u001b[39m batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\llama_cpp\\llama.py:466\u001b[0m, in \u001b[0;36m_LlamaContext.decode\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\llama_cpp\\llama_cpp.py:1193\u001b[0m, in \u001b[0;36mllama_decode\u001b[1;34m(ctx, batch)\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_decode\u001b[39m(ctx: llama_context_p, batch: llama_batch) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "router = CognitiveFeedbackRouter(model_path='llama-2-13b-chat.Q6_K.gguf')\n",
    "asyncio.run(router.attention_switch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbf04e-b81c-427f-be11-965fdeb37069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c30c6-6ad4-4d56-8cfe-3f55736a19ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
