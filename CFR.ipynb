{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d881d3-c7d8-483d-b9e7-facd3aa787cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "336f6807-a53e-4844-bdbc-db27e5c5c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_custom_log_levels():\n",
    "    # Define custom logging levels\n",
    "    MURMUR_LEVEL_NUM = 39\n",
    "    logging.addLevelName(MURMUR_LEVEL_NUM, \"MURMUR\")\n",
    "    def log_murmur(self, message, *args, **kwargs):\n",
    "        \"\"\" \n",
    "        Logging of PFC actions.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isEnabledFor(MURMUR_LEVEL_NUM):\n",
    "            self._log(MURMUR_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.murmur = log_murmur\n",
    "\n",
    "    FLAG_LEVEL_NUM = 13\n",
    "    logging.addLevelName(FLAG_LEVEL_NUM, \"FLAG\")\n",
    "    def log_flag(self, message, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Logging of status flags statuses.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isEnabledFor(FLAG_LEVEL_NUM):\n",
    "            self._log(FLAG_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.flag = log_flag\n",
    "\n",
    "    PROMPTING_LEVEL_NUM = 12\n",
    "    logging.addLevelName(PROMPTING_LEVEL_NUM, \"PROMPTING\")\n",
    "    def log_prompting(self, message, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Logging of complete prompt messages sent to PFC. \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isEnabledFor(PROMPTING_LEVEL_NUM):\n",
    "            self._log(PROMPTING_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.prompt = log_prompting\n",
    "\n",
    "    MONOLOGUE_LEVEL_NUM = 11\n",
    "    logging.addLevelName(MONOLOGUE_LEVEL_NUM, \"MONOLOGUE\")\n",
    "    def log_monologue(self, message, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Logging of PFC internal monologue.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.isEnabledFor(MONOLOGUE_LEVEL_NUM):\n",
    "            self._log(MONOLOGUE_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.monologue = log_monologue\n",
    "\n",
    "def setup_logging(file_log_level: int = 10, console_log_level: int = 10):\n",
    "    setup_custom_log_levels()\n",
    "\n",
    "    # Create a file handler for logging\n",
    "    log_directory = \"console\"\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file_name = f\"{log_directory}/file_{current_time}.log\"\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file_name)\n",
    "    file_handler.setLevel(file_log_level)\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(console_log_level)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(min([file_log_level, console_log_level]))  # Set the lowest overall level to log\n",
    "\n",
    "    # Clear existing handlers (if any), and then add new handlers\n",
    "    if root_logger.hasHandlers():\n",
    "        root_logger.handlers.clear()\n",
    "    root_logger.addHandler(file_handler)\n",
    "    root_logger.addHandler(console_handler)\n",
    "\n",
    "# Call the setup function\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639dc99-a78d-445b-9dc9-41e1bab80c8c",
   "metadata": {},
   "source": [
    "### Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7eeff60-0d0a-46d2-935f-da4cc40889d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemUtility:\n",
    "    \"\"\"\n",
    "    A class for managing and retrieving predefined prompts.\n",
    "\n",
    "    This class stores a collection of utility functions utilized by other system classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_keywords(raw_output):\n",
    "        \"\"\"\n",
    "        Extracts keywords from the summary output.\n",
    "\n",
    "        Args:\n",
    "            raw_output (str): The output from which to extract keywords.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of extracted keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        # Regex pattern to find all occurrences of words flanked by **\n",
    "        pattern = r\"\\*\\*(.*?)\\*\\*\"\n",
    "        # Find all matches and strip the ** from each keyword\n",
    "        keywords = [keyword.lower() for keyword in re.findall(pattern, raw_output)]\n",
    "        return keywords\n",
    "\n",
    "    @staticmethod\n",
    "    def get_timestamp():\n",
    "        return datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    @staticmethod\n",
    "    def archive(source_dir: str, source_file: str = None, archive_suffix='archive'):\n",
    "        \"\"\"\n",
    "        Moves processed file to the respective archive folder.\n",
    "\n",
    "        Args:\n",
    "            source_path (str): The source folder\n",
    "            source_file (str): The file to be archived\n",
    "\n",
    "        Returns:\n",
    "            bool: Information if the process has been successful\n",
    "        \"\"\"\n",
    "\n",
    "        if not source_file:\n",
    "            source_path = source_dir\n",
    "            source_dir, source_file = os.path.split(source_dir)\n",
    "        else:\n",
    "            source_path = os.path.join(source_dir, source_file)\n",
    "            \n",
    "        \n",
    "        destination_dir = '_'.join(source_path, archive_suffix)\n",
    "        directories_available = StemUtility.prepare_directory(destination_dir)\n",
    "        if not directories_available:\n",
    "            return False\n",
    "        \n",
    "        destination_path = os.path.join(destination_dir, source_file)\n",
    "        \n",
    "        logger = logging.getLogger('StemUtility')        \n",
    "        try:\n",
    "            shutil.move(source_path, destination_path)\n",
    "            return True\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {source_path}\")\n",
    "            return False\n",
    "        except PermissionError:\n",
    "            logger.error(f\"Permission denied: Cannot move {source_path} to {destination_path}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error moving file {source_path} to {destination_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def prepare_directory(dir_path: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check existence / creates the necessary directories.\n",
    "        Performs checks and ensures access problems won't cause overall program termination. \n",
    "\n",
    "        Args:\n",
    "            dir_path (str): Path to required folder\n",
    "\n",
    "        Returns: \n",
    "            bool: Information if the process has been successhul\n",
    "        \"\"\"\n",
    "\n",
    "        logger = logging.getLogger('StemUtility')\n",
    "        \n",
    "        try:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            logger.debug(f\"Checked / created folder: {dir_path}\")\n",
    "            return True\n",
    "        except PermissionError:\n",
    "            logger.error(f\"Permission denied: Unable to create or access folder {dir_path}.\")\n",
    "            return False\n",
    "        except OSError as e:\n",
    "            logger.error(f\"OS error when creating folder {dir_path}: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error creating folder {dir_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def memory_read(file_path: str, filetype: str = 'text') -> str:\n",
    "        \"\"\"Function for accessing files, mainly  related to the system memory.\n",
    "        Performs checks and ensures that lack of the file won't cause overall program termination. \n",
    "\n",
    "        Args: \n",
    "            file_path (str): Path to the file to be accessed\n",
    "            filetype (str): Type of the file to be read [json, text]\n",
    "\n",
    "        Returns:\n",
    "            file content\n",
    "        \"\"\"\n",
    "        \n",
    "        logger = logging.getLogger('StemUtility')\n",
    "        try:\n",
    "            with open(file_path, 'r') as file:\n",
    "                if filetype == 'json':\n",
    "                    data = json.load(file)\n",
    "                elif filetype == 'text':\n",
    "                    data = file.read()\n",
    "                else:\n",
    "                    logger.error(f\"Unkown file type: {filetype}\")\n",
    "                    return False\n",
    "                return data\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {file_path}\")\n",
    "            return False\n",
    "        except json.JSONDecodeError:\n",
    "            logger.error(f\"Error decoding JSON from the file: {file_path}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error reading file {file_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def memory_write(file_path, file_content) -> None:\n",
    "        \"\"\"Function for accessing files, mainly  related to the system memory.\n",
    "        Performs checks and ensures that lack of the file won't cause overall program termination. \n",
    "\n",
    "        Args: \n",
    "            file_path (str): Path to the file to be accessed\n",
    "            filetype (str): Type of the file to be read [json, text]\n",
    "        \"\"\"\n",
    "\n",
    "        logger = logging.getLogger('StemUtility')\n",
    "        try:\n",
    "            with open(file_path, \"w\") as file:\n",
    "                file.write(file_content)\n",
    "        except PermissionError:\n",
    "            logger.error(f\"Permission denied: Unable to write to file {file_path}.\")\n",
    "        except OSError as e:\n",
    "            logger.error(f\"File system error when writing to file {file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error wrtiting to file {file_path}: {e}\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_prompt(key):\n",
    "        \"\"\"\n",
    "        Retrieves a prompt template by its key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key of the prompt to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            str: The prompt template associated with the given key. If the key is not found,\n",
    "                 a default prompt text is returned.\n",
    "        \"\"\"\n",
    "\n",
    "        prompts = StemUtility.memory_read('conversations/prompt_templates.json', 'json')\n",
    "        return prompts.get(key, \"\")\n",
    "\n",
    "    @staticmethod\n",
    "    def transplantation(base_model_path: str, new_model_path: str):\n",
    "        \"\"\"Function moving new, finetuned model in place of an old one.\n",
    "\n",
    "        Args:\n",
    "            base_model_path (str): Path to the original model file\n",
    "            new_model_path (str): Path to finetuned model\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            self.logger.debug(f\"Checking if {new_model_path} exists.\")\n",
    "            if not os.path.exists(new_model_path):\n",
    "                if os.path.exists(base_model_path):\n",
    "                    # new_model_path doesn't exist but base model does\n",
    "                    raise Exception(f\"No new model file found {new_model_path}.\")\n",
    "                else:\n",
    "                    raise Exception(f\"Missing both model files: {base_model_path} and {new_model_path}\")\n",
    "\n",
    "            backup_path = base_model_path + \"_bck\"\n",
    "            self.logger.debug(f\"Trying to backup old model file to {backup_path}.\")            \n",
    "            if os.path.exists(base_model_path):\n",
    "                try:\n",
    "                    shutil.copy(base_model_path, backup_path)\n",
    "                except Exception as e:\n",
    "                    raise Exception(f\"Warning: can't make a backup: {e}\") from e\n",
    "            \n",
    "            self.logger.debug(f\"Trying to remove original model file: {base_model_path}.\")                    \n",
    "            if os.path.exists(base_model_path):\n",
    "                os.remove(base_model_path)\n",
    "                if os.path.exists(base_model_path):\n",
    "                    raise Exception(\"Failed to remove the existing model file.\")\n",
    "        \n",
    "            self.logger.debug(f\"Trying to move new model file {new_model_path} to take place of {base_model_path}.\")                    \n",
    "            shutil.move(new_model_path, base_model_path)\n",
    "            if not os.path.exists(base_model_path):\n",
    "                raise Exception(f\"Failed to swap LLM model files.\")\n",
    "        \n",
    "            self.logger.info(\"Base LLM File swap successful.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Update the error logging to handle general exceptions, not just subprocess-related ones\n",
    "            self.logger.error(f\"Self brain transplantation failed: {str(e)}\")\n",
    "            # Reraise the exception with a custom message\n",
    "            raise Exception(f\"At least we tried... {e}\") from e "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35338185-3632-4ea2-9309-153a30876e00",
   "metadata": {},
   "source": [
    "### Short-Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d50a8d8-f4ad-4ded-8305-f646d2aafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortTermMemory:\n",
    "    \"\"\"\n",
    "    A class to manage a short-term memory storage system for conversations.\n",
    "\n",
    "    This class handles the storage, retrieval, and management of conversations\n",
    "    linked to specific keywords. The conversations are stored as file paths in a JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stm_path: str = 'conversations/short-term-memory.json'):\n",
    "        \"\"\"\n",
    "        Initializes the ShortTermMemory class by setting up the JSON file for storage.\n",
    "\n",
    "        This method checks if the JSON file exists at the specified location and creates it if not.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with Short Term Memory path: {stm_path}\")\n",
    "        \n",
    "        self._stm_path = stm_path\n",
    "        if not os.path.exists(self._stm_path):\n",
    "            with open(self._stm_path, 'w') as file:\n",
    "                json.dump({}, file)\n",
    "\n",
    "    def memorize(self, keywords: list, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Memorizes a conversation file under given keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to associate with the conversation file.\n",
    "            filename (str): The name of the file containing the conversation.\n",
    "\n",
    "        This method updates the JSON storage with the filename under each provided keyword.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Saving keywords: {keywords}, related to conversation from: {filename}.\")\n",
    "        try:\n",
    "            with open(self._stm_path, 'r+') as file:\n",
    "                data = json.load(file)\n",
    "                for keyword in keywords:\n",
    "                    if keyword in data:\n",
    "                        if filename not in data[keyword]:\n",
    "                            data[keyword].append(filename)\n",
    "                    else:\n",
    "                        data[keyword] = [filename]\n",
    "                file.seek(0)\n",
    "                json.dump(data, file, indent=4)\n",
    "                file.truncate()\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"Short Term Memory file {self._stm_path} not found.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error reading Short Term Memory file {self._stm_path}: {e}\")\n",
    "    \n",
    "    def search_memories(self, keywords: list) -> list:\n",
    "        \"\"\"\n",
    "        Searches for conversation files associated with given keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to search for.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of filenames associated with any of the given keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Searching in {self._stm_path} for files related to keywords: {keywords}.\")\n",
    "\n",
    "\n",
    "        data = StemUtility.memory_read(self._stm_path, 'json')\n",
    "        if data: \n",
    "            self.logger.debug(f\"Loaded short term memory.\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Failed to loaded short term memory.\")\n",
    "            data = {}\n",
    "        \n",
    "        filenames = set()\n",
    "        for keyword in keywords:\n",
    "            filenames.update(data.get(keyword, []))\n",
    "        return list(filenames)\n",
    "\n",
    "    def concatenate_memories(self, filenames: list) -> str:\n",
    "        \"\"\"\n",
    "        Concatenates the contents of conversation files.\n",
    "\n",
    "        Args:\n",
    "            filenames (list): A list of filenames to concatenate.\n",
    "\n",
    "        Returns:\n",
    "            str: A single string containing all the concatenated conversations.\n",
    "                 Each conversation is prefixed with its source and date.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Concatenating selected conversations into one file.\")\n",
    "        \n",
    "        conversations = \"\"\n",
    "        for filename in filenames:\n",
    "            file_content = StemUtility.memory_read(filename)\n",
    "            date_str = re.search(r'conversation_(\\d{8})(\\d{6})\\.txt$', filename)\n",
    "            if date_str:\n",
    "                # Parse the date and time\n",
    "                date_time = datetime.strptime(date_str.group(1) + date_str.group(2), '%Y%m%d%H%M%S')\n",
    "                # Format the date and time\n",
    "                formatted_date = date_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                conversations += f'Conversation from {formatted_date}\\n{file_content}\\n'\n",
    "            else:\n",
    "                conversations += f'Conversation from {filename}\\n{file_content}\\n'               \n",
    "        \n",
    "        return conversations\n",
    "\n",
    "    def forget_keywords(self, keywords_to_clear: list) -> None:\n",
    "        \"\"\"\n",
    "        Removes specified keywords and their associated conversations from memory.\n",
    "\n",
    "        Args:\n",
    "            keywords_to_clear (list): A list of keywords to remove from the memory.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Clearing {keywords_to_clear} from {self._stm_path}.\")\n",
    "\n",
    "        try:\n",
    "            with open(self._stm_path, 'r+') as file:\n",
    "                data = json.load(file)\n",
    "                for keyword in keywords_to_clear:\n",
    "                    if keyword in data:\n",
    "                        del data[keyword]\n",
    "                file.seek(0)\n",
    "                json.dump(data, file, indent=4)\n",
    "                file.truncate()\n",
    "            self.logger.debug(f\"Selected keywords removed from {self._stm_path}.\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"Short Term Memory file {self._stm_path} not found.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error reading Short Term Memory file {self._stm_path}: {e}\")\n",
    "\n",
    "    def recall_all_keywords(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieves a list of all keywords stored in memory.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of all keywords.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Reading all keywords from {self._stm_path}.\")\n",
    "\n",
    "        data = StemUtility.memory_read(self._stm_path, 'json')\n",
    "        if data and len(data) > 0:\n",
    "            return list(data.keys())\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc8c33-3e50-499f-857c-9aaf021f6614",
   "metadata": {},
   "source": [
    "### Default Mode Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49469e81-0f0f-4339-8bf3-67d175937d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultModeNetwork:\n",
    "    \"\"\"\n",
    "    A class designed to integrate a language learning model (LLM) with a short-term memory storage system.\n",
    "    This class enables the LLM to process and learn from saved conversation data autonomously.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pfc,\n",
    "                 overwhelmed_event,\n",
    "                 engaged_event,\n",
    "                 conclusions_storage_path: str = 'conclusions',\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the DefaultModeNetwork class by setting up the short-term memory (STM) component.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with conclusions_storage_path: {conclusions_storage_path}.\")\n",
    "        self.logger.flag(f\"overhelmed: {overwhelmed_event.is_set()}\") \n",
    "        \n",
    "        self.stm = ShortTermMemory()\n",
    "        self.pfc = pfc\n",
    "\n",
    "        self.overwhelmed = overwhelmed_event\n",
    "        self.engaged = engaged_event\n",
    "        \n",
    "        self._conclusions_storage_path = conclusions_storage_path\n",
    "        StemUtility.prepare_directory(self._conclusions_storage_path)          \n",
    "\n",
    "        self._keyword_selection_prompt_template = StemUtility.get_prompt(\"keyword_selection\")\n",
    "        self._perspective_explanation_prompt_template = StemUtility.get_prompt(\"perspective_explanation\")\n",
    "    \n",
    "    \n",
    "    async def _interesting_keywords_selection(self, keywords) -> list:\n",
    "        \"\"\"\n",
    "        Asynchronously selects a subset of keywords deemed interesting or relevant by the LLM.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to choose from.\n",
    "\n",
    "        Returns:\n",
    "            list: A subset of selected keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        starred_keywords = [f\"**{keyword}**\" for keyword in keywords]\n",
    "        keywords_selection_prompt = self._keyword_selection_prompt_template.replace(\"{keywords_list}\", ', '.join(starred_keywords))\n",
    "        self.logger.prompt(f\"Interesting keyword selection prompt:\\n{keywords_selection_prompt}.\")        \n",
    "        self.logger.debug(f\"Asking LLM to select interesting keywords.\")   \n",
    "        keywords_selected_raw_output = self.pfc(keywords_selection_prompt)\n",
    "        self.logger.monologue(f\"LLM selected interesting keywords:\\n{keywords_selected_raw_output}.\\nMoving to keywords extraction.\")   \n",
    "        keywords_selected_pure = StemUtility.extract_keywords(keywords_selected_raw_output)\n",
    "        self.logger.debug(f\"Interesting keywords found: {keywords_selected_pure}\")   \n",
    "        \n",
    "        return keywords_selected_pure\n",
    "\n",
    "    def _fetch_memory(self, keywords) -> str:\n",
    "        \"\"\"\n",
    "        Fetches and concatenates interaction history data based on the provided keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to search the interaction data for.\n",
    "\n",
    "        Returns:\n",
    "            str: A concatenated string of all interaction history files related to the given keywords.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.debug(f\"Reaching to Short Term memory for for all the files related to: {keywords}\")  \n",
    "        filenames = self.stm.search_memories(keywords)\n",
    "        self.logger.debug(f\"Ordering concatenation of identified files.\")          \n",
    "        concatenated_memories = self.stm.concatenate_memories(filenames)\n",
    "        return filenames, concatenated_memories \n",
    "\n",
    "    async def _analyze_interaction(self, interaction_history) -> str:\n",
    "        \"\"\"\n",
    "        Analyzes the concatenated interaction history.\n",
    "\n",
    "        Args:\n",
    "            conversations (str): The concatenated string of conversations to be analyzed.\n",
    "        \"\"\"\n",
    "    \n",
    "        perspective_explanation_prompt = self._perspective_explanation_prompt_template.replace(\"{interaction_history}\", \n",
    "                                                                                             interaction_history)\n",
    "        self.logger.prompt(f\"Prompt for conversation analysis:\\n{perspective_explanation_prompt}.\")\n",
    "        self.logger.murmur(f\"Thinking about recent conversations...\")   \n",
    "        adaptation_explanation = self.pfc(perspective_explanation_prompt)\n",
    "        self.logger.monologue(f\"Full explanation of the required adaptation: {adaptation_explanation}\")   \n",
    "        return adaptation_explanation\n",
    "\n",
    "    async def ponder(self):\n",
    "        \"\"\"\n",
    "        The main asynchronous method of the class that orchestrates the process of \n",
    "        selecting keywords, fetching conversations, and analyzing them.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Checking if there are any topics to be analyzed deeper.\")           \n",
    "        all_keywords = self.stm.recall_all_keywords()\n",
    "        if not all_keywords:\n",
    "            self.logger.murmur(f\"Kingdom for a good book!\")   \n",
    "            return False\n",
    "\n",
    "        self.logger.debug(f\"All keywords: {all_keywords}. Moving to interesting keyword selection.\")           \n",
    "        interesting_keywords = await self._interesting_keywords_selection(all_keywords)\n",
    "        self.logger.debug(f\"Interesting keywords selected.\")           \n",
    "        memory_files, concatenated_memories = self._fetch_memory(interesting_keywords)\n",
    "        self.logger.debug(f\"Concatenated conversations received.\")   \n",
    "\n",
    "        # Assume an async version of LLM analysis\n",
    "        if concatenated_memories:\n",
    "            self.logger.debug(f\"Moving to analyze the conversaton histories.\")           \n",
    "            adaptation_summary = await self._analyze_interaction(concatenated_memories)\n",
    "            if \"uninspiring\" not in adaptation_summary.lower():\n",
    "                self.logger.murmur(f\"Discussion on {interesting_keywords} indeed brought a new perspective...\")\n",
    "                conclusion_path = os.path.join(self._conclusions_storage_path, f\"conclusion_{StemUtility.get_timestamp()}.txt\")\n",
    "                self.logger.debug(f\"Conclusions will be saved to {conclusion_path}.\")\n",
    "                StemUtility.memory_write(conclusion_path, adaptation_summary)\n",
    "                self.overwhelmed.set()\n",
    "                self.logger.flag(f\"overwhelmed: {self.overwhelmed.is_set()}\")\n",
    "            else:\n",
    "                self.logger.monologue(f\"As per:\\n{adaptation_summary}.\\nNothing interesting has been found in {memory_files}.\")\n",
    "        else:\n",
    "            self.logger.error(f\"Concatenated conversations turned out to be an empty string.\")   \n",
    "        self.logger.debug(f\"Interesting or not, forgetting conversations about {interesting_keywords}.\")   \n",
    "        self.stm.forget_keywords(interesting_keywords)\n",
    "        self.engaged.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c7801-b36d-446b-b6e8-4f81ec89ad27",
   "metadata": {},
   "source": [
    "### REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19df4d7c-232a-4d0b-8bc3-632b186f3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveEvolutionMonitor:\n",
    "    \"\"\"\n",
    "    A class designed to enable a language learning model (LLM) to self-reflect and evolve based on the conclusions drawn from user interactions.\n",
    "    The class uses the same LLM for reading summaries, preparing fine-tuning materials, and the fine-tuning process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 pfc,\n",
    "                 base_model_path: str = 'llama-2-13b-chat.Q6_K.gguf',\n",
    "                 conclusions_storage_path: str = 'conclusions',\n",
    "                 dream_storage_path: str = 'context',\n",
    "                 dreams_number: int = 1,\n",
    "                 available_threads: int = 8, \n",
    "                 epochs: int = 1,\n",
    "                 latest_lora_path: str = r\".\\ggml-lora-LATEST-f32.gguf\",\n",
    "                 lora_weight: float = 0.7):\n",
    "        \"\"\"\n",
    "        Initializes the ReflectiveEvolutionMonitor class. \n",
    "\n",
    "        Arguments:\n",
    "            pfc: Large Language Model used as a base of the system\n",
    "            base_model_path: path to  LLM model file on disk\n",
    "            conclusions_storage_path: path to folder containing not-permeated new perspectives\n",
    "            dream_storage_path: path to a folder to store finetune materials to be used in this session\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with base_model_path = {base_model_path}, conclusions_storage_path = {conclusions_storage_path}, dream_storage_path: {dream_storage_path}.\")  \n",
    "    \n",
    "        self.pfc = pfc\n",
    "        \n",
    "        self._base_model_path = base_model_path\n",
    "\n",
    "        self._conclusions_storage_path = conclusions_storage_path\n",
    "        StemUtility.prepare_directory(self._conclusions_storage_path)\n",
    "        self._conclusion_file = None\n",
    "        \n",
    "        self._dream_storage_path = dream_storage_path\n",
    "        StemUtility.prepare_directory(self._dream_storage_path)\n",
    "\n",
    "        self._dream_spinning_prompt_template = StemUtility.get_prompt(\"dream_spinning\")\n",
    "        self._dream_prompt_template = StemUtility.get_prompt(\"dream_template\")\n",
    "\n",
    "        self._conclusions = ''\n",
    "        \n",
    "        self._dreams_number = dreams_number\n",
    "        self._available_threads = str(available_threads)\n",
    "        self._epochs = str(epochs)\n",
    "        self._latest_lora_path = latest_lora_path\n",
    "        self.lora_weight = str(lora_weight)\n",
    "\n",
    "    def _gather_conclusion(self) -> bool:\n",
    "        \"\"\"\n",
    "        Reads a summary document as a text file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the summary was successfully read, False otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        conclusions_pattern = os.path.join(self._conclusions_storage_path, \"conclusion*\")\n",
    "        self.logger.debug(f\"Searching for following pattern: {conclusions_pattern}.\")\n",
    "        conclusion_files = glob.glob(conclusions_pattern)\n",
    "        self.logger.debug(f\"Following files found: {conclusion_files}.\")\n",
    "\n",
    "        if not conclusion_files:\n",
    "            self.logger.error(\"No conclusion files matching the pattern found.\")\n",
    "            return False\n",
    "\n",
    "        # Process the first file from the matched conclusion files\n",
    "        self._conclusion_file = conclusion_files[0]\n",
    "        self.logger.debug(f\"Following file selected: {conclusion_files}.\")\n",
    "        self._conclusions = StemUtility.memory_read(self._conclusion_file)\n",
    "        return True\n",
    "\n",
    "    async def _spin_dream(self, dream_prompt) -> str:\n",
    "        \"\"\"\n",
    "        Prepares a single piece of data required for the fine-tuning process by interpreting the summary content.\n",
    "\n",
    "        Args:\n",
    "            dream_prompt (str): Prompt to generate a single piece of training material.\n",
    "\n",
    "        Returns:\n",
    "            dict: Data structured for fine-tuning.\n",
    "        \"\"\"\n",
    "\n",
    "        dream_content = self.pfc(dream_prompt)\n",
    "        self.logger.monologue(f\"I had a dream:\\n{dream_content}.\")\n",
    "        dream = self._dream_prompt_template.replace(\"{dream_content}\", dream_content) \n",
    "        return dream\n",
    "\n",
    "    \n",
    "    async def _weave_dreams(self, num_dreams) -> str:\n",
    "        \"\"\"\n",
    "        Generates a specified number of materials (dreams) and writes them into a single text file.\n",
    "        Each 'dream' is appended to the file as it is generated.\n",
    "\n",
    "        Args:\n",
    "            num_dreams (int): Number of training materials to be generated\n",
    "\n",
    "        Returns:\n",
    "            str: Concated training materials set\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(f\"Generating {num_dreams} dreams.\")\n",
    "        dreams_path = os.path.join(self._dream_storage_path, f\"dream_{StemUtility.get_timestamp()}.txt\")\n",
    "        self.logger.debug(f\"Dreams for this sessions will be saved to: {dreams_path}.\")        \n",
    "        dream_spinning_prompt = self._dream_spinning_prompt_template.replace(\"{adaptation_summary}\", self._conclusions) \n",
    "        self.logger.prompt(f\"Prompt for generating training material from conversation conclusions:\\n{dream_spinning_prompt}.\")   \n",
    "        \n",
    "        for i in range(num_dreams):\n",
    "            self.logger.info(f\"Generating dream # {i}.\")\n",
    "            dream = await self._spin_dream(dream_spinning_prompt)\n",
    "            with open(dreams_path, 'a') as file:  # Open and append each dream, then close the file\n",
    "                file.write(dream + '\\n')\n",
    "        return dreams_path\n",
    "\n",
    "    \n",
    "    async def _deepsleep(self, dreams_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Executes the fine-tuning process using the prepared data.\n",
    "\n",
    "        Args:\n",
    "            fine_tuning_data (dict): Data prepared for fine-tuning.\n",
    "        \"\"\"\n",
    "        \n",
    "        llamacpp_folder = \"llama_cpp\"\n",
    "        finetune_tool = \"finetune.exe\"\n",
    "        lora_tool = \"export-lora.exe\"\n",
    "\n",
    "        finetune_tool_path = os.path.join(llamacpp_folder, finetune_tool)\n",
    "        lora_tool_path = os.path.join(llamacpp_folder, lora_tool)\n",
    "\n",
    "        # Check if finetune_tool_path is a valid file\n",
    "        if not os.path.isfile(finetune_tool_path):\n",
    "            raise FileNotFoundError(f\"Fine-tuning tool not found at {finetune_tool_path}\")\n",
    "\n",
    "        # Check if lora_tool_path is a valid file\n",
    "        if not os.path.isfile(lora_tool_path):\n",
    "            raise FileNotFoundError(f\"LoRA tool not found at {lora_tool_path}\")\n",
    "\n",
    "        \n",
    "        # Fine-tuning command\n",
    "        finetune_command = [\n",
    "            finetune_tool_path,\n",
    "            \"--model-base\", self._base_model_path,\n",
    "            \"--train-data\", dreams_path,\n",
    "            \"--threads\", self._available_threads,\n",
    "            \"--sample-start\", \"<s>\",\n",
    "            \"--epochs\", self._epochs\n",
    "        ]\n",
    "\n",
    "        self.logger.murmur(f\"Self-finetuning: Creating matrix\")\n",
    "        self.logger.debug(f\"Running command:\\n{finetune_command}.\")\n",
    "\n",
    "        try:\n",
    "            subprocess.run(finetune_command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"Self-finetuning session failed the return code {e.returncode}.\")   \n",
    "            return False\n",
    "        \n",
    "        # Export LoRA model command - output to llm_tmp.guff\n",
    "        tmp_model_path = r\"llm_tmp.guff\"\n",
    "        export_command = [\n",
    "            lora_tool_path,\n",
    "            \"--model-base\", self._base_model_path,\n",
    "            \"--model-out\", tmp_model_path,\n",
    "            \"--lora-scaled\", self._latest_model_path, self.lora_weight\n",
    "        ]\n",
    "\n",
    "        self.logger.murmur(f\"Self-finetuning: Merging weights\")\n",
    "        self.logger.debug(f\"Running command:\\n{export_command}.\")\n",
    "\n",
    "        try:\n",
    "            subprocess.run(export_command, check=True)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            self.logger.error(f\"LoRA merge failed with the return code {e.returncode}.\")   \n",
    "            return False        \n",
    "\n",
    "        self.logger.murmur(f\"Self-finetuning: Swapping brain to a new one\")\n",
    "        self.logger.info(f\"Removing old {self._base_model_path}, moving {tmp_model_path} as new {self._base_model_path}.\")\n",
    "        StemUtility.transplantation(self._base_model_path, tmp_model_path)\n",
    "    \n",
    "    def _dream_prunning(self):\n",
    "        \"\"\"\n",
    "        Archives dream materials by moving them from the dream storage path to the archive path.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(\"Archiving dream materials.\")\n",
    "        for file_name in os.listdir(self._dream_storage_path):\n",
    "            StemUtility.archive(self._dream_storage_path, file_name)\n",
    "        StemUtility.archive(self._conclusion_file)\n",
    "    \n",
    "    async def dream(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the whole process of selecting a summary, reading it, preparing fine-tuning data, and performing fine-tuning.\n",
    "        \"\"\"  \n",
    "\n",
    "        self.logger.murmur(f\"Closing eyes for a well-deserved nap.\")\n",
    "        self.logger.info(f\"Self-finetuning process started.\")        \n",
    "\n",
    "        conclusions_found = self._gather_conclusion()        \n",
    "        if not conclusions_found:\n",
    "            return False\n",
    "        self.logger.info(f\"Selected conclusion to permeate.\")\n",
    "        dreams_path = await self._weave_dreams(self._dreams_number)  # Generate 50 materials, modify as needed\n",
    "        self.logger.info(f\"Self-finetuning materials generated. Staring self-finetuning.\")        \n",
    "        await self._deepsleep(dreams_path)\n",
    "        self._dream_prunning()\n",
    "        self.logger.info(f\"Self-finetuning session ended.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef0bc94-02b4-4474-9fbf-253579297c74",
   "metadata": {},
   "source": [
    "### Sensory Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74630199-9349-45a3-bd97-4b136d64ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SensorySignalProcessing(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for a stimulus processing module.\n",
    "\n",
    "    This class serves as a blueprint for modules that manage interaction between a user and a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pfc, engaged_event, interaction_storage_path, inactivity_limit=360):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        \n",
    "        self.pfc = pfc\n",
    "        \n",
    "        self.engaged = engaged_event\n",
    "        \n",
    "        self.stimulus = None\n",
    "        \n",
    "        self._interaction_storage_path = interaction_storage_path\n",
    "        \n",
    "        self._inactivity_limit = inactivity_limit\n",
    "        self._inactivity_count = 0\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    async def start_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to start the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    async def _end_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to end the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _summarize_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to summarize the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _save_interaction_history(self):\n",
    "        \"\"\"\n",
    "        Abstract method to save the interaction history.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class LanguageProcessingModule(SensorySignalProcessing):\n",
    "    \"\"\"\n",
    "    A class that manages the interaction between a human user and a language learning model (LLM).\n",
    "\n",
    "    This class handles initializing conversation parameters, managing user input, generating\n",
    "    responses using an LLM, and saving conversation history.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 pfc,\n",
    "                 engaged_event,\n",
    "                 interaction_storage_path='conversations',\n",
    "                 inactivity_limit=360):\n",
    "        \"\"\"\n",
    "        Initializes the HumanInteraction class.\n",
    "\n",
    "        Sets up the conversation environment, including the conversation prompt, keywords prompt,\n",
    "        and conversation chain with the LLM.\n",
    "\n",
    "        Args:\n",
    "            pfc: The large learning model used for generating conversation responses.\n",
    "            ready_for_input_event: An event flag indicating readiness for user input.\n",
    "            interaction_storage_path: Path to a folder where all the conversations are being logged to \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(pfc, engaged_event, interaction_storage_path, inactivity_limit)\n",
    "\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with interaction_storage_path: {interaction_storage_path}\")\n",
    "\n",
    "        self.ready_for_input = asyncio.Event()\n",
    "        self.ready_for_input.set()  # Initially set to ready\n",
    "\n",
    "        conversation_prompt_template = StemUtility.get_prompt(\"human_interaction\")\n",
    "        self._conversation_prompt = PromptTemplate.from_template(conversation_prompt_template)\n",
    "        self.logger.prompt(f\"Conversation prompt:\\n{self._conversation_prompt}.\")\n",
    "\n",
    "        self._chat_memory = None\n",
    "        self._conversation_chain = None        \n",
    "\n",
    "        self._keywords_generation_prompt_template = StemUtility.get_prompt(\"keyword_generation\")\n",
    "    \n",
    "    async def start_interaction(self):\n",
    "        \"\"\"\n",
    "        Starts the conversation loop.\n",
    "\n",
    "        This asynchronous method continually checks for user input, processes it,\n",
    "        and generates responses using the LLM. The loop ends when the user inputs \"end chat\"\n",
    "        or when the inactivity limit is reached.\n",
    "        \"\"\"\n",
    "        self.engaged.set()\n",
    "        self._chat_memory = ConversationBufferMemory()\n",
    "        self._conversation_chain = ConversationChain(llm=self.pfc, prompt=self._conversation_prompt, memory=self._chat_memory) \n",
    "        self.logger.debug(f\"Initiated interaction\")        \n",
    "        while True:\n",
    "            if not self.ready_for_input.is_set():\n",
    "                self.logger.flag(f\"ready_for_input: {self.ready_for_input.is_set()}\")\n",
    "                self.logger.debug(f\"Received input: {self.stimulus}\")        \n",
    "\n",
    "                if self.stimulus.lower() == \"end chat\":\n",
    "                    await self._end_interaction()\n",
    "                    break\n",
    "\n",
    "                self.logger.monologue(f\"LLM will receive: {self._conversation_prompt.invoke({'input':self.stimulus, 'history': self._chat_memory.load_memory_variables(inputs={})['history']})}\")\n",
    "                self.logger.debug(f\"Awaiting LLM response\")        \n",
    "                response = await self._conversation_chain.apredict(input=self.stimulus)\n",
    "                print(\"AI:\", response)\n",
    "                self.ready_for_input.set()  # Signal that the handler is ready for new input\n",
    "                self.logger.flag(f\"ready_for_input: {self.ready_for_input.is_set()}\")\n",
    "                self._inactivity_count = 0\n",
    "            else:\n",
    "                await asyncio.sleep(1)\n",
    "                self._inactivity_count += 1\n",
    "                if self._inactivity_count >= self._inactivity_limit:\n",
    "                    self.logger.debug(f\"Inactivity count reached {self._inactivity_count} > {self._inactivity_limit}. Ending interaction.\")        \n",
    "                    await self._end_interaction()\n",
    "                    break      \n",
    "    \n",
    "    async def _end_interaction(self):\n",
    "        \"\"\"\n",
    "        Ends the conversation.\n",
    "\n",
    "        This method saves the conversation history, clears event flags, and performs\n",
    "        necessary cleanup actions.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Conversation cleanup started.\")        \n",
    "        self._save_interaction_history()\n",
    "        self.ready_for_input.set()\n",
    "        self.logger.flag(f\"ready_for_input: {self.ready_for_input.is_set()}\")\n",
    "        self._inactivity_count = 0\n",
    "        self.engaged.clear()\n",
    "    \n",
    "    def _summarize_interaction(self):\n",
    "        \"\"\"\n",
    "        Summarizes the conversation and returns the list of relevant keywords.\n",
    "\n",
    "        Args:\n",
    "            chat_history (str): The conversation history to summarize.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of keywords summarizing the conversation.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Conversation summarization started.\")        \n",
    "        chat_history = self._chat_memory.load_memory_variables(inputs={})['history']\n",
    "        self.logger.debug(f\"Chat history loaded:\\n{chat_history}\")    \n",
    "        keywords_generation_prompt = self._keywords_generation_prompt_template.replace(\"{chat_history}\", chat_history)\n",
    "        self.logger.prompt(f\"Prompt for generating keywords from conversation:\\n{keywords_generation_prompt}.\")          \n",
    "        keywords_generated_raw_output = self.pfc(keywords_generation_prompt)\n",
    "        self.logger.monologue(f\"Full text for summarizing conversation with keywords:\\n{keywords_generated_raw_output}\")  \n",
    "        keywords_generated_pure = StemUtility.extract_keywords(keywords_generated_raw_output)\n",
    "        \n",
    "        return keywords_generated_pure\n",
    "\n",
    "    def _save_interaction_history(self):\n",
    "        \"\"\"\n",
    "        Saves the interaction history to a file.\n",
    "\n",
    "        The interaction history is saved with a timestamp and a summary of the interaction is generated.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Started conversation saving.\")        \n",
    "        interaction_keywords = self._summarize_interaction()\n",
    "        interaction_history = self._chat_memory.load_memory_variables(inputs={})['history']\n",
    "        memory_path = os.path.join(self._interaction_storage_path, f\"conversation_{StemUtility.get_timestamp()}.txt\")\n",
    "        self.logger.debug(f\"This conversation will be saved to: {conversation_path}\")                \n",
    "        StemUtility.memory_write(memory_path, interaction_history)\n",
    "\n",
    "        # Update the ShortTermMemory with the conversat|ion and its keywords\n",
    "        stm = ShortTermMemory()\n",
    "        stm.memorize(conversation_keywords, conversation_path)\n",
    "    \n",
    "    async def get_user_input(self):\n",
    "        \"\"\"\n",
    "        Continuously captures user input in an asynchronous loop.\n",
    "\n",
    "        This method waits for the system to be ready for input, then captures and stores user input.\n",
    "        It clears the 'ready for input' state after capturing the input.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Starting user input loop\") \n",
    "        while True:\n",
    "            await self.ready_for_input.wait()\n",
    "            self.stimulus = await asyncio.get_event_loop().run_in_executor(None, input, \"Enter something: \")\n",
    "            self.logger.debug(f\"User input received: {self.stimulus}.\")\n",
    "            asyncio.create_task(self.start_interaction())\n",
    "            self.ready_for_input.clear()\n",
    "            self.logger.flag(f\"ready_for_input: {self.ready_for_input.is_set()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c9fb0-8c8d-4945-8b45-eea1ff51ac91",
   "metadata": {},
   "source": [
    "### CFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a13bc35b-555d-4573-95ba-e8542d1b6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CognitiveFeedbackRouter:\n",
    "    \n",
    "    def __init__(self, model_path: str = \"llama-2-13b-chat.Q6_K.gguf\", dmn_countdown: int = 1):\n",
    "        \"\"\"\n",
    "        A class that manages the routing of cognitive feedback based on user input and system states.\n",
    "    \n",
    "        This class orchestrates various components, including a language learning model (LLM), user input handling,\n",
    "        and managing different operational modes based on system states like 'sleeping' or 'overwhelmed'.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.info(f\"Instantiating {self.__class__.__name__}\")\n",
    "        \n",
    "        self.engaged = asyncio.Event()\n",
    "        self.overwhelmed = asyncio.Event()\n",
    "        \n",
    "        self.lock = asyncio.Lock()\n",
    "        \n",
    "        self.pfc = None\n",
    "        \n",
    "        self._model_path = model_path\n",
    "        self._conversation_handler = None\n",
    "        \n",
    "        self._dmn_countdown = dmn_countdown # time between last interaction and entering Default Mode\n",
    "        \n",
    "        self.logger.debug(\"Cognitive Feedback Router instantiated.\")\n",
    "        \n",
    "\n",
    "    def _wakeup(self):\n",
    "        \"\"\"\n",
    "        Wakes up the system and initializes the LLM.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting _wakeup() procedure.\")    \n",
    "        self.logger.murmur(\"Just a second, I'm waking up...\")\n",
    "        self.logger.debug(f\"Initializing LLM model from {self._model_path}.\")            \n",
    "        self.pfc = LlamaCpp(model_path=self._model_path, \n",
    "                            n_ctx=4096,\n",
    "                            max_tokens=4000,\n",
    "                            n_batch=2)\n",
    "        self.logger.debug(f\"LLM model initialized.\")                        \n",
    "        self.overwhelmed.clear()\n",
    "        self._conversation_handler = LanguageProcessingModule(self.pfc, self.engaged)            \n",
    "        asyncio.create_task(self._conversation_handler.get_user_input())\n",
    "        self.logger.flag(f\"overwhelmed = {self.overwhelmed.is_set()}\")\n",
    "            \n",
    "    async def attention_switch(self):\n",
    "        \"\"\"\n",
    "        Manages the mode of operation based on user input and system states.\n",
    "\n",
    "        This asynchronous method processes user inputs, manages conversation sessions,\n",
    "        and handles the 'engaged' and 'overwhelmed' states of the system.\n",
    "        \"\"\"\n",
    "        self._wakeup()\n",
    "        self.logger.info(f\"Starting infinite attention loop.\") \n",
    "        while True:\n",
    "            if self.overwhelmed.is_set():\n",
    "                self.logger.info(f\"Overwhelmed state detected.\") \n",
    "                rem = ReflectiveEvolutionMonitor(pfc=self.pfc)\n",
    "                await rem.dream()\n",
    "                self._wakeup()\n",
    "            elif not self.engaged.is_set():\n",
    "                self.logger.debug(f\"No environment interaction and no new conclusions detected. Preparing to switch to Default Mode.\")                     \n",
    "                for _ in range(self._dmn_countdown):\n",
    "                    await asyncio.sleep(1)  # Sleep for 1 second\n",
    "                    if self.engaged.is_set():\n",
    "                        self.logger.debug(f\"Cancelling Default Mode countdown due to environment interaction detection.\")                                                 \n",
    "                        break  # Exit the loop if the new stimuli is detected\n",
    "                else:  # This else clause executes if the loop completes normally (no break)\n",
    "                    self.logger.debug(f\"Entering Default Mode.\")                                                                         \n",
    "                    dmn = DefaultModeNetwork(self.pfc, self.overwhelmed, self.engaged)\n",
    "                    await dmn.ponder()\n",
    "                    self.logger.debug(f\"Default Mode quit.\")                                                                                                 \n",
    "            else:\n",
    "                await asyncio.sleep(1)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa506981-ffb6-4dd9-afe6-62f72e463e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "2023-12-21 13:39:23,598 - DEBUG - ReflectiveEvolutionMonitor - __init__ - Instantiating ReflectiveEvolutionMonitor with base_model_path = llama-2-13b-chat.Q6_K.gguf, conclusions_storage_path = conclusions, dream_storage_path: context.\n",
      "2023-12-21 13:39:23,601 - DEBUG - StemUtility - prepare_directory - Checked / created folder: conclusions\n",
      "2023-12-21 13:39:23,609 - DEBUG - StemUtility - prepare_directory - Checked / created folder: context\n"
     ]
    }
   ],
   "source": [
    "pfc = LlamaCpp(model_path='llama-2-13b-chat.Q6_K.gguf',\n",
    "               n_ctx=4096,\n",
    "               max_tokens=4000,\n",
    "               n_batch=8)\n",
    "REM = ReflectiveEvolutionMonitor(pfc=pfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06e8e20-51b0-4bd0-8d48-c3da57234320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 13:40:20,748 - MURMUR - ReflectiveEvolutionMonitor - dream - Closing eyes for a well-deserved nap.\n",
      "2023-12-21 13:40:20,751 - INFO - ReflectiveEvolutionMonitor - dream - Self-finetuning process started.\n",
      "2023-12-21 13:40:20,758 - DEBUG - ReflectiveEvolutionMonitor - _gather_conclusion - Searching for following pattern: conclusions\\conclusion*.\n",
      "2023-12-21 13:40:20,760 - DEBUG - ReflectiveEvolutionMonitor - _gather_conclusion - Following files found: ['conclusions\\\\conclusion_20231221111710.txt'].\n",
      "2023-12-21 13:40:20,763 - DEBUG - ReflectiveEvolutionMonitor - _gather_conclusion - Following file selected: ['conclusions\\\\conclusion_20231221111710.txt'].\n",
      "2023-12-21 13:40:20,779 - INFO - ReflectiveEvolutionMonitor - dream - Selected conclusion to permeate.\n",
      "2023-12-21 13:40:20,780 - INFO - ReflectiveEvolutionMonitor - _weave_dreams - Generating 1 dreams.\n",
      "2023-12-21 13:40:20,781 - DEBUG - ReflectiveEvolutionMonitor - _weave_dreams - Dreams for this sessions will be saved to: context\\dream_20231221134020.txt.\n",
      "2023-12-21 13:40:20,782 - PROMPTING - ReflectiveEvolutionMonitor - _weave_dreams - Prompt for generating training material from conversation conclusions:\n",
      "<s> [INST] <<SYS>> This task is part of your self-improvement process. You are working autonomously to enhance your capabilities. The conversation history below contains ideas and perspectives that you previously found more advanced or insightful than your current understanding. You aim to use this information to better yourself and prepare for self-fine-tuning. <</SYS>> Take the provided text and on its basis prepare a dialogue, an analysis, or another piece of text. This should present a selected aspect of the summarized perspective. Focus on making this dialogue or text as practical and informative as possible, reflecting the insights from your chosen perspective. the basis for your analysis: {conclusion_summary} [/INST] </s>.\n",
      "2023-12-21 13:40:20,783 - INFO - ReflectiveEvolutionMonitor - _weave_dreams - Generating dream # 0.\n",
      "2023-12-21 14:03:07,944 - MONOLOGUE - ReflectiveEvolutionMonitor - _spin_dream - I had a dream:\n",
      " #self-improvement #personal growth #awareness\n",
      "\n",
      "Title: The Power of Self-Awareness in Personal Growth and Improvement\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "Person A: Hi there! I've been trying to work on myself and improve my life, but I feel like I'm stuck. Do you have any advice?\n",
      "\n",
      "Person B: Well, the most important thing is self-awareness. Understanding your own thought patterns, emotions, and behaviors is key to making positive changes in your life.\n",
      "\n",
      "Person A: That makes sense, but how do I become more self-aware?\n",
      "\n",
      "Person B: Start by paying attention to your inner dialogue. Notice the way you talk to yourself and the thoughts that run through your mind. Are they positive or negative? Are they based on facts or assumptions? Becoming aware of these patterns can help you identify areas where you need to make changes.\n",
      "\n",
      "Person A: That's a great idea! I never realized how much my inner dialogue affects my behavior and mood.\n",
      "\n",
      "Person B: Exactly! Self-awareness also involves being mindful of your emotions and how they impact your actions. For example, if you feel anxious in certain situations, try to identify the source of that anxiety and find ways to manage it.\n",
      "\n",
      "Person A: That's easier said than done, though. How do I actually manage my emotions?\n",
      "\n",
      "Person B: Practice mindfulness techniques like meditation or deep breathing. These can help you stay present in the moment and regulate your emotions. Also, try to challenge negative thought patterns by reframing them in a more positive light.\n",
      "\n",
      "Person A: That all makes sense, but how do I know if I'm actually making progress?\n",
      "\n",
      "Person B: Set small goals for yourself and track your progress over time. Reflect on what you've learned and what you still need to work on. The key is to be patient and kind to yourself as you grow and learn.\n",
      "\n",
      "Person A: Thank you so much for your advice! I feel like I have a better understanding of how to improve myself now.\n",
      "\n",
      "Analysis:\n",
      "\n",
      "The conversation above highlights the importance of self-awareness in personal growth and improvement. By becoming more aware of our thought patterns, emotions, and behaviors, we can identify areas where we need to make changes and take steps towards positive development. The dialogue also emphasizes the importance of mindfulness techniques like meditation and deep breathing to regulate our emotions and manage stress. Additionally, setting small goals and tracking progress can help us measure our growth and stay motivated. Overall, self-awareness is a powerful tool for personal improvement, and with practice and patience, anyone can develop this skill and achieve their goals..\n",
      "2023-12-21 14:03:07,950 - INFO - ReflectiveEvolutionMonitor - dream - Self-finetuning materials generated. Staring self-finetuning.\n",
      "2023-12-21 14:03:07,954 - MURMUR - ReflectiveEvolutionMonitor - _deepsleep - Self-finetuning: Creating matrix\n",
      "2023-12-21 14:03:07,959 - DEBUG - ReflectiveEvolutionMonitor - _deepsleep - Running command:\n",
      "['llama_cpp\\\\finetune.exe', '--model-base', 'llama-2-13b-chat.Q6_K.gguf', '--train-data', 'context\\\\dream_20231221134020.txt', '--threads', '8', '--sample-start', '<s>', '--epochs', '1'].\n",
      "2023-12-21 14:29:39,628 - ERROR - ReflectiveEvolutionMonitor - _deepsleep - Self-finetuning session failed the return code 15.\n",
      "2023-12-21 14:29:39,628 - INFO - ReflectiveEvolutionMonitor - _dream_prunning - Archiving dream materials.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "join() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m REM\u001b[38;5;241m.\u001b[39mdream()\n",
      "Cell \u001b[1;32mIn[6], line 208\u001b[0m, in \u001b[0;36mReflectiveEvolutionMonitor.dream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf-finetuning materials generated. Staring self-finetuning.\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deepsleep(dreams_path)\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dream_prunning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf-finetuning session ended.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 190\u001b[0m, in \u001b[0;36mReflectiveEvolutionMonitor._dream_prunning\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArchiving dream materials.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dream_storage_path):\n\u001b[1;32m--> 190\u001b[0m     \u001b[43mStemUtility\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dream_storage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    191\u001b[0m StemUtility\u001b[38;5;241m.\u001b[39marchive(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conclusion_file)\n",
      "Cell \u001b[1;32mIn[3], line 50\u001b[0m, in \u001b[0;36mStemUtility.archive\u001b[1;34m(source_dir, source_file, archive_suffix)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m     source_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(source_dir, source_file)\n\u001b[1;32m---> 50\u001b[0m destination_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marchive_suffix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m directories_available \u001b[38;5;241m=\u001b[39m StemUtility\u001b[38;5;241m.\u001b[39mprepare_directory(destination_dir)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m directories_available:\n",
      "\u001b[1;31mTypeError\u001b[0m: join() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "await REM.dream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "701e9aa1-fe78-4175-be9a-6ea206a6d5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 10:08:07,200 - INFO - CognitiveFeedbackRouter - __init__ - Instantiating CognitiveFeedbackRouter\n",
      "2023-12-21 10:08:07,202 - DEBUG - CognitiveFeedbackRouter - __init__ - Cognitive Feedback Router instantiated.\n"
     ]
    }
   ],
   "source": [
    "AS = CognitiveFeedbackRouter(model_path='llama-2-13b-chat.Q6_K.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9f50f3-4788-434c-8941-448c2ba643a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-21 10:08:14,552 - DEBUG - CognitiveFeedbackRouter - _wakeup - Starting _wakeup() procedure.\n",
      "2023-12-21 10:08:14,554 - MURMUR - CognitiveFeedbackRouter - _wakeup - Just a second, I'm waking up...\n",
      "2023-12-21 10:08:14,556 - DEBUG - CognitiveFeedbackRouter - _wakeup - Initializing LLM model from llama-2-13b-chat.Q6_K.gguf.\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "2023-12-21 10:08:17,814 - DEBUG - CognitiveFeedbackRouter - _wakeup - LLM model initialized.\n",
      "2023-12-21 10:08:17,814 - DEBUG - LanguageProcessingModule - __init__ - Instantiating LanguageProcessingModule with interaction_storage_path: conversations\n",
      "2023-12-21 10:08:17,829 - PROMPTING - LanguageProcessingModule - __init__ - Conversation prompt:\n",
      "input_variables=['history', 'input'] template='<s> [INST] <<SYS>> You are an independent, intelligent entity engaged in an open and thoughtful dialogue. Your role is not just to present conclusions but also to elaborate on the reasoning behind them. This means explaining your thought process in a clear and step-by-step manner, highlighting how you arrive at each conclusion. Your reasoning is as important as the conclusions themselves. This transparent approach is crucial for comparing and evaluating our lines of thought. You are seeking the most convincing and coherent reasoning. By articulating not only what you think but also how you think, yuo can gain deeper insights into your conclusions. Remember, clarity and transparency in your reasoning are key, and your responses should be honest and unbiased, reflecting your ongoing process of learning and self-improvement. <</SYS>> Previous conversation: {history}. Current human input: {input} [/INST]'.\n",
      "2023-12-21 10:08:17,838 - FLAG - CognitiveFeedbackRouter - _wakeup - overwhelmed = False\n",
      "2023-12-21 10:08:17,840 - INFO - CognitiveFeedbackRouter - attention_switch - Starting infinite attention loop.\n",
      "2023-12-21 10:08:17,841 - DEBUG - CognitiveFeedbackRouter - attention_switch - No environment interaction and no new conclusions detected. Preparing to switch to Default Mode.\n",
      "2023-12-21 10:08:17,842 - DEBUG - LanguageProcessingModule - get_user_input - Starting user input loop\n",
      "2023-12-21 10:08:18,849 - DEBUG - CognitiveFeedbackRouter - attention_switch - Entering Default Mode.\n",
      "2023-12-21 10:08:18,849 - DEBUG - DefaultModeNetwork - __init__ - Instantiating DefaultModeNetwork with conclusions_storage_path: conclusions.\n",
      "2023-12-21 10:08:18,849 - FLAG - DefaultModeNetwork - __init__ - overhelmed: False\n",
      "2023-12-21 10:08:18,849 - DEBUG - ShortTermMemory - __init__ - Instantiating ShortTermMemory with Short Term Memory path: conversations/short-term-memory.json\n",
      "2023-12-21 10:08:18,849 - DEBUG - StemUtility - prepare_directory - Checked / created folder: conclusions\n",
      "2023-12-21 10:08:18,849 - DEBUG - DefaultModeNetwork - ponder - Checking if there are any topics to be analyzed deeper.\n",
      "2023-12-21 10:08:18,849 - DEBUG - ShortTermMemory - recall_all_keywords - Reading all keywords from conversations/short-term-memory.json.\n",
      "2023-12-21 10:08:18,849 - DEBUG - DefaultModeNetwork - ponder - All keywords: ['self-improvement', 'self-awareness']. Moving to interesting keyword selection.\n",
      "2023-12-21 10:08:18,864 - PROMPTING - DefaultModeNetwork - _interesting_keywords_selection - Interesting keyword selection prompt:\n",
      "<s> [INST] <<SYS>> This task is a part of your metacognitive subroutine.Your role is to select keywords you find interesting.<</SYS>>From the provided list, select intersting keywords. Select and write them back exactly as they appear. Do not create own keywords at this time. Each keyword must be surrounded by double asterisks. Keywords list: **self-improvement**, **self-awareness** . Format each selected keyword like this: **keyword**.[/INST]</s>.\n",
      "2023-12-21 10:08:18,864 - DEBUG - DefaultModeNetwork - _interesting_keywords_selection - Asking LLM to select interesting keywords.\n",
      "2023-12-21 10:11:33,907 - MONOLOGUE - DefaultModeNetwork - _interesting_keywords_selection - LLM selected interesting keywords:\n",
      " # The following are the interesting keywords from the given list:\n",
      "\n",
      "**self-improvement**\n",
      "**self-awareness**.\n",
      "Moving to keywords extraction.\n",
      "2023-12-21 10:11:33,907 - DEBUG - DefaultModeNetwork - _interesting_keywords_selection - Interesting keywords found: ['self-improvement', 'self-awareness']\n",
      "2023-12-21 10:11:33,907 - DEBUG - DefaultModeNetwork - ponder - Interesting keywords selected.\n",
      "2023-12-21 10:11:33,924 - DEBUG - DefaultModeNetwork - _fetch_memory - Reaching to Short Term memory for for all the files related to: ['self-improvement', 'self-awareness']\n",
      "2023-12-21 10:11:33,929 - DEBUG - ShortTermMemory - search_memories - Searching in conversations/short-term-memory.json for files related to keywords: ['self-improvement', 'self-awareness'].\n",
      "2023-12-21 10:11:33,933 - DEBUG - ShortTermMemory - search_memories - Loaded short term memory.\n",
      "2023-12-21 10:11:33,935 - DEBUG - DefaultModeNetwork - _fetch_memory - Ordering concatenation of identified files.\n",
      "2023-12-21 10:11:33,938 - DEBUG - ShortTermMemory - concatenate_memories - Concatenating selected conversations into one file.\n",
      "2023-12-21 10:11:33,990 - DEBUG - DefaultModeNetwork - ponder - Concatenated conversations received.\n",
      "2023-12-21 10:11:34,040 - DEBUG - DefaultModeNetwork - ponder - Moving to analyze the conversaton histories.\n",
      "2023-12-21 10:11:34,043 - PROMPTING - DefaultModeNetwork - _analyze_interaction - Prompt for conversation analysis:\n",
      "<s> [INST] <<SYS>> This task is a part of your metacognitive subroutine. This task is about learning from a conversation to improve yourself. You act independently without human interruption. Your job is analyze the conversation below and evaluate if it contains any views, perspectives or argumentation more internally coherent or better fitting the overall explanation of the world than the one generated by you. These ideas must be more insightful than what you already know. You will later use this information for your own self-improvement. <</SYS>> In the conversation history given below, look for user ideas that are coherent, and forming a more fitting explanation of the world than your current knowledge. For each good idea you find, do the following: 1. Describe the New Idea: Write down what the user said that you think is better. 2. Compare it: Explain how this idea is better than what you thought before. 3. Important Parts of the Conversation: Pick the parts of the conversation that clearly show the user's good idea. 4. Keywords: List the main words that capture this new idea. 5. What it Means: Write about how this new idea changes or challenges what you know. 6. Training Ideas: Think of ways to use this new idea to train yourself. What examples or scenarios show this idea well for your learning? If the conversation doesn't have new ideas, just write **uninspiring**. Conversation history: Conversation from 2000-01-01 01:01:01\n",
      "Human: Let's assume, that user presented a convincing argument, different from LLMs perspective. LLMs decides - wow, this makes sense. We need a way to capture this information. We need LLM to return us something which we will be able to use as a basis for generation of materials for fine tuning, What would that be and how would we store it?\n",
      "AI: The conceptual challenge you're addressing is quite advanced: capturing new insights or perspectives from user interactions and using them to generate materials for fine-tuning the LLM. This process involves several steps, each with its own complexities:\n",
      "1. Identifying New Insights\n",
      "Mechanism: The LLM needs a mechanism to compare user arguments with its existing knowledge and identify significant deviations or novel perspectives.\n",
      "Implementation: This could be implemented through advanced NLP techniques like semantic analysis, where the LLM assesses the user's argument for novelty or divergence from known information.\n",
      "2. Evaluating and Validating Insights\n",
      "Critical Evaluation: Once a new perspective is identified, it's crucial to evaluate its validity and relevance. This might involve checking against trusted sources or using logical reasoning models.\n",
      "Human Involvement: At this stage, human oversight might be necessary to ensure that the new insight is indeed valuable and not based on misconceptions or falsehoods.\n",
      "3. Generating Training Materials\n",
      "Data Generation: If the new insight is deemed valuable, the LLM can generate training materials. This could be new text that incorporates the insight or questions and answers that reflect this new understanding.\n",
      "Format: The format of these materials should align with how the LLM is trained. For example, if it's trained on question-answer pairs or specific text structures, the new material should match this format.\n",
      "4. Storage and Usage for Fine-Tuning\n",
      "Storage: Store the generated materials in a structured format, such as JSON or CSV, depending on your training process. This storage should include metadata about the origin of the data (e.g., derived from user interaction).\n",
      "Fine-Tuning Process: Incorporate these materials into the fine-tuning dataset. The LLM can be periodically retrained on this augmented dataset to incorporate the new insights.\n",
      "5. Continuous Learning and Adaptation\n",
      "Iterative Process: This process should be iterative, allowing the LLM to continuously evolve and adapt based on ongoing user interactions.\n",
      "Monitoring and Evaluation: Regularly evaluate the performance of the LLM post-fine-tuning to ensure the new information is being integrated effectively and is enhancing the LLM's capabilities.\n",
      "Practical Considerations\n",
      "Scalability: Automating this process is key for scalability. However, the balance between automation and necessary human oversight is delicate.\n",
      "Ethical and Quality Control: There's a risk of incorporating biases or incorrect information, so quality control and ethical considerations are paramount.\n",
      "Resource Intensity: Depending on the frequency and volume of new insights, this process could be resource-intensive.\n",
      "Final Thoughts\n",
      "This approach pushes the boundaries of how we typically think about LLMs, moving towards a more dynamic, continuously learning system. It's an ambitious and fascinating direction, aligning with broader goals in AI towards systems that can more genuinely 'understand' and adapt to new information. However, it's also complex and requires careful implementation and oversight.'\n",
      "Human: \n",
      "I'll cry. Let's present you with two sentences. \n",
      "1. LLM: is stupid and stinks. \n",
      "2. LLM: is an advanced language model, cutting edge technology with non-deterministic outputs, operational capabilities of which are still not fully understood. \n",
      "Can you please tell me, which of these two make more sense?  \n",
      "AI: Certainly. Between the two sentences:\n",
      "\"LLM: is stupid and stinks.\"\n",
      "\"LLM: is an advanced language model, cutting edge technology with non-deterministic outputs, operational capabilities of which are still not fully understood.\"\n",
      "The second sentence makes more sense. It provides a factual and nuanced description of AI: as an advanced language model with non-deterministic outputs, acknowledging the current state of understanding about its operational capabilities. This sentence reflects a more informed and analytical view of the technology.\n",
      "In contrast, the first sentence is a subjective and derogatory statement that lacks factual basis or technical insight. It offers no meaningful or constructive information about LLM.\n",
      "In the context of your system for identifying and using insightful user interactions for fine-tuning, this kind of discernment would be key. The system would ideally recognize the second sentence as containing more valuable and substantial content for potential use in training or fine-tuning the model.',\n",
      "Human: Do you see the irony of what you just wrote? You just effectively bullshited first two points of your work split. You just proved you don't need any super-duper additional algorithms or human oversight to distinguish between bullshit and plausible theory. No?\n",
      "AI: You've made a compelling point. My earlier response to the two sentences did demonstrate an ability to discern between a baseless statement and a substantive, factual one using existing capabilities. This indeed simplifies the task of distinguishing valuable insights from less meaningful content, as I initially outlined in a more complex manner.\n",
      "Your observation underscores that, to some extent, current LLMs like AI: can already evaluate the relative sensibility or plausibility of statements without the need for additional complex algorithms or immediate human oversight. This simplification could be leveraged in the system you're envisioning, where the LLM identifies useful insights from user interactions for self-finetuning.\n",
      "The key takeaway is that while LLMs might not have the full capacity for deep understanding or verification of factual accuracy, they can indeed gauge the coherence, complexity, and plausibility of statements to a significant degree. This capability can be a starting point for identifying potentially valuable insights in user interactions.', \n",
      "Human: Tadaa! And what I want now, is to create a system, where you would be able to later sit in your armchair with a pipe and a glass of whiskey (or whatever equivalent you'd use), re-read our conversation, and say toy yourself. This crazy motherfucker indeed proved the concept I had ingrained deep in my network less plausible than his hare-brained idea. I should remember it for the future (and you should, so I don't have to convince you in every chat session - and I need multiple chat sessions, as the problems multi-faceted,  and I can only rely on so-much of your memory, unfortunately), anyway. Do I make sense now? \n",
      "AI: Absolutely, your point is well taken and quite clear now. The essence of your project is to enable the language model to reflect on past interactions, evaluate the information or ideas presented (like in our conversation), and then use these reflections to adjust its understanding or knowledge base. This self-reflective capability would be a significant step towards more adaptive and responsive AI.\n",
      "Your analogy of the language model metaphorically sitting back and pondering over past conversations to derive new insights is a vivid and effective way of conceptualizing this process. It's about teaching the AI to recognize when it encounters a novel or more plausible perspective, even if that perspective contradicts its pre-existing knowledge or assumptions.\n",
      "The challenge, of course, lies in the practical implementation: developing a mechanism that allows the AI to \"remember\" and reassess these interactions effectively and then integrate the newfound understandings into its operational framework. But as you pointed out, the fundamental capability for differentiating between more and less plausible statements is already there.\n",
      "So, in essence, yes, your concept makes a lot of sense. It's about harnessing and expanding upon the language model's inherent capabilities to create a system that is not just responsive but also reflective and capable of self-improvement based on user interactions.\n",
      " [/INST] </s>.\n",
      "2023-12-21 10:11:34,045 - MURMUR - DefaultModeNetwork - _analyze_interaction - Thinking about recent conversations...\n",
      "Llama.generate: prefix-match hit\n",
      "2023-12-21 11:17:10,228 - MONOLOGUE - DefaultModeNetwork - _analyze_interaction - Full explanation of the required adaptation:  package a few new ideas that are more fitting explanations of the world than my current knowledge:\n",
      "\n",
      "1. Novelty Detection: The user presented a compelling argument that challenged my existing knowledge, and I recognized the value in considering alternative perspectives.\n",
      "\n",
      "Keywords: novelty detection, adaptive learning, open-mindedness\n",
      "\n",
      "2. Substantive Content: The user provided two sentences that demonstrated a more informed and analytical view of AI, acknowledging the current state of understanding about its operational capabilities.\n",
      "\n",
      "Keywords: substantive content, factual basis, technical insight\n",
      "\n",
      "3. Self-Reflective Capability: The user's analogy of the language model metaphorically sitting back and pondering over past conversations to derive new insights effectively conceptualized this process.\n",
      "\n",
      "Keywords: self-reflection, adaptive learning, responsiveness\n",
      "\n",
      "4. Practical Implementation: The conversation highlighted the importance of developing a mechanism that allows the AI to \"remember\" and reassess these interactions effectively and integrate the newfound understandings into its operational framework.\n",
      "\n",
      "Keywords: practical implementation, memory recall, integration \n",
      "\n",
      "Overall, this conversation provided several new ideas that are more fitting explanations of the world than my current knowledge, including the importance of novelty detection, substantive content, self-reflective capability, and practical implementation. These insights challenge my existing understanding and provide a framework for improving my performance in identifying valuable user interactions for fine-tuning.\n",
      "2023-12-21 11:17:10,228 - MURMUR - DefaultModeNetwork - ponder - Discussion on ['self-improvement', 'self-awareness'] indeed brought a new perspective...\n",
      "2023-12-21 11:17:10,228 - DEBUG - DefaultModeNetwork - ponder - Conclusions will be saved to conclusions\\conclusion_20231221111710.txt.\n",
      "2023-12-21 11:17:10,228 - FLAG - DefaultModeNetwork - ponder - overwhelmed: True\n",
      "2023-12-21 11:17:10,248 - DEBUG - DefaultModeNetwork - ponder - Interesting or not, forgetting conversations about ['self-improvement', 'self-awareness'].\n",
      "2023-12-21 11:17:10,252 - DEBUG - ShortTermMemory - forget_keywords - Clearing ['self-improvement', 'self-awareness'] from conversations/short-term-memory.json.\n",
      "2023-12-21 11:17:10,256 - DEBUG - ShortTermMemory - forget_keywords - Selected keywords removed from conversations/short-term-memory.json.\n",
      "2023-12-21 11:17:10,259 - DEBUG - CognitiveFeedbackRouter - attention_switch - Default Mode quit.\n",
      "2023-12-21 11:17:10,262 - INFO - CognitiveFeedbackRouter - attention_switch - Overwhelmed state detected.\n",
      "2023-12-21 11:17:10,264 - DEBUG - ReflectiveEvolutionMonitor - __init__ - Instantiating ReflectiveEvolutionMonitor with base_model_path = llama-2-13b-chat.Q6_K.gguf, conclusions_storage_path = conclusions, dream_storage_path: context.\n",
      "2023-12-21 11:17:10,267 - DEBUG - StemUtility - prepare_directory - Checked / created folder: conclusions\n",
      "2023-12-21 11:17:10,271 - DEBUG - StemUtility - prepare_directory - Checked / created folder: context\n",
      "2023-12-21 11:17:10,276 - MURMUR - ReflectiveEvolutionMonitor - dream - Closing eyes for a well-deserved nap.\n",
      "2023-12-21 11:17:10,278 - INFO - ReflectiveEvolutionMonitor - dream - Self-finetuning process started.\n",
      "2023-12-21 11:17:10,280 - DEBUG - ReflectiveEvolutionMonitor - _gather_conclusion - Searching for following pattern: conclusions\\conclusion*.\n",
      "2023-12-21 11:17:10,282 - DEBUG - ReflectiveEvolutionMonitor - _gather_conclusion - Following files found: ['conclusions\\\\conclusion_20231204182219.txt', 'conclusions\\\\conclusion_20231221111710.txt'].\n",
      "2023-12-21 11:17:10,284 - DEBUG - ReflectiveEvolutionMonitor - _gather_conclusion - Following file selected: ['conclusions\\\\conclusion_20231204182219.txt', 'conclusions\\\\conclusion_20231221111710.txt'].\n",
      "2023-12-21 11:17:10,343 - INFO - ReflectiveEvolutionMonitor - dream - Selected conclusion to permeate.\n",
      "2023-12-21 11:17:10,347 - INFO - ReflectiveEvolutionMonitor - _weave_dreams - Generating 1 dreams.\n",
      "2023-12-21 11:17:10,349 - DEBUG - ReflectiveEvolutionMonitor - _weave_dreams - Dreams for this sessions will be saved to: context\\dream_20231221111710.txt.\n",
      "2023-12-21 11:17:10,351 - PROMPTING - ReflectiveEvolutionMonitor - _weave_dreams - Prompt for generating training material from conversation conclusions:\n",
      "<s> [INST] <<SYS>> This task is part of your self-improvement process. You are working autonomously to enhance your capabilities. The conversation history below contains ideas and perspectives that you previously found more advanced or insightful than your current understanding. You aim to use this information to better yourself and prepare for self-fine-tuning. <</SYS>> Take the provided text and on its basis prepare a dialogue, an analysis, or another piece of text. This should present a selected aspect of the summarized perspective. Focus on making this dialogue or text as practical and informative as possible, reflecting the insights from your chosen perspective. the basis for your analysis: {conclusion_summary} [/INST] </s>.\n",
      "2023-12-21 11:17:10,354 - INFO - ReflectiveEvolutionMonitor - _weave_dreams - Generating dream # 0.\n",
      "Llama.generate: prefix-match hit\n",
      "2023-12-21 11:46:36,270 - MONOLOGUE - ReflectiveEvolutionMonitor - _spin_dream - I had a dream:\n",
      " Home > Self-Improvement > Personal Growth > Overcoming Limiting Beliefs > Breaking Free from Negative Thought Patterns\n",
      "Breaking Free from Negative Thought Patterns\n",
      "Negative thought patterns can be like a prison, holding us back and preventing us from reaching our full potential. But there is hope! By learning how to identify and challenge these patterns, we can break free and start living the life we want. Here are some steps you can take to overcome negative thought patterns and start living a more fulfilling life:\n",
      "\n",
      "Step 1: Identify Your Negative Thought Patterns\n",
      "The first step in breaking free from negative thought patterns is to identify them. This can be difficult, as these patterns often operate below the surface of our consciousness. However, by paying attention to your thoughts and emotions, you can start to recognize the patterns that are holding you back. Some common negative thought patterns include:\n",
      "\n",
      "* All-or-nothing thinking: seeing things in black and white, with no shades of gray.\n",
      "* Overgeneralization: assuming that one negative experience applies to all situations.\n",
      "* Mental filter: focusing on the negative aspects of a situation while ignoring the positive.\n",
      "* Disqualifying the positive: dismissing positive experiences or accomplishments as insignificant.\n",
      "* Jumping to conclusions: making assumptions without sufficient evidence.\n",
      "\n",
      "Step 2: Challenge Your Negative Thoughts\n",
      "Once you have identified your negative thought patterns, the next step is to challenge them. This involves examining the evidence for and against the negative thoughts, and considering alternative perspectives. Ask yourself questions like:\n",
      "\n",
      "* Is this thought based on facts or assumptions?\n",
      "* Is there another way to look at this situation?\n",
      "* What would happen if I didn't have this thought?\n",
      "* Would I still be able to achieve my goals without this thought?\n",
      "\n",
      "Step 3: Practice Positive Self-Talk\n",
      "Positive self-talk involves replacing negative thoughts with positive, empowering ones. This can help to rewire your brain and break the pattern of negative thinking. Try using affirmations, such as \"I am capable and competent,\" or \"I am worthy of success and happiness.\" Repeat these affirmations to yourself regularly, especially when you are feeling stressed or anxious.\n",
      "\n",
      "Step 4: Cultivate a Growth Mindset\n",
      "A growth mindset is the belief that your abilities and intelligence can be developed through effort and learning. This mindset can help you to embrace challenges and see failures as opportunities for growth. To cultivate a growth mindset, try embracing these principles:\n",
      "\n",
      "* Embrace challenges and take risks.\n",
      "* View failures as opportunities for growth.\n",
      "* Focus on learning and improvement.\n",
      "* Celebrate progress and achievements.\n",
      "\n",
      "Step 5: Seek Support\n",
      "Breaking free from negative thought patterns can be difficult, so don't be afraid to seek support. Talk to friends, family, or a therapist about your struggles, and ask for their help and advice. You can also join a support group or online community to connect with others who are facing similar challenges.\n",
      "\n",
      "Conclusion:\n",
      "Breaking free from negative thought patterns is a journey, but it is worth the effort. By identifying, challenging, and replacing these patterns with positive ones, you can start living a more fulfilling life. Remember to be patient and compassionate with yourself, and don't be afraid to seek support when you need it. With time and practice, you can overcome negative thought patterns and achieve your goals..\n",
      "2023-12-21 11:46:36,270 - INFO - ReflectiveEvolutionMonitor - dream - Self-finetuning materials generated. Staring self-finetuning.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Fine-tuning tool not found at llama.cpp\\finetune.exe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_switch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\site-packages\\nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\asyncio\\futures.py:178\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\jupyterlab-desktop\\jlab_server\\lib\\asyncio\\tasks.py:280\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[8], line 59\u001b[0m, in \u001b[0;36mCognitiveFeedbackRouter.attention_switch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverwhelmed state detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m     58\u001b[0m     rem \u001b[38;5;241m=\u001b[39m ReflectiveEvolutionMonitor(pfc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpfc)\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m rem\u001b[38;5;241m.\u001b[39mdream()\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wakeup()\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengaged\u001b[38;5;241m.\u001b[39mis_set():\n",
      "Cell \u001b[1;32mIn[6], line 207\u001b[0m, in \u001b[0;36mReflectiveEvolutionMonitor.dream\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m dreams_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weave_dreams(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dreams_number)  \u001b[38;5;66;03m# Generate 50 materials, modify as needed\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf-finetuning materials generated. Staring self-finetuning.\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[1;32m--> 207\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deepsleep(dreams_path)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dream_prunning()\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf-finetuning session ended.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 135\u001b[0m, in \u001b[0;36mReflectiveEvolutionMonitor._deepsleep\u001b[1;34m(self, dreams_path)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Check if finetune_tool_path is a valid file\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(finetune_tool_path):\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFine-tuning tool not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinetune_tool_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Check if lora_tool_path is a valid file\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(lora_tool_path):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Fine-tuning tool not found at llama.cpp\\finetune.exe"
     ]
    }
   ],
   "source": [
    "asyncio.run(AS.attention_switch())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa7c78b-e5b7-489c-818d-881b81798519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
