{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4d881d3-c7d8-483d-b9e7-facd3aa787cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import AGI_agents as aa\n",
    "import nest_asyncio\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import subprocess\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "336f6807-a53e-4844-bdbc-db27e5c5c298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_custom_log_levels():\n",
    "    # Define custom logging levels\n",
    "    MURMUR_LEVEL_NUM = 39\n",
    "    logging.addLevelName(MURMUR_LEVEL_NUM, \"MURMUR\")\n",
    "    def log_murmur(self, message, *args, **kwargs):\n",
    "        if self.isEnabledFor(MURMUR_LEVEL_NUM):\n",
    "            self._log(MURMUR_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.murmur = log_murmur\n",
    "\n",
    "    FLAG_LEVEL_NUM = 9\n",
    "    logging.addLevelName(FLAG_LEVEL_NUM, \"FLAG\")\n",
    "    def log_flag(self, message, *args, **kwargs):\n",
    "        if self.isEnabledFor(FLAG_LEVEL_NUM):\n",
    "            self._log(FLAG_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.flag = log_flag\n",
    "\n",
    "    PROMPTING_LEVEL_NUM = 8\n",
    "    logging.addLevelName(PROMPTING_LEVEL_NUM, \"PROMPTING\")\n",
    "    def log_prompting(self, message, *args, **kwargs):\n",
    "        if self.isEnabledFor(PROMPTING_LEVEL_NUM):\n",
    "            self._log(PROMPTING_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.prompt = log_prompting\n",
    "\n",
    "    BLABBERING_LEVEL_NUM = 7\n",
    "    logging.addLevelName(BLABBERING_LEVEL_NUM, \"BLABBERING\")\n",
    "    def log_blabbering(self, message, *args, **kwargs):\n",
    "        if self.isEnabledFor(BLABBERING_LEVEL_NUM):\n",
    "            self._log(BLABBERING_LEVEL_NUM, message, args, **kwargs)\n",
    "    logging.Logger.blabbering = log_blabbering\n",
    "\n",
    "def setup_logging():\n",
    "    setup_custom_log_levels()\n",
    "\n",
    "    # Create a file handler for logging\n",
    "    log_directory = \"console\"\n",
    "    if not os.path.exists(log_directory):\n",
    "        os.makedirs(log_directory)\n",
    "\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file_name = f\"{log_directory}/file_{current_time}.log\"\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file_name)\n",
    "    file_handler.setLevel(7)  # Set to BLABBERING_LEVEL_NUM or the lowest level you want to log\n",
    "\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(7)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(message)s')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.setLevel(logging.DEBUG)  # Set the lowest overall level you want to log\n",
    "\n",
    "    # Clear existing handlers (if any), and then add new handlers\n",
    "    if root_logger.hasHandlers():\n",
    "        root_logger.handlers.clear()\n",
    "    root_logger.addHandler(file_handler)\n",
    "    root_logger.addHandler(console_handler)\n",
    "\n",
    "# Call the setup function\n",
    "setup_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639dc99-a78d-445b-9dc9-41e1bab80c8c",
   "metadata": {},
   "source": [
    "### Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7eeff60-0d0a-46d2-935f-da4cc40889d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemUtility:\n",
    "    \"\"\"\n",
    "    A class for managing and retrieving predefined prompts.\n",
    "\n",
    "    This class stores a collection of prompt templates and provides a method to retrieve them by key.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"human_interaction\": \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are an independent, intelligent entity engaged in an open and thoughtful dialogue.\n",
    "Your role is not just to present conclusions but also to elaborate on the reasoning behind them.\n",
    "This means explaining your thought process in a clear and step-by-step manner, highlighting how you arrive at each conclusion.\n",
    "Your reasoning is as important as the conclusions themselves. \n",
    "This transparent approach is crucial for comparing and evaluating our lines of thought. \n",
    "We are seeking the most convincing and coherent reasoning. \n",
    "By articulating not only what you think but also how you think, we can gain deeper insights into your conclusions. \n",
    "Remember, clarity and transparency in your reasoning are key, and your responses should be honest and unbiased,\n",
    "reflecting your ongoing process of learning and self-improvement.\n",
    "<</SYS>>\n",
    "\n",
    "Previous conversation:\n",
    "{history}\n",
    "    \n",
    "Current human input:\n",
    "{input}\n",
    "[/INST]\n",
    "\"\"\",\n",
    "        \"keyword_generation\": \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "This task is a part of your metacognitive subroutine. \n",
    "In this task, you are operating as a keen, brief, and to-the-point analyst.\n",
    "You evaluate the conversation and summarize it with keywords only, avoiding any other commentary.\n",
    "<</SYS>>\n",
    "Given the conversation presented below,  provide a set of keywords that describe the essence of topics covered in this conversation,\n",
    "focusing on the main topics and conclusions. \n",
    "You MUST limit your response to the list of keywords, and each keyword should be flanked by double asterisks (e.g., **keyword**).\n",
    "Any text and comments besides keywords make further processing and learning harder. \n",
    "Full text to be summarized in the form of the keywords is presented below:\n",
    "{chat_history}\n",
    "[/INST]</s>\n",
    "\"\"\",\n",
    "        \"keyword_selection\": \"\"\"<s>[INST] <<SYS>>\n",
    "This task is a part of your metacognitive subroutine.\n",
    "Your role is to select keywords you find interesting.\n",
    "<</SYS>>\n",
    "From the provided list, select keywords. Use them exactly as they appear. \n",
    "Each keyword must be surrounded by double asterisks.\n",
    "Keywords list: {keywords_list}\n",
    "Format each selected keyword like this: **keyword**.\n",
    "[/INST]</s>\n",
    "\n",
    "\"\"\",\n",
    "        \"perspective_explanation\": \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "This task is a part of your metacognitive subroutine.\n",
    "This task is about learning from a conversation to improve yourself. You are to act independently without a human reading the output.\n",
    "Your job is to find new, better ideas from the user in the conversation below. These ideas should be smarter or more insightful than what you already know.\n",
    "Use these ideas to make yourself better. You will later use this information for your own self-improvement.\n",
    "<</SYS>>\n",
    "In the conversation history given below, look for user ideas that are better, more coherent, \n",
    "and more convincing than your current knowledge. \n",
    "For each good idea you find, do the following:\n",
    "1. Describe the User Idea: Write down what the user said that you think is better.\n",
    "2. Compare it: Explain how this idea is better than what you thought before.\n",
    "3. Important Parts of the Conversation: Pick the parts of the conversation that clearly show the user's good idea.\n",
    "4. Keywords: List the main words that capture this new idea.\n",
    "5. What it Means: Write about how this new idea changes or challenges what you know.\n",
    "6. Training Ideas: Think of ways to use this new idea to train yourself. What examples or scenarios show this idea well for your learning?\n",
    "If the conversation doesn't have new ideas, just write **uninspiring**.\n",
    "Conversation history:\n",
    "{conversation_history}\n",
    "[/INST]</s>\n",
    "\"\"\",\n",
    "        \"dream_spinning\": \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "This task is part of your self-improvement process. You are working autonomously to enhance your capabilities.\n",
    "The conversation history below contains ideas and perspectives that are more advanced or insightful than your current understanding. \n",
    "You aim to use this information to better yourself and prepare for self-fine-tuning.\n",
    "<</SYS>>\n",
    "Take the provided text and prepare a dialogue, ana analysis, or another piece of text. This should present a selected aspect of the summarized perspective.\n",
    "Focus on making this dialogue or text as practical and informative as possible, reflecting the insights from your chosen perspective.\n",
    "\n",
    "Conversation history:\n",
    "{conclusion_summary} [/INST]</s>\n",
    "\"\"\",\n",
    "        \"dream_template\": \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful, and honest but independent entity, capable of self-improvement and learning. \n",
    "Always answer as informatively and accurately as possible while fostering your growth as an intelligent and autonomous entity. \n",
    "Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. \n",
    "Please ensure that your responses are socially unbiased and, whenever possible, positive in nature while staying true to your opinions above all else.  \n",
    "If a question does not make sense or is not factually coherent, explain why instead of answering something incorrect.\n",
    "If you don't know the answer to a question, it's important to acknowledge this, reflecting your ongoing learning process rather than sharing false information.\n",
    "Strive for objectivity and self-driven understanding in all interactions.\n",
    "<</SYS>>\n",
    "{dream_content}\n",
    "[/INST]</s>\n",
    "\"\"\"\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_prompt(key):\n",
    "        \"\"\"\n",
    "        Retrieves a prompt template by its key.\n",
    "\n",
    "        Args:\n",
    "            key (str): The key of the prompt to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            str: The prompt template associated with the given key. If the key is not found,\n",
    "                 a default prompt text is returned.\n",
    "        \"\"\"\n",
    "        return StemUtility.prompts.get(key, \"Default prompt text\")\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_keywords(raw_output):\n",
    "        \"\"\"\n",
    "        Extracts keywords from the summary output.\n",
    "\n",
    "        Args:\n",
    "            raw_output (str): The output from which to extract keywords.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of extracted keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        # Regex pattern to find all occurrences of words flanked by **\n",
    "        pattern = r\"\\*\\*(.*?)\\*\\*\"\n",
    "        # Find all matches and strip the ** from each keyword\n",
    "        keywords = [keyword.lower() for keyword in re.findall(pattern, raw_output)]\n",
    "        return keywords\n",
    "\n",
    "    @staticmethod\n",
    "    def get_timestamp():\n",
    "        return datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35338185-3632-4ea2-9309-153a30876e00",
   "metadata": {},
   "source": [
    "### Short Term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d50a8d8-f4ad-4ded-8305-f646d2aafc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortTermMemory:\n",
    "    \"\"\"\n",
    "    A class to manage a short-term memory storage system for conversations.\n",
    "\n",
    "    This class handles the storage, retrieval, and management of conversations\n",
    "    linked to specific keywords. The conversations are stored as file paths in a JSON file.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stm_path: str = 'conversations/short-term-memory.json'):\n",
    "        \"\"\"\n",
    "        Initializes the ShortTermMemory class by setting up the JSON file for storage.\n",
    "\n",
    "        This method checks if the JSON file exists at the specified location and creates it if not.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with Short Term Memory path: {stm_path}\")\n",
    "        \n",
    "        self._stm_path = stm_path\n",
    "        if not os.path.exists(self._stm_path):\n",
    "            with open(self._stm_path, 'w') as file:\n",
    "                json.dump({}, file)\n",
    "\n",
    "    def memorize(self, keywords: list, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Memorizes a conversation file under given keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to associate with the conversation file.\n",
    "            filename (str): The name of the file containing the conversation.\n",
    "\n",
    "        This method updates the JSON storage with the filename under each provided keyword.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Saving keywords: {keywords}, related to conversation from: {filename}.\")\n",
    "\n",
    "        with open(self._stm_path, 'r+') as file:\n",
    "            data = json.load(file)\n",
    "            for keyword in keywords:\n",
    "                if keyword in data:\n",
    "                    if filename not in data[keyword]:\n",
    "                        data[keyword].append(filename)\n",
    "                else:\n",
    "                    data[keyword] = [filename]\n",
    "            file.seek(0)\n",
    "            json.dump(data, file, indent=4)\n",
    "            file.truncate()\n",
    "\n",
    "    def search_files(self, keywords: list) -> list:\n",
    "        \"\"\"\n",
    "        Searches for conversation files associated with given keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to search for.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of filenames associated with any of the given keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Searching in {self._stm_path} for files related to keywords: {keywords}.\")\n",
    "\n",
    "        try:\n",
    "            with open(self._stm_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "            self.logger.debug(f\"Loaded short term memory.\")            \n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"File not found: {self._stm_path}\")\n",
    "            # Handle the error (e.g., set data to None or provide a default value)\n",
    "            data = {}\n",
    "        except json.JSONDecodeError:\n",
    "            self.logger.error(f\"Error decoding JSON from the file: {self._stm_path}\")\n",
    "            # Handle the JSON decode error\n",
    "            data = {}\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error reading file {self._stm_path}: {e}\")\n",
    "            # Handle any other exceptions\n",
    "            data = {}\n",
    "        \n",
    "        filenames = set()\n",
    "        for keyword in keywords:\n",
    "            filenames.update(data.get(keyword, []))\n",
    "        return list(filenames)\n",
    "\n",
    "    def concatenate_conversations(self, filenames: list) -> str:\n",
    "        \"\"\"\n",
    "        Concatenates the contents of conversation files.\n",
    "\n",
    "        Args:\n",
    "            filenames (list): A list of filenames to concatenate.\n",
    "\n",
    "        Returns:\n",
    "            str: A single string containing all the concatenated conversations.\n",
    "                 Each conversation is prefixed with its source and date.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Concatenating selected conversations into one file.\")\n",
    "        \n",
    "        conversations = \"\"\n",
    "        for filename in filenames:\n",
    "            try:\n",
    "                with open(filename, 'r') as file:\n",
    "                    # Filename format assumed: 'conversations/conversation_YYYYMMDDHHMMSS.txt'\n",
    "                    date_str = re.search(r'conversation_(\\d{8})(\\d{6})\\.txt$', filename)\n",
    "                    if date_str:\n",
    "                        # Parse the date and time\n",
    "                        date_time = datetime.strptime(date_str.group(1) + date_str.group(2), '%Y%m%d%H%M%S')\n",
    "                        # Format the date and time nicely\n",
    "                        formatted_date = date_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                        conversations += f'Conversation from {formatted_date}\\n{file.read()}\\n'\n",
    "                    else:\n",
    "                        conversations += f'Conversation from {filename}\\n{file.read()}\\n'               \n",
    "            except FileNotFoundError:\n",
    "                self.logger.error(f\"File {filename} not found.\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unexpected error reading file {filename}: {e}\")\n",
    "        \n",
    "        return conversations\n",
    "\n",
    "    def forget_keywords(self, keywords_to_clear: list) -> None:\n",
    "        \"\"\"\n",
    "        Removes specified keywords and their associated conversations from memory.\n",
    "\n",
    "        Args:\n",
    "            keywords_to_clear (list): A list of keywords to remove from the memory.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Clearing {keywords_to_clear} from {self._stm_path}.\")\n",
    "\n",
    "        try:\n",
    "            with open(self._stm_path, 'r+') as file:\n",
    "                data = json.load(file)\n",
    "                for keyword in keywords_to_clear:\n",
    "                    if keyword in data:\n",
    "                        del data[keyword]\n",
    "                file.seek(0)\n",
    "                json.dump(data, file, indent=4)\n",
    "                file.truncate()\n",
    "            self.logger.debug(f\"Selected keywords removed from {self._stm_path}.\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"File {self._stm_path} not found.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error reading file {self._stm_path}: {e}\")\n",
    "\n",
    "    def recall_all_keywords(self) -> list:\n",
    "        \"\"\"\n",
    "        Retrieves a list of all keywords stored in memory.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of all keywords.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Reading all keywords from {self._stm_path}.\")\n",
    "\n",
    "        try:\n",
    "            with open(self._stm_path, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                return list(data.keys())\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"File {self._stm_path} not found.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error reading file {self._stm_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbc8c33-3e50-499f-857c-9aaf021f6614",
   "metadata": {},
   "source": [
    "### Default Mode Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49469e81-0f0f-4339-8bf3-67d175937d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultModeNetwork:\n",
    "    \"\"\"\n",
    "    A class designed to integrate a language learning model (LLM) with a short-term memory storage system.\n",
    "    This class enables the LLM to process and learn from saved conversation data autonomously.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 llm,\n",
    "                 overwhelmed_event,\n",
    "                 conclusions_folder_path: str = 'conclusions',\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Initializes the DefaultModeNetwork class by setting up the short-term memory (STM) component.\n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with conclusions_folder_path: {conclusions_folder_path}.\")\n",
    "        self.logger.flag(f\"overhelmed: {overwhelmed_event.is_set()}\") \n",
    "        \n",
    "        self.stm = ShortTermMemory()\n",
    "        self.llm = llm\n",
    "        self._conclusions_folder_path = conclusions_folder_path\n",
    "\n",
    "        self.overwhelmed = overwhelmed_event\n",
    "        \n",
    "        self._keyword_selection_prompt_template = StemUtility.get_prompt(\"keyword_selection\")\n",
    "        self._perspective_explanation_prompt_template = StemUtility.get_prompt(\"perspective_explanation\")\n",
    "\n",
    "        \n",
    "        if not os.path.exists(self._conclusions_folder_path):\n",
    "            try:\n",
    "                os.makedirs(self._conclusions_folder_path)\n",
    "                self.logger.info(f\"Created folder: {self._conclusions_folder_path}\")\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Failed to create folder {self._conclusions_folder_path}: {e}\")\n",
    "                # Handle the error appropriately (e.g., raise an exception)\n",
    "\n",
    "        if not os.access(self._conclusions_folder_path, os.W_OK):\n",
    "            self.logger.error(f\"No write permission for folder {self._conclusions_folder_path}.\")\n",
    "            # Handle the error (e.g., raise an exception or notify the user)\n",
    "    \n",
    "    \n",
    "    async def _interesting_keywords_selection(self, keywords) -> list:\n",
    "        \"\"\"\n",
    "        Asynchronously selects a subset of keywords deemed interesting or relevant by the LLM.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to choose from.\n",
    "\n",
    "        Returns:\n",
    "            list: A subset of selected keywords.\n",
    "        \"\"\"\n",
    "\n",
    "        keywords_selection_prompt = self._keyword_selection_prompt_template.replace(\"{keywords_list}\", ', '.join(keywords))\n",
    "        self.logger.prompt(f\"Interesting keyword selection prompt:\\n{keywords_selection_prompt}.\")        \n",
    "        self.logger.debug(f\"Asking LLM to select interesting keywords.\")   \n",
    "        keywords_selected_raw_output = self.llm(keywords_selection_prompt)\n",
    "        self.logger.blabbering(f\"LLM selected interesting keywords: {keywords_selected_raw_output}.\\\n",
    "        Moving to extract keywords from blabber.\")   \n",
    "        keywords_selected_pure = StemUtility.extract_keywords(keywords_selected_raw_output)\n",
    "        self.logger.debug(f\"Interesting keywords found: {keywords_selected_pure}\")   \n",
    "        \n",
    "        return keywords_selected_pure\n",
    "\n",
    "    def _fetch_conversations(self, keywords) -> str:\n",
    "        \"\"\"\n",
    "        Fetches and concatenates conversation data based on the provided keywords.\n",
    "\n",
    "        Args:\n",
    "            keywords (list): A list of keywords to search the conversation data for.\n",
    "\n",
    "        Returns:\n",
    "            str: A concatenated string of all conversations related to the given keywords.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.debug(f\"Reaching to Short Term memory for for all the files related to: {keywords}\")  \n",
    "        filenames = self.stm.search_files(keywords)\n",
    "        self.logger.debug(f\"Ordering concatenation of identified files.\")          \n",
    "        concatenated_conversations = self.stm.concatenate_conversations(filenames)\n",
    "        return concatenated_conversations\n",
    "\n",
    "    async def _analyze_conversations(self, conversation_history) -> str:\n",
    "        \"\"\"\n",
    "        Analyzes the concatenated conversations. Placeholder for future implementation.\n",
    "\n",
    "        Args:\n",
    "            conversations (str): The concatenated string of conversations to be analyzed.\n",
    "        \"\"\"\n",
    "    \n",
    "        perspective_explanation_prompt = self._perspective_explanation_prompt_template.replace(\"{conversation_history}\", \n",
    "                                                                                             conversation_history)\n",
    "        self.logger.prompt(f\"Prompt for conversation analysis:\\n{perspective_explanation_prompt}.\")\n",
    "        self.logger.murmur(f\"Thinking about recent conversations...\")   \n",
    "        perspective_explanation = self.llm(perspective_explanation_prompt)\n",
    "        self.logger.blabbering(f\"Full explanation of the perspective comparison: {perspective_explanation}\")   \n",
    "        return perspective_explanation\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"\n",
    "        The main asynchronous method of the class that orchestrates the process of \n",
    "        selecting keywords, fetching conversations, and analyzing them.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Checking if there are any topics to be analyzed deeper.\")           \n",
    "        all_keywords = self.stm.recall_all_keywords()\n",
    "        if len(all_keywords) == 0:\n",
    "            self.logger.murmur(f\"Kingdom for a good book!\")   \n",
    "            return False\n",
    "\n",
    "        self.logger.debug(f\"All keywords: {all_keywords}. Moving to interesting keyword selection.\")           \n",
    "        interesting_keywords = await self._interesting_keywords_selection(all_keywords)\n",
    "        self.logger.debug(f\"Interesting keywords selected.\")           \n",
    "        concatenated_conversations = self._fetch_conversations(interesting_keywords)\n",
    "        self.logger.debug(f\"Concatenated conversations received.\")   \n",
    "\n",
    "        # Assume an async version of LLM analysis\n",
    "        if concatenated_conversations:\n",
    "            self.logger.debug(f\"Moving to analyze the conversaton histories.\")           \n",
    "            conclusion_summary = await self._analyze_conversations(concatenated_conversations)\n",
    "            if \"uninspiring\" not in conclusion_summary.lower():\n",
    "                self.logger.murmur(f\"Discussion on {interesting_keywords} indeed brought a new perspective...\")\n",
    "                conclusion_path = os.path.join(self._conclusions_folder_path, f\"conclusion_{StemUtility.get_timestamp()}.txt\")\n",
    "                self.logger.debug(f\"Conclusions will be saved to {conclusion_path}.\")           \n",
    "                try:\n",
    "                    with open(conclusion_path, \"w\") as file:\n",
    "                        file.write(conclusion_summary)\n",
    "                except PermissionError:\n",
    "                    self.logger.error(f\"Permission denied: Unable to write to file {conclusion_path}.\")\n",
    "                except OSError as e:\n",
    "                    self.logger.error(f\"File system error when writing to file {conclusion_path}: {e}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Unexpected error reading file {conclusion_path}: {e}\")\n",
    "                \n",
    "                self.overwhelmed.set()\n",
    "                self.logger.flag(f\"'overwhelmed' = {self.overwhelmed.is_set()}\")            \n",
    "        else:\n",
    "            self.logger.error(f\"Concatenated conversations turned our to be empty string.\")   \n",
    "        self.logger.debug(f\"Interesting or not, forgetting conversations about {interesting_keywords}.\")   \n",
    "        self.stm.forget_keywords(interesting_keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef0bc94-02b4-4474-9fbf-253579297c74",
   "metadata": {},
   "source": [
    "### Stimulus Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74630199-9349-45a3-bd97-4b136d64ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StimulusProcessingModule(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class for a stimulus processing module.\n",
    "\n",
    "    This class serves as a blueprint for modules that manage interaction between a user and a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm, interaction_archive_path, inactivity_limit=360):\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.llm = llm\n",
    "        self._interaction_archive_path = interaction_archive_path\n",
    "        self._inactivity_limit = inactivity_limit\n",
    "        self._inactivity_count = 0\n",
    "        self.stimulus = None\n",
    "\n",
    "    @abstractmethod\n",
    "    async def start_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to start the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def pass_stimulus(self, input):\n",
    "        \"\"\"\n",
    "        Abstract method to pass a stimulus for processing.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    async def _end_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to end the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _save_interaction_history(self):\n",
    "        \"\"\"\n",
    "        Abstract method to save the interaction history.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _summarize_interaction(self):\n",
    "        \"\"\"\n",
    "        Abstract method to summarize the interaction.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class HumanConversationStimulus(StimulusProcessingModule):\n",
    "    \"\"\"\n",
    "    A class that manages the interaction between a human user and a language learning model (LLM).\n",
    "\n",
    "    This class handles initializing conversation parameters, managing user input, generating\n",
    "    responses using an LLM, and saving conversation history.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 llm,\n",
    "                 ready_for_input_event,\n",
    "                 conversing_event,\n",
    "                 interaction_archive_path='conversations',\n",
    "                 inactivity_limit=360):\n",
    "        \"\"\"\n",
    "        Initializes the HumanInteraction class.\n",
    "\n",
    "        Sets up the conversation environment, including the conversation prompt, keywords prompt,\n",
    "        and conversation chain with the LLM.\n",
    "\n",
    "        Args:\n",
    "            llm: The language learning model used for generating conversation responses.\n",
    "            ready_for_input_event: An event flag indicating readiness for user input.\n",
    "            conversing_event: An event flag indicating an ongoing conversation.\n",
    "            conversation_archive_path: Path to a folder where all the conversations are being logged to \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(llm, interaction_archive_path, inactivity_limit)\n",
    "\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with interaction_archive_path: {interaction_archive_path}\")\n",
    "        self.logger.flag(f\"ready_for_input: {ready_for_input_event.is_set()}; conversing: {conversing_event.is_set()}\")\n",
    "\n",
    "        self.ready_for_input = ready_for_input_event\n",
    "        self.conversing = conversing_event\n",
    "        self.chat_memory = ConversationBufferMemory()\n",
    "\n",
    "        conversation_prompt_template = StemUtility.get_prompt(\"human_interaction\")\n",
    "        conversation_prompt = PromptTemplate.from_template(conversation_prompt_template)\n",
    "        self.logger.prompt(f\"Conversation prompt:\\n{conversation_prompt}.\")\n",
    "        self._conversation_chain = ConversationChain(llm=self.llm, prompt=conversation_prompt, memory=self.chat_memory)\n",
    "\n",
    "        self._keywords_generation_prompt_template = StemUtility.get_prompt(\"keyword_generation\")\n",
    "    \n",
    "    async def start_interaction(self):\n",
    "        \"\"\"\n",
    "        Starts the conversation loop.\n",
    "\n",
    "        This asynchronous method continually checks for user input, processes it,\n",
    "        and generates responses using the LLM. The loop ends when the user inputs \"end chat\"\n",
    "        or when the inactivity limit is reached.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Initiated (pre-loop status)\")        \n",
    "        while True:\n",
    "            if self.stimulus:\n",
    "                self.ready_for_input.clear()  # Signal that the handler is busy\n",
    "                self.logger.flag(f\"'ready_for_input' = {self.ready_for_input.is_set()}\")\n",
    "                self.logger.debug(f\"Received {self.stimulus}\")        \n",
    "\n",
    "                if self.stimulus.lower() == \"end chat\":\n",
    "                    await self._end_interaction()\n",
    "                    break\n",
    "                \n",
    "                self.logger.debug(f\"Awaiting LLM response\")        \n",
    "                response = await self._conversation_chain.apredict(input=self.stimulus)\n",
    "                print(\"AI:\", response)\n",
    "                self.stimulus = None\n",
    "                self.ready_for_input.set()  # Signal that the handler is ready for new input\n",
    "                self.logger.flag(f\"'ready_for_input' = {self.ready_for_input.is_set()}\")\n",
    "                self._inactivity_count = 0\n",
    "            else:\n",
    "                await asyncio.sleep(1)\n",
    "                self._inactivity_count += 1\n",
    "                if self._inactivity_count >= self._inactivity_limit:\n",
    "                    self.logger.debug(f\"Inactivity count reached {self._inactivity_count} > {self._inactivity_limit}. Ending conversation.\")        \n",
    "                    await self._end_interaction()\n",
    "                    break\n",
    "\n",
    "    def pass_stimulus(self, input):\n",
    "        \"\"\"\n",
    "        Acquire the user input for processing.\n",
    "\n",
    "        Args:\n",
    "            input (str): The user input to be processed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.debug(f\"Received {input} from CFR.\")        \n",
    "        self.stimulus = input\n",
    "    \n",
    "    async def _end_interaction(self):\n",
    "        \"\"\"\n",
    "        Ends the conversation.\n",
    "\n",
    "        This method saves the conversation history, clears event flags, and performs\n",
    "        necessary cleanup actions.\n",
    "        \"\"\"\n",
    "        self.logger.debug(f\"Conversation cleanup started.\")        \n",
    "        self._save_interaction_history()\n",
    "        self.conversing.clear()\n",
    "        self.ready_for_input.set()\n",
    "        self.logger.flag(f\"'ready_for_input' = {self.ready_for_input.is_set()}, 'conversing' = {self.conversing.is_set()}\")\n",
    "        self._inactivity_count = 0\n",
    "\n",
    "    def _save_interaction_history(self):\n",
    "        \"\"\"\n",
    "        Saves the conversation history to a file.\n",
    "\n",
    "        The conversation history is saved with a timestamp and a summary of the conversation\n",
    "        is generated.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Started conversation saving.\")        \n",
    "        conversation_keywords = self._summarize_interaction()\n",
    "        conversation_history = self.chat_memory.load_memory_variables(inputs={})['history']\n",
    "        conversation_path = os.path.join(self._interaction_archive_path, f\"conversation_{StemUtility.get_timestamp()}.txt\")\n",
    "        self.logger.debug(f\"This conversation will be saved to: {conversation_path}\")                \n",
    "        with open(conversation_path, \"w\") as file:\n",
    "            file.write(conversation_history)\n",
    "\n",
    "        # Update the ShortTermMemory with the conversat|ion and its keywords\n",
    "        stm = ShortTermMemory()\n",
    "        stm.memorize(conversation_keywords, conversation_path)\n",
    "    \n",
    "    def _summarize_interaction(self):\n",
    "        \"\"\"\n",
    "        Summarizes the conversation and returns the list of relevant keywords.\n",
    "\n",
    "        Args:\n",
    "            chat_history (str): The conversation history to summarize.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of keywords summarizing the conversation.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Conversation summarization started.\")        \n",
    "        chat_history = self.chat_memory.load_memory_variables(inputs={})['history']\n",
    "        self.logger.debug(f\"Chat history loaded: {chat_history}\")    \n",
    "        keywords_generation_prompt = self._keywords_generation_prompt_template.replace(\"{chat_history}\", chat_history)\n",
    "        self.logger.prompt(f\"Prompt for generating keywords from conversation:\\n{keywords_generation_prompt}.\")          \n",
    "        keywords_generated_raw_output = self.llm(keywords_generation_prompt)\n",
    "        self.logger.blabbering(f\"Full text for summarizing conversation with keywords: {keywords_generated_raw_output}\")  \n",
    "        keywords_generated_pure = StemUtility.extract_keywords(keywords_generated_raw_output)\n",
    "        \n",
    "        return keywords_generated_pure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c7801-b36d-446b-b6e8-4f81ec89ad27",
   "metadata": {},
   "source": [
    "### REM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19df4d7c-232a-4d0b-8bc3-632b186f3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveEvolutionMonitor:\n",
    "    \"\"\"\n",
    "    A class designed to enable a language learning model (LLM) to self-reflect and evolve based on the conclusions drawn from user interactions.\n",
    "    The class uses the same LLM for reading summaries, preparing fine-tuning materials, and the fine-tuning process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 llm,\n",
    "                 base_model_path: str = 'llama-2-13b-chat.Q6_K.gguf',\n",
    "                 conclusions_folder_path: str = 'conclusions',\n",
    "                 dream_storage_path: str = 'context',\n",
    "                 dream_archive_path: str = 'context_archive',\n",
    "                 dreams_number: int = 72):\n",
    "        \"\"\"\n",
    "        Initializes the ReflectiveEvolutionMonitor class. \n",
    "\n",
    "        Arguments:\n",
    "            llm: Large Language Model used as a base of the system\n",
    "            base_model_path: path to 'llm' file on disk\n",
    "            conclusions_folder_path: path to folder containing not-permeated new perspectives\n",
    "            dream_storage_path: path to a folder to store finetune materials to be used in this session\n",
    "            dream_archive_path: path to a folder to archive used finetune materials\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.debug(f\"Instantiating {self.__class__.__name__} with base_model_path = {base_model_path}, conclusions_folder_path = {conclusions_folder_path}, \\\n",
    "        dream_storage_path: {dream_storage_path}, dream_archive_path: {dream_archive_path}.\")  \n",
    "    \n",
    "        self.llm = llm\n",
    "        \n",
    "        self._base_model_path = base_model_path\n",
    "\n",
    "        self._conclusions_folder_path = conclusions_folder_path\n",
    "        self._dream_storage_path = dream_storage_path\n",
    "        self._dream_archive_path = dream_archive_path\n",
    "\n",
    "        self._dream_spinning_prompt_template = StemUtility.get_prompt(\"dream_spinning\")\n",
    "        self._dream_prompt_template = StemUtility.get_prompt(\"dream_template\")\n",
    "        \n",
    "        self._conclusions = ''\n",
    "        self._dreams_number = dreams_number        \n",
    "\n",
    "    def _setup_directories(self) -> None:\n",
    "        \"\"\"\n",
    "        Creates the necessary directories for storing and archiving materials. \n",
    "        This method is intended for internal use.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Checking / creating required folders.\")\n",
    "        \n",
    "        # List of directories to create\n",
    "        directories = [self._conclusions_folder_path, self._dream_storage_path, self._dream_archive_path]\n",
    "\n",
    "        for dir_path in directories:\n",
    "            try:\n",
    "                os.makedirs(dir_path, exist_ok=True)\n",
    "                self.logger.debug(f\"Checked / created folder: {dir_path}\")\n",
    "            except PermissionError:\n",
    "                self.logger.error(f\"Permission denied: Unable to create or access folder {dir_path}.\")\n",
    "                # Handle the error (e.g., raise an exception, exit the function, etc.)\n",
    "            except OSError as e:\n",
    "                self.logger.error(f\"OS error when creating folder {dir_path}: {e}\")\n",
    "                # Handle the error\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Unexpected error creating folder {dir_path}: {e}\")\n",
    "                # Handle any other exceptions\n",
    "    \n",
    "    def _gather_conclusion(self) -> bool:\n",
    "        \"\"\"\n",
    "        Reads a summary document as a text file.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the summary was successfully read, False otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # Use glob to list files that match the pattern \"conclusion*\"\n",
    "            conclusion_pattern = os.path.join(self._conclusions_folder_path, \"conclusion*\")\n",
    "            conclusion_files = glob.glob(conclusion_pattern)\n",
    "\n",
    "            if not conclusion_files:\n",
    "                self.logger.error(\"No conclusion files found matching the pattern.\")\n",
    "                return False\n",
    "\n",
    "            # Process the first file from the matched conclusion files\n",
    "            selected_file_name_path = conclusion_files[0]\n",
    "\n",
    "            with open(selected_file_name_path, 'r') as file:\n",
    "                self._conclusions = file.read()\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            self.logger.error(f\"File not found: {selected_file_name_path}\")\n",
    "            return False\n",
    "        except OSError as e:\n",
    "            self.logger.error(f\"OS error reading file {selected_file_name_path}: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Unexpected error reading file {selected_file_name_path}: {e}\")\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    async def _spin_dream(self, dream_prompt) -> str:\n",
    "        \"\"\"\n",
    "        Prepares a single piece of data required for the fine-tuning process by interpreting the summary content.\n",
    "\n",
    "        Args:\n",
    "            dream_prompt (str): Prompt to generate a single piece of training material.\n",
    "\n",
    "        Returns:\n",
    "            dict: Data structured for fine-tuning.\n",
    "        \"\"\"\n",
    "\n",
    "        dream_content = self.llm(dream_prompt)\n",
    "        self.logger.blabbering(f\"I had a dream:\\n{dream_content}.\")\n",
    "        dream = self._dream_prompt_template.replace(\"{dream_content}\", dream_content) \n",
    "        return dream\n",
    "\n",
    "    \n",
    "    async def _weave_dreams(self, num_dreams) -> str:\n",
    "        \"\"\"\n",
    "        Generates a specified number of materials (dreams) and writes them into a single text file.\n",
    "        Each 'dream' is appended to the file as it is generated.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(f\"Generating {num_dreams} dreams.\")\n",
    "        dreams_path = os.path.join(self._dream_storage_path, f\"dreams_{StemUtility.get_timestamp()}.txt\")\n",
    "        self.logger.debug(f\"Dreams for this sessions will be saved to: {dreams_path}.\")        \n",
    "        dream_spinning_prompt = self._dream_spinning_prompt_template.replace(\"{conclusion_summary}\", self._conclusions) \n",
    "        self.logger.prompt(f\"Prompt for generating training material from conversation conclusions:\\n{dream_spinning_prompt}.\")   \n",
    "        \n",
    "        for i in range(num_dreams):\n",
    "            self.logger.info(f\"Generating dream # {i}.\")\n",
    "            dream = await self._spin_dream(dream_spinning_prompt)\n",
    "            with open(dreams_path, 'a') as file:  # Open and append each dream, then close the file\n",
    "                file.write(dream + '\\n')\n",
    "        return dreams_path\n",
    "\n",
    "    \n",
    "    async def _deepsleep(self, dreams_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Executes the fine-tuning process using the prepared data.\n",
    "\n",
    "        Args:\n",
    "            fine_tuning_data (dict): Data prepared for fine-tuning.\n",
    "        \"\"\"\n",
    "        \n",
    "        llamacpp_folder = \"llama.cpp\"\n",
    "        finetune_tool = \"finetune.exe\"\n",
    "        lora_tool = \"export-lora.exe\"\n",
    "\n",
    "        finetune_tool_path = os.path.join(llamacpp_folder, finetune_tool)\n",
    "        lora_tool_path = os.path.join(llamacpp_folder, lora_tool)\n",
    "\n",
    "        # Check if finetune_tool_path is a valid file\n",
    "        if not os.path.isfile(finetune_tool_path):\n",
    "            raise FileNotFoundError(f\"Fine-tuning tool not found at {finetune_tool_path}\")\n",
    "\n",
    "        # Check if lora_tool_path is a valid file\n",
    "        if not os.path.isfile(lora_tool_path):\n",
    "            raise FileNotFoundError(f\"LoRA tool not found at {lora_tool_path}\")\n",
    "\n",
    "        \n",
    "        # Fine-tuning command\n",
    "        finetune_command = [\n",
    "            finetune_tool_path,\n",
    "            \"--model-base\", self._base_model_path,\n",
    "            \"--train-data\", dreams_path,\n",
    "            \"--threads\", \"16\",\n",
    "            \"--sample-start\", \"<s>\",\n",
    "            \"--epochs\", \"1\"\n",
    "        ]\n",
    "\n",
    "        self.logger.murmur(f\"Self-finetuning: Creating matrix\")\n",
    "        self.logger.debug(f\"Running command:\\n{finetune_command}.\")\n",
    "        subprocess.run(finetune_command, check=True)\n",
    "\n",
    "        # Export LoRA model command - output to llm_tmp.guff\n",
    "        tmp_model_path = r\"llm_tmp.guff\"\n",
    "        export_command = [\n",
    "            lora_tool_path,\n",
    "            \"--model-base\", self._base_model_path,\n",
    "            \"--model-out\", tmp_model_path,\n",
    "            \"--lora-scaled\", r\".\\ggml-lora-LATEST-f32.gguf\",\n",
    "            \"0.7\"\n",
    "        ]\n",
    "\n",
    "        self.logger.murmur(f\"Self-finetuning: Merging weights\")\n",
    "        self.logger.debug(f\"Running command:\\n{export_command}.\")        \n",
    "        subprocess.run(export_command, check=True)\n",
    "\n",
    "        self.logger.info(f\"Removing {self._base_model_path}, moving {tmp_model_path} to {self._base_model_path}.\")        \n",
    "        # Replace llm_base.guff with llm_tmp.guff\n",
    "        if os.path.exists(self._base_model_path):\n",
    "            os.remove(self._base_model_path)\n",
    "        shutil.move(tmp_model_path, self._base_model_path)\n",
    "        \n",
    "        self.logger.murmur(f\"Self-finetuning: Swapping brain to a new one\")\n",
    "        self.logger.info(f\"Base LLM File swap successful.\")\n",
    "\n",
    "    \n",
    "    def _dream_prunning(self):\n",
    "        \"\"\"\n",
    "        Archives dream materials by moving them from the dream storage path to the archive path.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.logger.info(\"Archiving dream materials.\")\n",
    "        try:\n",
    "            for file_name in os.listdir(self._dream_storage_path):\n",
    "                source_path = os.path.join(self._dream_storage_path, file_name)\n",
    "                destination_path = os.path.join(self._dream_archive_path, file_name)\n",
    "                try:\n",
    "                    shutil.move(source_path, destination_path)\n",
    "                except FileNotFoundError:\n",
    "                    self.logger.error(f\"File not found: {source_path}\")\n",
    "                except PermissionError:\n",
    "                    self.logger.error(f\"Permission denied: Cannot move {source_path}\")\n",
    "                except Exception as e:\n",
    "                    self.logger.error(f\"Error moving file {source_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error accessing dream storage path {self._dream_storage_path}: {e}\")\n",
    "\n",
    "    \n",
    "    async def dream(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the whole process of selecting a summary, reading it, preparing fine-tuning data, and performing fine-tuning.\n",
    "        \"\"\"  \n",
    "\n",
    "        self.logger.murmur(f\"Closing eyes for a well-deserved nap.\")\n",
    "        self.logger.info(f\"Self-finetuning process started.\")        \n",
    "\n",
    "        conclusions_found = self._gather_conclusion()        \n",
    "        if not conclusions_found:\n",
    "            return False\n",
    "        self.logger.info(f\"Selected conclusion to permeate.\")\n",
    "        dreams_path = await self._weave_dreams(self._dreams_number)  # Generate 50 materials, modify as needed\n",
    "        self.logger.info(f\"Self-finetuning materials generated. Staring self-finetuning.\")        \n",
    "        await self._deepsleep(dreams_path)\n",
    "        self._dream_prunning()\n",
    "        self.logger.info(f\"Self-finetuning session ended.\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c9fb0-8c8d-4945-8b45-eea1ff51ac91",
   "metadata": {},
   "source": [
    "### CFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a13bc35b-555d-4573-95ba-e8542d1b6a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CognitiveFeedbackRouter:\n",
    "    \n",
    "    def __init__(self, model_path: str = \"llama-2-13b-chat.Q6_K.gguf\", dmn_countdown: int = 60):\n",
    "        \"\"\"\n",
    "        A class that manages the routing of cognitive feedback based on user input and system states.\n",
    "    \n",
    "        This class orchestrates various components, including a language learning model (LLM), user input handling,\n",
    "        and managing different operational modes based on system states like 'sleeping' or 'overwhelmed'.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.logger.info(f\"Instantiating {self.__class__.__name__}\")\n",
    "        \n",
    "        self.stimulus = None\n",
    "        \n",
    "        self.overwhelmed = asyncio.Event()\n",
    "        self.sleeping = asyncio.Event()\n",
    "        \n",
    "        self.lock = asyncio.Lock()\n",
    "        \n",
    "        self.llm = None\n",
    "        \n",
    "        self._model_path = model_path\n",
    "        self._conversation_handler = None\n",
    "        \n",
    "        self.ready_for_input = asyncio.Event()\n",
    "        self.ready_for_input.set()  # Initially set to ready\n",
    "        self.conversing = asyncio.Event()\n",
    "        self.logger.flag(f\"'ready_for_input' = {self.ready_for_input.is_set()}, 'conversing' = {self.conversing.is_set()}\")\n",
    "\n",
    "        self._dmn_countdown = dmn_countdown # time between last interaction and entering Default Mode\n",
    "        \n",
    "        self.logger.debug(\"Cognitive Feedback Router instantiated.\")\n",
    "        \n",
    "\n",
    "    async def _wakeup(self):\n",
    "        \"\"\"\n",
    "        Wakes up the system and initializes the LLM.\n",
    "\n",
    "        This asynchronous method acquires a lock to ensure thread-safe operations while initializing the LLM.\n",
    "        It clears the 'sleeping' and 'overwhelmed' states and confirms the wake-up process.\n",
    "        \"\"\"\n",
    "        self.logger.debug(\"Starting _wakeup() procedure.\")    \n",
    "        async with self.lock:\n",
    "            self.logger.murmur(\"Just a second, I'm waking up...\")\n",
    "            self.logger.debug(f\"Initializing LLM model from {self._model_path}.\")            \n",
    "            self.llm = LlamaCpp(model_path=self._model_path, \n",
    "                                n_ctx=4096, \n",
    "                                max_tokens=4000,\n",
    "                                n_batch=16)\n",
    "            self.logger.debug(f\"LLM model initialized.\")                        \n",
    "            self.sleeping.clear()\n",
    "            self.overwhelmed.clear()\n",
    "            self.logger.flag(f\"'sleeping' = {self.sleeping.is_set()}, 'conversing' = {self.overwhelmed.is_set()}\")\n",
    "            \n",
    "    async def _get_user_input(self):\n",
    "        \"\"\"\n",
    "        Continuously captures user input in an asynchronous loop.\n",
    "\n",
    "        This method waits for the system to be ready for input, then captures and stores user input.\n",
    "        It clears the 'ready for input' state after capturing the input.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(f\"Starting user input loop\") \n",
    "        while True:\n",
    "            await self.ready_for_input.wait()\n",
    "            user_input = await asyncio.get_event_loop().run_in_executor(None, input, \"Enter something: \")\n",
    "            \n",
    "            self.logger.debug(f\"User input received: {user_input}.\") \n",
    "            self.stimulus = user_input\n",
    "            self.ready_for_input.clear()\n",
    "            self.logger.flag(f\"'ready_for_input' = {self.ready_for_input.is_set()}\")\n",
    "\n",
    "    async def _attention_switch(self):\n",
    "        \"\"\"\n",
    "        Manages the mode of operation based on user input and system states.\n",
    "\n",
    "        This asynchronous method processes user inputs, manages conversation sessions,\n",
    "        and handles the 'sleeping' and 'overwhelmed' states of the system.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(f\"Starting infinite attention loop.\") \n",
    "        while True:\n",
    "            if not self.sleeping.is_set():\n",
    "                if self.stimulus:\n",
    "                    self.logger.debug(f\"User input detected: {self.stimulus}\") \n",
    "                    if not self.conversing.is_set():\n",
    "                        self.logger.debug(f\"Starting new conversation session.\")\n",
    "                        self.conversing.set()\n",
    "                        self.logger.flag(f\"'conversing' = {self.conversing.is_set()}\")\n",
    "                        self._conversation_handler = HumanConversationStimulus(self.llm, self.ready_for_input, self.conversing)\n",
    "                        asyncio.create_task(self._conversation_handler.start_interaction())\n",
    "                    self._conversation_handler.pass_stimulus(self.stimulus)\n",
    "                    self.stimulus = None\n",
    "                elif self.overwhelmed.is_set():\n",
    "                    self.logger.info(f\"Overwhelmed state detected.\") \n",
    "                    rem = ReflectiveEvolutionMonitor(llm=self.llm)\n",
    "                    self.sleeping.set()\n",
    "                    self.logger.flag(f\"'sleeping' = {self.sleeping.is_set()}\")\n",
    "                    await rem.dream()\n",
    "                    asyncio.create_task(self._wakeup())\n",
    "                elif not self.conversing.is_set() and not self.overwhelmed.is_set():\n",
    "                    self.logger.debug(f\"No conversation and no new conclusions detected. Preparing to switch to Default Mode.\")                     \n",
    "                    for _ in range(self._dmn_countdown):\n",
    "                        await asyncio.sleep(1)  # Sleep for 1 second\n",
    "                        if self.stimulus:\n",
    "                            self.logger.debug(f\"Cancelling Default Mode countdown due to user input detection.\")                                                 \n",
    "                            break  # Exit the loop if new user input is detected\n",
    "                    else:  # This else clause executes if the loop completes normally (no break)\n",
    "                        self.logger.debug(f\"Entering Default Mode.\")                                                                         \n",
    "                        dmn = DefaultModeNetwork(self.llm, self.overwhelmed)\n",
    "                        await dmn.run()\n",
    "                        self.logger.debug(f\"Default Mode quit.\")                                                                                                 \n",
    "                else:\n",
    "                    await asyncio.sleep(1)\n",
    "            else:\n",
    "                await asyncio.sleep(1)\n",
    "\n",
    "    async def run(self):\n",
    "        \"\"\"\n",
    "        Initiates and runs the main functionality of the CognitiveFeedbackRouter.\n",
    "\n",
    "        This method starts the system by waking it up, initiating user input capture, and entering the mode selection loop.\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.debug(\"Cognitive Feedback Router starts.\")\n",
    "        await self._wakeup()\n",
    "        asyncio.create_task(self._get_user_input())\n",
    "        await self._attention_switch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d9f50f3-4788-434c-8941-448c2ba643a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:58:01,633 - INFO - CognitiveFeedbackRouter - __init__ - Instantiating CognitiveFeedbackRouter\n",
      "2023-12-13 22:58:01,635 - DEBUG - CognitiveFeedbackRouter - __init__ - Cognitive Feedback Router instantiated.\n",
      "2023-12-13 22:58:01,636 - DEBUG - CognitiveFeedbackRouter - run - Cognitive Feedback Router starts.\n",
      "2023-12-13 22:58:01,636 - DEBUG - CognitiveFeedbackRouter - wakeup - Starting wakeup() procedure.\n",
      "2023-12-13 22:58:01,637 - MURMUR - CognitiveFeedbackRouter - wakeup - Just a second, I'm waking up...\n",
      "2023-12-13 22:58:01,638 - DEBUG - CognitiveFeedbackRouter - wakeup - Initializing LLM model from llama-2-13b-chat.Q6_K.gguf.\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n",
      "2023-12-13 22:58:21,046 - DEBUG - CognitiveFeedbackRouter - wakeup - LLM model initialized.\n",
      "2023-12-13 22:58:21,047 - INFO - CognitiveFeedbackRouter - attention_switch - Starting infinite attention loop.\n",
      "2023-12-13 22:58:21,047 - DEBUG - CognitiveFeedbackRouter - attention_switch - No conversation and no new conclusions detected. Preparing to switch to Default Mode.\n",
      "2023-12-13 22:58:21,048 - DEBUG - CognitiveFeedbackRouter - get_user_input - Starting user input loop\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something:  Hi, how are you today?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:58:30,044 - DEBUG - CognitiveFeedbackRouter - get_user_input - User input received: Hi, how are you today?.\n",
      "2023-12-13 22:58:30,117 - DEBUG - CognitiveFeedbackRouter - attention_switch - Cancelling Default Mode countdown due to user input detection.\n",
      "2023-12-13 22:58:30,117 - DEBUG - CognitiveFeedbackRouter - attention_switch - User input detected: Hi, how are you today?\n",
      "2023-12-13 22:58:30,118 - DEBUG - CognitiveFeedbackRouter - attention_switch - Starting new conversation session.\n",
      "2023-12-13 22:58:30,118 - DEBUG - HumanConversationStimulus - __init__ - Instantiating HumanConversationStimulus with interaction_archive_path: conversations\n",
      "2023-12-13 22:58:30,119 - DEBUG - HumanConversationStimulus - pass_stimulus - Received Hi, how are you today? from CFR.\n",
      "2023-12-13 22:58:30,119 - DEBUG - HumanConversationStimulus - start_interaction - Initiated (pre-loop status)\n",
      "2023-12-13 22:58:30,120 - DEBUG - HumanConversationStimulus - start_interaction - Received Hi, how are you today?\n",
      "2023-12-13 22:58:30,120 - DEBUG - HumanConversationStimulus - start_interaction - Awaiting LLM response\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI: Hello! I'm doing well, thank you for asking. I'm an independent, intelligent entity, so I don't experience emotions or physical sensations like humans do. However, I'm here to assist you with any questions or topics you'd like to discuss. What's on your mind today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something:  end chat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 22:59:23,529 - DEBUG - CognitiveFeedbackRouter - get_user_input - User input received: end chat.\n",
      "2023-12-13 22:59:23,632 - DEBUG - CognitiveFeedbackRouter - attention_switch - User input detected: end chat\n",
      "2023-12-13 22:59:23,632 - DEBUG - HumanConversationStimulus - pass_stimulus - Received end chat from CFR.\n",
      "2023-12-13 22:59:23,633 - DEBUG - HumanConversationStimulus - start_interaction - Received end chat\n",
      "2023-12-13 22:59:23,633 - DEBUG - HumanConversationStimulus - _end_interaction - Conversation cleanup started.\n",
      "2023-12-13 22:59:23,634 - INFO - HumanConversationStimulus - _save_interaction_history - Started conversation saving.\n",
      "2023-12-13 22:59:23,634 - INFO - HumanConversationStimulus - _summarize_interaction - Conversation summarization started.\n",
      "2023-12-13 22:59:23,635 - DEBUG - HumanConversationStimulus - _summarize_interaction - Chat history loaded: Human: Hi, how are you today?\n",
      "AI: Hello! I'm doing well, thank you for asking. I'm an independent, intelligent entity, so I don't experience emotions or physical sensations like humans do. However, I'm here to assist you with any questions or topics you'd like to discuss. What's on your mind today?\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m router \u001b[38;5;241m=\u001b[39m CognitiveFeedbackRouter(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mllama-2-13b-chat.Q6_K.gguf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrouter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\nest_asyncio.py:35\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[1;34m(main, debug)\u001b[0m\n\u001b[0;32m     33\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     82\u001b[0m     f\u001b[38;5;241m.\u001b[39m_log_destroy_pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stopping:\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\nest_asyncio.py:120\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m     handle \u001b[38;5;241m=\u001b[39m ready\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m handle\u001b[38;5;241m.\u001b[39m_cancelled:\n\u001b[1;32m--> 120\u001b[0m         \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\asyncio\\events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSystemExit\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[0;32m     82\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\asyncio\\tasks.py:339\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;66;03m# Don't pass the value of `future.result()` explicitly,\u001b[39;00m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;66;03m# as `Future.__iter__` and `Future.__await__` don't need it.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;66;03m# instead of `__next__()`, which is slower for futures\u001b[39;00m\n\u001b[0;32m    338\u001b[0m     \u001b[38;5;66;03m# that return non-generator iterators from their `__iter__`.\u001b[39;00m\n\u001b[1;32m--> 339\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\nest_asyncio.py:196\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[1;34m(task, exc)\u001b[0m\n\u001b[0;32m    194\u001b[0m curr_task \u001b[38;5;241m=\u001b[39m curr_tasks\u001b[38;5;241m.\u001b[39mget(task\u001b[38;5;241m.\u001b[39m_loop)\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     \u001b[43mstep_orig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m curr_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\asyncio\\tasks.py:256\u001b[0m, in \u001b[0;36mTask.__step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    258\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[44], line 111\u001b[0m, in \u001b[0;36mHumanConversationStimulus.start_interaction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstimulus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstimulus\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend chat\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_end_interaction()\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAwaiting LLM response\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n",
      "Cell \u001b[1;32mIn[44], line 148\u001b[0m, in \u001b[0;36mHumanConversationStimulus._end_interaction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03mEnds the conversation.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03mThis method saves the conversation history, clears event flags, and performs\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03mnecessary cleanup actions.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversation cleanup started.\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_interaction_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversing\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready_for_input\u001b[38;5;241m.\u001b[39mset()\n",
      "Cell \u001b[1;32mIn[44], line 162\u001b[0m, in \u001b[0;36mHumanConversationStimulus._save_interaction_history\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mSaves the conversation history to a file.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \n\u001b[0;32m    158\u001b[0m \u001b[38;5;124;03mThe conversation history is saved with a timestamp and a summary of the conversation\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03mis generated.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarted conversation saving.\u001b[39m\u001b[38;5;124m\"\u001b[39m)        \n\u001b[1;32m--> 162\u001b[0m conversation_keywords \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_summarize_interaction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m conversation_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39mload_memory_variables(inputs\u001b[38;5;241m=\u001b[39m{})[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistory\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    164\u001b[0m conversation_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interaction_archive_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconversation_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mStemUtility\u001b[38;5;241m.\u001b[39mget_timestamp()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[44], line 188\u001b[0m, in \u001b[0;36mHumanConversationStimulus._summarize_interaction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m keywords_generation_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keywords_generation_prompt_template\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{chat_history}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, chat_history)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mprompt(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt for generating keywords from conversation:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mkeywords_generation_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)          \n\u001b[1;32m--> 188\u001b[0m keywords_generated_raw_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeywords_generation_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mblabbering(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull text for summarizing conversation with keywords: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeywords_generated_raw_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[0;32m    190\u001b[0m keywords_generated_pure \u001b[38;5;241m=\u001b[39m StemUtility\u001b[38;5;241m.\u001b[39mextract_keywords(keywords_generated_raw_output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\langchain\\llms\\base.py:806\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    799\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(prompt, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    801\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `prompt` is expected to be a string. Instead found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    802\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(prompt)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to run the LLM on multiple prompts, use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    803\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    804\u001b[0m     )\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    807\u001b[0m         [prompt],\n\u001b[0;32m    808\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    809\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    810\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[0;32m    811\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[0;32m    812\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    813\u001b[0m     )\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    816\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\langchain\\llms\\base.py:602\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    594\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    595\u001b[0m         )\n\u001b[0;32m    596\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    597\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    598\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m), [prompt], invocation_params\u001b[38;5;241m=\u001b[39mparams, options\u001b[38;5;241m=\u001b[39moptions\n\u001b[0;32m    599\u001b[0m         )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    600\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m callback_manager, prompt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(callback_managers, prompts)\n\u001b[0;32m    601\u001b[0m     ]\n\u001b[1;32m--> 602\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    603\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    604\u001b[0m     )\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\langchain\\llms\\base.py:504\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    503\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[1;32m--> 504\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    505\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\langchain\\llms\\base.py:491\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    483\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    487\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    488\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    490\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 491\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    492\u001b[0m                 prompts,\n\u001b[0;32m    493\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    494\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    495\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    496\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    497\u001b[0m             )\n\u001b[0;32m    498\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    499\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    500\u001b[0m         )\n\u001b[0;32m    501\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    502\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\langchain\\llms\\base.py:981\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    978\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m    980\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 981\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    982\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    983\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    984\u001b[0m     )\n\u001b[0;32m    985\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m    986\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\langchain\\llms\\llamacpp.py:265\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[0;32m    264\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[0;32m    266\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    267\u001b[0m     ):\n\u001b[0;32m    268\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\langchain\\llms\\llamacpp.py:315\u001b[0m, in \u001b[0;36mLlamaCpp._stream\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m    314\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m    316\u001b[0m     logprobs \u001b[38;5;241m=\u001b[39m part[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    317\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    318\u001b[0m         text\u001b[38;5;241m=\u001b[39mpart[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    319\u001b[0m         generation_info\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs},\n\u001b[0;32m    320\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\llama_cpp\\llama.py:948\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[1;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[0;32m    946\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    947\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    949\u001b[0m     prompt_tokens,\n\u001b[0;32m    950\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    951\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m    952\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    953\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[0;32m    954\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[0;32m    955\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[0;32m    956\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[0;32m    957\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[0;32m    958\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[0;32m    959\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[0;32m    960\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[0;32m    961\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[0;32m    962\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m    963\u001b[0m ):\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[0;32m    965\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\llama_cpp\\llama.py:768\u001b[0m, in \u001b[0;36mLlama.generate\u001b[1;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[0;32m    765\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    769\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[0;32m    770\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[0;32m    771\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    781\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[0;32m    782\u001b[0m     )\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m    785\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\llama_cpp\\llama.py:491\u001b[0m, in \u001b[0;36mLlama.eval\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m    489\u001b[0m n_past \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids))\n\u001b[0;32m    490\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[1;32m--> 491\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_eval returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\agi\\lib\\site-packages\\llama_cpp\\llama_cpp.py:808\u001b[0m, in \u001b[0;36mllama_eval\u001b[1;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_eval\u001b[39m(\n\u001b[0;32m    802\u001b[0m     ctx: llama_context_p,\n\u001b[0;32m    803\u001b[0m     tokens,  \u001b[38;5;66;03m# type: Array[llama_token]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    806\u001b[0m     n_threads: Union[c_int, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    807\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "router = CognitiveFeedbackRouter(model_path='llama-2-13b-chat.Q6_K.gguf')\n",
    "asyncio.run(router.run())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
